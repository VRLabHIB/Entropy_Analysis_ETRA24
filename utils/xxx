@article{lamb_virtual_2020,
	title = {Virtual reality: a tool for preservice science teachers to put theory into practice},
	volume = {29},
	doi = {10.1007/s10956-020-09837-5},
	pages = {573--585},
	number = {4},
	journaltitle = {Journal of Science Education and Technology},
	author = {Lamb, Richard and Etopio, Elisabeth A.},
	date = {2020},
	keywords = {Methods courses, Professional learning, Science education, Teacher preparation, Virtual reality},
}

@article{schubert_experience_2001,
	title = {The experience of presence: Factor analytic insights},
	volume = {10},
	doi = {10.1162/105474601300343603},
	pages = {266--281},
	number = {3},
	journaltitle = {Presence},
	author = {Schubert, Thomas and Friedmann, Frank and Regenbrecht, Holger},
	date = {2001},
}

@inproceedings{liao_virtual_2019,
	location = {New York, {NY}, {USA}},
	title = {Virtual classmates: Embodying historical learners' messages as learning companions in a {VR} classroom through comment mapping},
	doi = {10.1109/VR.2019.8797708},
	pages = {163--171},
	booktitle = {2019 {IEEE} conference on virtual reality and 3D user interfaces ({VR})},
	publisher = {{IEEE}},
	author = {Liao, Meng-Yun and Sung, Ching-Ying and Wang, Hao-Chuan and Lin, Wen-Chieh},
	date = {2019},
	note = {Place: Osaka, Japan},
}

@article{rizzo_virtual_2000,
	title = {The virtual classroom: a virtual reality environment for the assessment and rehabilitation of attention deficits},
	volume = {3},
	doi = {10.1089/10949310050078940},
	pages = {483--499},
	number = {3},
	journaltitle = {{CyberPsychology} \& Behavior},
	author = {Rizzo, Albert A. and Buckwalter, J. Galen and Bowerly, T. and Van Der Zaag, C. and Humphrey, L. and Neumann, U. and Chua, C. and Kyriakakis, C. and Van Rooyen, A. and Sisemore, D.},
	date = {2000},
}

@inproceedings{sharma_virtual_2013,
	location = {New York, {NY}, {USA}},
	title = {Virtual reality classroom as an constructivist approach},
	isbn = {978-1-4799-0052-7},
	doi = {10.1109/SECON.2013.6567441},
	pages = {1--5},
	booktitle = {2013 proceedings of {IEEE} southeastcon},
	publisher = {{IEEE}},
	author = {Sharma, Sharad and Agada, Ruth and Ruffin, Jeff},
	date = {2013},
	note = {Place: Jacksonville, {FL}, {USA}},
}

@article{alhalabi_virtual_2016,
	title = {Virtual reality systems enhance students’ achievements in engineering education},
	volume = {35},
	doi = {10.1080/0144929X.2016.1212931},
	pages = {1--7},
	number = {11},
	journaltitle = {Behaviour \& Information Technology},
	author = {Alhalabi, Wadee},
	date = {2016},
}

@article{moro_effectiveness_2017,
	title = {The effectiveness of virtual and augmented reality in health sciences and medical anatomy},
	volume = {10},
	doi = {10.1002/ase.1696},
	pages = {549--559},
	number = {6},
	journaltitle = {Anatomical Sciences Education},
	author = {Moro, Christian and Štromberga, Zane and Raikos, Athanasios and Stirling, Allan},
	date = {2017},
}

@inproceedings{casu_riftart_2015,
	location = {Geneva, Switzerland},
	title = {{RiftArt}: Bringing masterpieces in the classroom through immersive virtual reality},
	isbn = {978-3-905674-97-2},
	doi = {10.2312/stag.20151294},
	pages = {77--84},
	booktitle = {Smart tools and apps for graphics - eurographics italian chapter conference},
	publisher = {The Eurographics Association},
	author = {Casu, Andrea and Spano, Lucio Davide and Sorrentino, Fabio and Scateni, Riccardo},
	date = {2015},
	note = {Place: Verona, Italy},
}

@inproceedings{wobbrock_aligned_2011,
	location = {New York, {NY}, {USA}},
	title = {The aligned rank transform for nonparametric factorial analyses using only anova procedures},
	isbn = {978-1-4503-0228-9},
	doi = {10.1145/1978942.1978963},
	pages = {143--146},
	booktitle = {Proceedings of the {SIGCHI} conference on human factors in computing systems},
	publisher = {{ACM}},
	author = {Wobbrock, Jacob O. and Findlater, Leah and Gergle, Darren and Higgins, James J.},
	date = {2011},
	note = {Number of pages: 4
Place: Vancouver, {BC}, Canada},
}

@article{arabadzhiyska_saccade_2017,
	title = {Saccade landing position prediction for gaze-contingent rendering},
	volume = {36},
	issn = {0730-0301},
	doi = {10.1145/3072959.3073642},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Arabadzhiyska, Elena and Tursun, Okan Tarhan and Myszkowski, Karol and Seidel, Hans-Peter and Didyk, Piotr},
	date = {2017},
	note = {Number of pages: 12
Publisher: {ACM}
tex.articleno: 50},
}

@inproceedings{lombard_measuring_2009,
	location = {Los Angeles, {CA}, {USA}},
	title = {Measuring presence: The temple presence inventory},
	pages = {1--15},
	booktitle = {Proceedings of the 12th annual international workshop on presence},
	publisher = {The International Society for Presence Research},
	author = {Lombard, Matthew and Bolmarcich, Theresa and Weinstein, Lisa},
	date = {2009},
}

@article{fuhl_pupil_2016,
	title = {Pupil detection for head-mounted eye tracking in the wild: an evaluation of the state of the art},
	volume = {27},
	doi = {10.1007/s00138-016-0776-4},
	number = {8},
	journaltitle = {Machine Vision and Applications},
	author = {Fuhl, Wolfgang and Tonsen, Marc and Bulling, Andreas and Kasneci, Enkelejda},
	date = {2016},
}

@article{roth_ray_1982,
	title = {Ray casting for modeling solids},
	volume = {18},
	issn = {0146-664X},
	doi = {10.1016/0146-664X(82)90169-1},
	pages = {109--144},
	number = {2},
	journaltitle = {Computer Graphics and Image Processing},
	author = {Roth, Scott D.},
	date = {1982},
}

@article{orlosky_emulation_2017,
	title = {Emulation of physician tasks in eye-tracked virtual reality for remote diagnosis of neurodegenerative disease},
	volume = {23},
	doi = {10.1109/TVCG.2017.2657018},
	pages = {1302--1311},
	number = {4},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Orlosky, Jason and Itoh, Yuta and Ranchet, Maud and Kiyokawa, Kiyoshi and Morgan, John and Devos, Hannes},
	date = {2017},
}

@article{marsh_determinants_1984,
	title = {Determinants of student self-concept: Is it better to be a relatively large fish in a small pond even if you don't learn to swim as well?},
	volume = {47},
	doi = {10.1037/0022-3514.47.1.213},
	pages = {213--231},
	number = {1},
	journaltitle = {Journal of Personality and Social Psychology},
	author = {Marsh, Herbert W. and Parker, John W.},
	date = {1984},
}

@article{kavanagh_systematic_2017,
	title = {A systematic review of Virtual Reality in education},
	volume = {10},
	issn = {1792-8788},
	pages = {85--119},
	number = {2},
	journaltitle = {Themes in Science and Technology Education},
	author = {Kavanagh, Sam and Luxton-Reilly, Andrew and Wuensche, Burkhard and Plimmer, Beryl},
	date = {2017},
}

@inproceedings{freina_literature_2015,
	location = {Bucharest, Romania},
	title = {A literature review on immersive virtual reality in education: State of the art and perspectives},
	doi = {10.12753/2066-026X-15-020},
	pages = {133--141},
	booktitle = {Proceedings of the 11th international scientific conference {eLearning} and software for education},
	publisher = {Carol I {NDU} Publishing House},
	author = {Freina, Laura and Ott, Michela},
	date = {2015},
	note = {Place: Bucharest, Romania},
}

@article{weintrop_defining_2016,
	title = {Defining computational thinking for mathematics and science classrooms},
	volume = {25},
	doi = {10.1007/s10956-015-9581-5},
	pages = {127--147},
	number = {1},
	journaltitle = {Journal of Science Education and Technology},
	author = {Weintrop, David and Beheshti, Elham and Horn, Michael and Kai, Orton and Jona, Kemi and Trouille, Laura and Wilensky, Uri},
	date = {2016},
}

@collection{song_vrar_2021,
	location = {Singapore},
	title = {{VR}/{AR} and 3D Displays: First International Conference, {ICVRD} 2020, Hangzhou, China, December 20, 2020, Revised Selected Papers},
	volume = {1313},
	isbn = {978-981-336-548-3 978-981-336-549-0},
	series = {Communications in Computer and Information Science},
	shorttitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	editor = {Song, Weitao and Xu, Feng},
	urldate = {2023-09-05},
	date = {2021},
	doi = {10.1007/978-981-33-6549-0},
}

@article{caissie_what_2009,
	title = {What does the Mental Rotation Test Measure? An Analysis of Item Difficulty and Item Characteristics},
	volume = {2},
	doi = {10.2174/1874350100902010094},
	pages = {94--102},
	journaltitle = {The Open Psychology Journal},
	author = {Caissie, André and Vigneau, Francois and Bors, Douglas},
	date = {2009},
}

@inproceedings{fan_virtual_2021,
	title = {Virtual Reality App for {ASD} Child Early Training},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_9},
	series = {Communications in Computer and Information Science},
	pages = {89--102},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Fan, Lei and Cao, Wei and Du, Yasong and Chen, Jing and Zhou, Jiantao and Zhai, Guangtao},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {{ASD}, Autism, Child, Early training, Virtual reality},
}

@report{htc_vive_2019,
	title = {{VIVE} pro eye user guide},
	institution = {{HTC} Corporation},
	type = {manual},
	author = {{HTC}},
	date = {2019},
}

@article{holliman_visual_2022,
	title = {Visual Entropy and the Visualization of Uncertainty},
	url = {http://arxiv.org/abs/1907.12879},
	journaltitle = {{arXiv}:1907.12879},
	author = {Holliman, Nicolas S. and Coltekin, Arzu and Fernstad, Sara J. and {McLaughlin}, Lucy and Simpson, Michael D. and Woods, Andrew J.},
	urldate = {2022-05-03},
	date = {2022-04},
	keywords = {Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Information Theory},
}

@misc{epic_games_unreal_2019,
	title = {Unreal engine, version 4.23.1},
	url = {https://www.unrealengine.com},
	author = {{Epic Games}},
	date = {2019},
}

@article{kozhevnikov_understanding_2012,
	title = {Understanding Immersivity: Image Generation and Transformation Processes in 3D Immersive Environments},
	volume = {3},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00284},
	doi = {10.3389/fpsyg.2012.00284},
	shorttitle = {Understanding Immersivity},
	journaltitle = {Frontiers in Psychology},
	author = {Kozhevnikov, Maria and Dhond, Rupali},
	urldate = {2022-09-10},
	date = {2012},
}

@inproceedings{aitsiselmi_using_2009,
	title = {Using mental rotation to evaluate the benefits of stereoscopic displays},
	volume = {7237},
	doi = {10.1117/12.824527},
	booktitle = {Society of Photo-Optical Instrumentation Engineers ({SPIE}) Conference Series},
	author = {Aitsiselmi, Y. and Holliman, N. S.},
	urldate = {2023-09-05},
	date = {2009-02-01},
}

@article{todd_visual_2004,
	title = {The visual perception of 3D shape},
	volume = {8},
	issn = {1364-6613, 1879-307X},
	doi = {10.1016/j.tics.2004.01.006},
	pages = {115--121},
	number = {3},
	journaltitle = {Trends in Cognitive Sciences},
	author = {Todd, James T.},
	urldate = {2023-09-07},
	date = {2004-03-01},
	note = {Publisher: Elsevier
tex.eprint: 15301751
tex.eprinttype: pmid},
}

@article{cole_tonic_2022,
	title = {Tonic and phasic effects of reward on the pupil: Implications for locus coeruleus function},
	volume = {289},
	doi = {10.1098/rspb.2022.1545},
	shorttitle = {Tonic and phasic effects of reward on the pupil},
	pages = {20221545},
	number = {1982},
	journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Cole, Laura and Lightman, Stafford and Clark, Rosie and Gilchrist, Iain D.},
	urldate = {2023-09-07},
	date = {2022-09-14},
	note = {Publisher: Royal Society},
	keywords = {auditory oddball paradigm, locus coeruleus, pupil, reward},
}

@article{voyer_time_2011,
	title = {Time limits and gender differences on paper-and-pencil tests of mental rotation: a meta-analysis},
	volume = {18},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-010-0042-0},
	doi = {10.3758/s13423-010-0042-0},
	pages = {267--277},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	author = {Voyer, Daniel},
	date = {2011},
}

@article{bruce_role_2015,
	title = {The role of 2D and 3D mental rotation in mathematics for young children: what is it? Why does it matter? And what can we do about it?},
	volume = {47},
	doi = {10.1007/s11858-014-0637-4},
	shorttitle = {The role of 2D and 3D mental rotation in mathematics for young children},
	pages = {331--343},
	journaltitle = {{ZDM} Mathematics Education},
	author = {Bruce, Cathy and Hawes, Zachary},
	date = {2015-06},
}

@article{hoyek_use_2012,
	title = {The Use of the Vandenberg and Kuse Mental Rotation Test in Children},
	volume = {33},
	issn = {1614-0001, 2151-2299},
	url = {https://econtent.hogrefe.com/doi/10.1027/1614-0001/a000063},
	doi = {10.1027/1614-0001/a000063},
	pages = {62--67},
	number = {1},
	journaltitle = {Journal of Individual Differences},
	author = {Hoyek, Nady and Collet, Christian and Fargier, Patrick and Guillot, Aymeric},
	urldate = {2021-10-27},
	date = {2012-01},
	langid = {english},
}

@article{chmielewski_norepinephrine_2017,
	title = {The norepinephrine system affects specific neurophysiological subprocesses in the modulation of inhibitory control by working memory demands},
	volume = {38},
	issn = {1097-0193},
	doi = {10.1002/hbm.23344},
	pages = {68--81},
	number = {1},
	journaltitle = {Human Brain Mapping},
	author = {Chmielewski, Witold X. and Mückschel, Moritz and Ziemssen, Tjalf and Beste, Christian},
	urldate = {2023-09-13},
	date = {2017},
	keywords = {{EEG}, mental rotation, norepinephrine, pupil diameter, response inhibition, source localization, working memory},
}

@inproceedings{iqbal_task-evoked_2004,
	location = {Vienna, Austria},
	title = {Task-evoked pupillary response to mental workload in human-computer interaction},
	isbn = {978-1-58113-703-3},
	url = {http://portal.acm.org/citation.cfm?doid=985921.986094},
	doi = {10.1145/985921.986094},
	pages = {1477},
	booktitle = {Extended abstracts of the 2004 conference on Human factors and computing systems - {CHI} '04},
	publisher = {{ACM} Press},
	author = {Iqbal, Shamsi T. and Zheng, Xianjun Sam and Bailey, Brian P.},
	urldate = {2022-12-12},
	date = {2004},
	langid = {english},
}

@article{holleman_real-world_2020,
	title = {The ‘Real-World Approach’ and Its Problems: A Critique of the Term Ecological Validity},
	volume = {11},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00721},
	doi = {10.3389/fpsyg.2020.00721},
	shorttitle = {The ‘Real-World Approach’ and Its Problems},
	journaltitle = {Frontiers in Psychology},
	author = {Holleman, Gijs A. and Hooge, Ignace T. C. and Kemner, Chantal and Hessels, Roy S.},
	urldate = {2022-09-15},
	date = {2020},
}

@article{lauer_development_2019,
	title = {The development of gender differences in spatial reasoning: A meta-analytic review},
	volume = {145},
	issn = {0033-2909},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com},
	doi = {10.1037/bul0000191},
	shorttitle = {The development of gender differences in spatial reasoning},
	pages = {537--565},
	number = {6},
	journaltitle = {Psychological Bulletin},
	author = {Lauer, Jillian E. and Yhang, Eukyung and Lourenco, Stella F.},
	urldate = {2022-09-11},
	date = {2019-06},
	keywords = {Human Sex Differences, Mental Rotation, Reasoning, Spatial Ability, gender difference, mental rotation, meta-analysis, spatial development},
}

@inproceedings{nakayama_act_2002,
	title = {The act of task difficulty and eye-movement frequency for the 'Oculo-motor indices'},
	isbn = {978-1-58113-467-4},
	doi = {10.1145/507072.507080},
	series = {{ETRA} '02},
	pages = {37--42},
	booktitle = {Proceedings of the 2002 symposium on Eye tracking research \& applications},
	publisher = {{ACM}, New York},
	author = {Nakayama, Minoru and Takahashi, Koji and Shimizu, Yasutaka},
	date = {2002},
	note = {Place: New York, {NY}, {USA}},
}

@article{bahill_main_1975,
	title = {The main sequence, a tool for studying human eye movements},
	volume = {24},
	issn = {0025-5564},
	doi = {10.1016/0025-5564(75)90075-9},
	pages = {191--204},
	number = {3},
	journaltitle = {Mathematical Biosciences},
	author = {Bahill, A. Terry and Clark, Michael R. and Stark, Lawrence},
	date = {1975},
}

@inproceedings{wan_stereoscopic_2021,
	title = {Stereoscopic 3D Depth Perception Analysis of H.264/{AVC} Coded Video},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_7},
	series = {Communications in Computer and Information Science},
	pages = {66--77},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Wan, Wenfei and Wu, Hong Ren and Wu, Jinjian and Shi, Guangming},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Depth perception, Frequency components., Stereoscopic 3d video, Subjective experiments},
}

@article{nazareth_strategy_2019,
	title = {Strategy selection versus flexibility: Using eye-trackers to investigate strategy use during mental rotation},
	volume = {45},
	issn = {1939-1285},
	doi = {10.1037/xlm0000574},
	shorttitle = {Strategy selection versus flexibility},
	pages = {232--245},
	number = {2},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Nazareth, Alina and Killick, Rebecca and Dick, Anthony S. and Pruden, Shannon M.},
	date = {2019},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Cognition, Cognitive Processes, Eye Movements, Human Sex Differences, Spatial Ability},
}

@article{friedman_stochastic_2002,
	title = {Stochastic gradient boosting},
	volume = {38},
	issn = {0167-9473},
	doi = {10.1016/S0167-9473(01)00065-2},
	series = {Nonlinear Methods and Data Mining},
	pages = {367--378},
	number = {4},
	journaltitle = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	urldate = {2023-09-14},
	date = {2002-02-28},
}

@article{kawamichi_spatio-temporal_2007,
	title = {Spatio-temporal brain activity related to rotation method during a mental rotation task of three-dimensional objects: An {MEG} study},
	volume = {37},
	doi = {10.1016/j.neuroimage.2007.06.001},
	shorttitle = {Spatio-temporal brain activity related to rotation method during a mental rotation task of three-dimensional objects},
	pages = {956--65},
	journaltitle = {{NeuroImage}},
	author = {Kawamichi, Hiroaki and Kikuchi, Yoshiaki and Ueno, Shoogo},
	date = {2007-10},
}

@article{fisher_sex_2018,
	title = {Sex Differences in Mental Rotation Ability Are a Consequence of Procedure and Artificiality of Stimuli},
	volume = {4},
	doi = {10.1007/s40806-017-0120-x},
	pages = {1--10},
	journaltitle = {Evolutionary Psychological Science},
	author = {Fisher, Maryanne and Meredith, Tami and Gray, Melissa},
	date = {2018-06},
}

@article{moen_strengthening_2020,
	title = {Strengthening spatial reasoning: elucidating the attentional and neural mechanisms associated with mental rotation skill development},
	volume = {5},
	issn = {2365-7464},
	url = {https://doi.org/10.1186/s41235-020-00211-y},
	doi = {10.1186/s41235-020-00211-y},
	pages = {20},
	number = {1},
	journaltitle = {Cognitive Research: Principles and Implications},
	author = {Moen, Katherine C. and Beck, Melissa R. and Saltzmann, Stephanie M. and Cowan, Tovah M. and Burleigh, Lauryn M. and Butler, Leslie G. and Ramanujam, Jagannathan and Cohen, Alex S. and Greening, Steven G.},
	date = {2020},
}

@article{burton_sex_2013,
	title = {Sex differences in relationships between verbal fluency and personality},
	volume = {32},
	issn = {1936-4733},
	url = {https://doi.org/10.1007/s12144-013-9167-4},
	doi = {10.1007/s12144-013-9167-4},
	pages = {168--174},
	number = {2},
	journaltitle = {Current Psychology},
	author = {Burton, Leslie A. and Henninger, Debra},
	urldate = {2021-10-19},
	date = {2013-06-01},
}

@book{schiffman_sensation_2001,
	location = {New York},
	title = {Sensation and Perception: An Integrated Approach},
	isbn = {978-0-471-24930-6},
	shorttitle = {Sensation and Perception},
	pagetotal = {608},
	publisher = {John Wiley \& Sons},
	author = {Schiffman, Harvey Richard},
	date = {2001-01-15},
}

@article{desperati_saccades_1999,
	title = {Saccades to mentally rotated targets},
	volume = {126},
	issn = {1432-1106},
	doi = {10.1007/s002210050765},
	pages = {563--577},
	number = {4},
	journaltitle = {Experimental brain research},
	shortjournal = {Exp Brain Res},
	author = {de’Sperati, Claudio},
	urldate = {2023-09-13},
	date = {1999-06-01},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	shorttitle = {Scikit-learn},
	pages = {2825--2830},
	number = {85},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	urldate = {2022-09-09},
	date = {2011},
}

@article{mathot_safe_2018,
	title = {Safe and sensible preprocessing and baseline correction of pupil-size data},
	volume = {50},
	issn = {1554-3528},
	doi = {10.3758/s13428-017-1007-2},
	pages = {94--106},
	number = {1},
	journaltitle = {Behavior Research Methods},
	author = {Mathôt, Sebastiaan and Fabius, Jasper and Van Heusden, Elle and Van der Stigchel, Stefan},
	date = {2018},
}

@article{scheer_sex_2018,
	title = {Sex differences in a chronometric mental rotation test with cube figures: a behavioral, electroencephalography, and eye-tracking pilot study},
	volume = {29},
	issn = {0959-4965},
	url = {https://journals.lww.com/neuroreport/Fulltext/2018/07010/Sex_differences_in_a_chronometric_mental_rotation.14.aspx},
	doi = {10.1097/WNR.0000000000001046},
	number = {10},
	journaltitle = {{NeuroReport}},
	author = {Scheer, Clara and Mattioni Maturana, Felipe and Jansen, Petra},
	date = {2018},
	keywords = {electroencephalography, gaze patterns, neural efficiency, spatial ability},
}

@article{andersson_sampling_2010,
	title = {Sampling frequency and eye-tracking measures: how speed affects durations, latencies, and more},
	volume = {3},
	rights = {Copyright (c) 2010 Richard Andersson, Marcus Nyström, Kenneth Holmqvist},
	issn = {1995-8692},
	url = {https://bop.unibe.ch/JEMR/article/view/2300},
	doi = {10.16910/jemr.3.3.6},
	number = {3},
	journaltitle = {Journal of Eye Movement Research},
	author = {Andersson, Richard and Nyström, Marcus and Holmqvist, Kenneth},
	urldate = {2023-09-12},
	date = {2010-09-13},
}

@article{khooshabeh_representations_2010,
	title = {Representations of Shape during Mental Rotation.},
	journaltitle = {{AAAI} Spring Symposium: Cognitive Shape Processing},
	author = {Khooshabeh, Peter and Hegarty, Mary},
	date = {2010-01-01},
}

@article{goldstein_rotation_1979,
	title = {Rotation of objects in pictures viewed at an angle: Evidence for different properties of two types of pictorial space},
	volume = {5},
	issn = {0096-1523},
	doi = {10.1037//0096-1523.5.1.78},
	pages = {78--87},
	number = {1},
	journaltitle = {Journal of experimental psychology. Human perception and performance},
	shortjournal = {J Exp Psychol Hum Percept Perform},
	author = {Goldstein, E. B.},
	date = {1979},
}

@inproceedings{fu_research_2021,
	title = {Research on the Application of 3D Visualization of Marine Environmental Data in Underwater Sub-mersibles Route Planning},
	doi = {10.1007/978-981-33-6549-0_1},
	series = {Communications in Computer and Information Science},
	pages = {1--13},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Fu, Jun and Chang, Yang and Ning, Zhiwen and Han, Hongxiang},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {3D visualization, Assistant decision, Marine environmental data, Underwater submersibles},
}

@article{seideman_saccade_2018,
	title = {Saccade metrics reflect decision-making dynamics during urgent choices},
	volume = {9},
	issn = {2041-1723},
	doi = {10.1038/s41467-018-05319-w},
	pages = {2907},
	number = {1},
	journaltitle = {Nature communications},
	shortjournal = {Nat Commun},
	author = {Seideman, Joshua A. and Stanford, Terrence R. and Salinas, Emilio},
	urldate = {2023-06-24},
	date = {2018-07-25},
	note = {Publisher: Nature Publishing Group},
}

@article{gilzenrat_pupil_2010,
	title = {Pupil diameter tracks changes in control state predicted by the adaptive gain theory of locus coeruleus function},
	volume = {10},
	issn = {1531-135X},
	url = {https://doi.org/10.3758/CABN.10.2.252},
	doi = {10.3758/CABN.10.2.252},
	pages = {252--269},
	number = {2},
	journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
	author = {Gilzenrat, Mark S. and Nieuwenhuis, Sander and Jepma, Marieke and Cohen, Jonathan D.},
	urldate = {2023-09-13},
	date = {2010-06-01},
	keywords = {Comparison Tone, Locus Coeruleus, Pupil Diameter, Pupillary Response, Task Engagement},
}

@inproceedings{jiang_modeling_2021,
	title = {Modeling the Self-navigation Behavior of Patients with Alzheimer’s Disease in Virtual Reality},
	doi = {10.1007/978-981-33-6549-0_11},
	series = {Communications in Computer and Information Science},
	pages = {121--136},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Jiang, Jinghui and Zhai, Guangtao and Jiang, Zheng},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Alzheimer’s disease, Deep reinforcement learning, Spatial navigation},
}

@article{kret_preprocessing_2019,
	title = {Preprocessing pupil size data: Guidelines and code},
	volume = {51},
	issn = {1554-3528},
	doi = {10.3758/s13428-018-1075-y},
	pages = {1336--1342},
	number = {3},
	journaltitle = {Behavior Research Methods},
	author = {Kret, Mariska E. and Sjak-Shie, Elio E.},
	date = {2019},
}

@article{zacks_neuroimaging_2008,
	title = {Neuroimaging Studies of Mental Rotation: A Meta-analysis and Review},
	volume = {20},
	url = {https://doi.org/10.1162/jocn.2008.20013},
	doi = {10.1162/jocn.2008.20013},
	pages = {1--19},
	number = {1},
	journaltitle = {Journal of Cognitive Neuroscience},
	author = {Zacks, Jeffrey M.},
	date = {2008},
}

@article{goumans_peaks_2010,
	title = {Peaks and Troughs of Three-Dimensional Vestibulo-ocular Reflex in Humans},
	volume = {11},
	doi = {10.1007/s10162-010-0210-y},
	pages = {383--393},
	number = {3},
	journaltitle = {Journal of the Association for Research in Otolaryngology : {JARO}},
	shortjournal = {J Assoc Res Otolaryngol},
	author = {Goumans, Janine and Houben, Mark M. J. and Dits, Joyce and Steen, Johannes van der},
	date = {2010},
}

@article{taragin_mental_2019,
	title = {Mental rotation: The effects of processing strategy, gender and task characteristics on children's accuracy, reaction time and eye movements’ pattern},
	volume = {12},
	url = {https://bop.unibe.ch/JEMR/article/view/JEMR.12.8.2},
	doi = {10.16910/jemr.12.8.2},
	number = {8},
	journaltitle = {{JEMR}},
	author = {Taragin, Dorit and Tzuriel, David and Vakil, Eli},
	date = {2019-11},
}

@article{jansen_mental_2020,
	title = {Mental rotation with abstract and embodied objects as stimuli: evidence from event-related potential ({ERP})},
	volume = {238},
	issn = {1432-1106},
	url = {https://doi.org/10.1007/s00221-020-05734-w},
	doi = {10.1007/s00221-020-05734-w},
	pages = {525--535},
	number = {3},
	journaltitle = {Experimental Brain Research},
	author = {Jansen, Petra and Render, Anna and Scheer, Clara and Siebertz, Markus},
	date = {2020},
}

@article{paschke_mirrored_2012,
	title = {Mirrored or identical—Is the role of visual perception underestimated in the mental rotation process of 3D-objects?: A combined {fMRI}-eye tracking-study},
	volume = {50},
	issn = {1873-3514},
	doi = {10.1016/j.neuropsychologia.2012.04.010},
	shorttitle = {Mirrored or identical—Is the role of visual perception underestimated in the mental rotation process of 3D-objects?},
	pages = {1844--1851},
	journaltitle = {Neuropsychologia},
	author = {Paschke, Kerstin and Jordan, Kirsten and Wüstenberg, Torsten and Baudewig, Jürgen and Müller, Jürgen Leo},
	date = {2012},
	keywords = {Cognitive Processes, Functional Magnetic Resonance Imaging, Mental Rotation, Visual Perception, Visual Tracking},
}

@article{voyer_mental_2017,
	title = {Mental Rotation with Egocentric and Object-Based Transformations},
	volume = {70},
	issn = {1747-0218},
	url = {https://doi.org/10.1080/17470218.2016.1233571},
	doi = {10.1080/17470218.2016.1233571},
	pages = {2319--2330},
	number = {11},
	journaltitle = {Quarterly Journal of Experimental Psychology},
	author = {Voyer, Daniel and Jansen, Petra and Kaltner, Sandra},
	urldate = {2022-12-13},
	date = {2017-11},
	langid = {english},
}

@article{van_acker_mobile_2020,
	title = {Mobile pupillometry in manual assembly: A pilot study exploring the wearability and external validity of a renowned mental workload lab measure},
	volume = {75},
	issn = {0169-8141},
	url = {http://www.sciencedirect.com/science/article/pii/S0169814119300514},
	doi = {10.1016/j.ergon.2019.102891},
	pages = {102891},
	journaltitle = {International Journal of Industrial Ergonomics},
	author = {Van Acker, Bram B. and Bombeke, Klaas and Durnez, Wouter and Parmentier, Davy D. and Mateus, João Costa and Biondi, Alessandro and Saldien, Jelle and Vlerick, Peter},
	date = {2020},
	keywords = {Assembly, External validity, Eye tracking glasses, Mental workload, Pupillometry, Wearability},
}

@article{shepard_mental_1971,
	title = {Mental rotation of three-dimensional objects},
	volume = {171},
	issn = {3972},
	doi = {10.1126/science.171.3972.701},
	pages = {701--703},
	number = {3972},
	journaltitle = {Science},
	author = {Shepard, Roger N. and Metzler, Jacqueline},
	date = {1971},
}

@article{wohlschlager_mental_1998,
	title = {Mental and manual rotation},
	volume = {24},
	issn = {1939-1277},
	doi = {10.1037/0096-1523.24.2.397},
	pages = {397--412},
	number = {2},
	journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Wohlschläger, Andreas and Wohlschläger, Astrid},
	date = {1998},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {College Students, Form and Shape Perception, Mental Rotation, Reaction Time, Spatial Orientation (Perception)},
}

@book{shepard_mental_1986,
	location = {Cambridge, {MA}, {US}},
	title = {Mental images and their transformations},
	isbn = {978-0-262-19200-2},
	pagetotal = {viii, 364},
	publisher = {The {MIT} Press},
	author = {Shepard, Roger N. and Cooper, Lynn A.},
	date = {1986},
	note = {Pages: viii, 364},
}

@inproceedings{dai_large_2021,
	title = {Large Horizontal Viewing-Angle Three-Dimensional Light Field Display Based on Liquid Crystal Barrier and Time-Division-Multiplexing},
	doi = {10.1007/978-981-33-6549-0_5},
	series = {Communications in Computer and Information Science},
	pages = {45--55},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Dai, Renxiang and Sang, Xinzhu and Xing, Shujun and Yu, Xunbo and Gao, Xin and Liu, Li and Liu, Boyang and Gao, Chao and Wang, Yuedi and Ge, Fan},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Elemental image array, Fill factor, Lens array, Light field display, Liquid crystal barrier, Time-division-multiplexing},
}

@article{cooper_mental_1990,
	title = {Mental representation of three-dimensional objects in visual problem solving and recognition},
	volume = {16},
	issn = {1939-1285},
	doi = {10.1037/0278-7393.16.6.1097},
	pages = {1097--1106},
	number = {6},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Cooper, Lynn A.},
	date = {1990},
	note = {Place: {US}
Publisher: American Psychological Association},
}

@article{varriale_mental_2018,
	title = {Mental rotation and fluid intelligence: A brain potential analysis},
	volume = {69},
	issn = {0160-2896},
	url = {http://www.sciencedirect.com/science/article/pii/S0160289618300011},
	doi = {10.1016/j.intell.2018.05.007},
	pages = {146 -- 157},
	journaltitle = {Intelligence},
	author = {Varriale, Vincenzo and Molen, Maurits W. van der and Pascalis, Vilfredo De},
	date = {2018},
	keywords = {{EEG}, Intelligence, Mental Rotation, {RRN}, {RT}},
}

@article{fitzhugh_mental_2010,
	title = {Mental rotation of real word Shepard-Metzler figures: An eye tracking study},
	volume = {8},
	doi = {10.1167/8.6.648},
	pages = {648--648},
	journaltitle = {Journal of Vision},
	author = {Fitzhugh, S. and Shipley, T. and Newcombe, Nora and {McKenna}, K. and Dumay, D.},
	date = {2010},
}

@inproceedings{you_high-quality_2021,
	title = {High-Quality Facial Expression Animation Synthesis System Based on Virtual Reality},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_3},
	series = {Communications in Computer and Information Science},
	pages = {21--32},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {You, Yang and Song, Limei and Yang, Yangang},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Expression capture, Expression synthesis, Kinect, Online education, Virtual human},
}

@inproceedings{xia_integral_2021,
	title = {Integral Imaging Tabletop 3D Display System Based on Compound Lens Array},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_2},
	series = {Communications in Computer and Information Science},
	pages = {14--20},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Xia, Yun-Peng and Xing, Yan and Ren, Hui and Li, Shuang and Wang, Qiong-Hua},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Compound lens array, Integral imaging, Tabletop 3D display},
}

@book{buswell_how_1935,
	location = {Oxford, England},
	title = {How people look at pictures: a study of the psychology and perception in art},
	series = {How people look at pictures: a study of the psychology and perception in art},
	shorttitle = {How people look at pictures},
	publisher = {Univ. Chicago Press},
	author = {Buswell, G. T.},
	date = {1935},
}

@article{lin_how_2019,
	title = {How Motion-Control Influences a {VR}-Supported Technology for Mental Rotation Learning: From the Perspectives of Playfulness, Gender Difference and Technology Acceptance Model},
	volume = {35},
	issn = {1044-7318},
	doi = {10.1080/10447318.2019.1571784},
	pages = {1736--1746},
	number = {18},
	journaltitle = {International Journal of Human-computer Interaction},
	author = {Lin, Po-Han and Yeh, Shih-Ching},
	urldate = {2023-06-19},
	date = {2019-02-12},
	note = {Publisher: Taylor \& Francis},
}

@article{khooshabeh_individual_2012,
	title = {Individual Differences in Mental Rotation},
	volume = {60},
	doi = {10.1027/1618-3169/a000184},
	pages = {1--8},
	journaltitle = {Experimental Psychology},
	author = {Khooshabeh, Peter and Hegarty, Mary and Shipley, Thomas},
	date = {2012-11},
}

@article{toth_investigating_2019,
	title = {Investigating sex differences, cognitive effort, strategy, and performance on a computerised version of the mental rotations test via eye tracking},
	url = {https://ulir.ul.ie/handle/10344/8541},
	doi = {10.1038/s41598-019-56041-6},
	journaltitle = {Scientific Reports},
	author = {Toth, Adam J. and Campbell, Mark J.},
	urldate = {2022-01-26},
	date = {2019},
}

@article{zelinsky_eye_1997,
	title = {Eye movements during parallel-serial visual search},
	volume = {23},
	issn = {0096-1523},
	doi = {10.1037//0096-1523.23.1.244},
	pages = {244--262},
	number = {1},
	journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
	author = {Zelinsky, G. J. and Sheinberg, D. L.},
	date = {1997-02},
	pmid = {9090154},
	keywords = {Female, Fixation, Humans, Male, Ocular, Reaction Time, Saccades, Visual Perception},
}

@article{jansen_gender_2009,
	title = {Gender Differences in Mental Rotation Across Adulthood},
	volume = {36},
	issn = {0361-073X},
	url = {https://doi.org/10.1080/03610730903422762},
	doi = {10.1080/03610730903422762},
	abstract = {Although gender differences in mental rotation in younger adults are prominent in paper-pencil tests as well as in chronometric tests with polygons as stimuli, less is known about this topic in the older age ranges. Therefore, performance was assessed with the Mental Rotation Test ({MRT}) paper-pencil test as well as with a computer-based two-stimulus same-different task with polygons in a sample of 150 adults divided into three age groups, 20–30, 40–50, and 60–70 years. Performance decreased with age, and men outperformed women in all age groups. The gender effect decreased with age in the {MRT}, possibly due to a floor effect. Gender differences remained constant across age, however, in the error rates of the computer-based task.},
	pages = {94--104},
	number = {1},
	journaltitle = {Experimental Aging Research},
	author = {Jansen, Petra and Heil, Martin},
	urldate = {2021-10-27},
	date = {2009-12},
}

@book{holmqvist_eye_2011,
	title = {Eye tracking: A comprehensive guide to methods and measures},
	shorttitle = {Eye tracking},
	publisher = {{OUP} Oxford},
	author = {Holmqvist, Kenneth and Nyström, Marcus and Andersson, Richard and Dewhurst, Richard and Jarodzka, Halszka and Van de Weijer, Joost},
	date = {2011},
}

@misc{hao_global_2014,
	title = {Global integrated drought monitoring and prediction system ({GIDMaPS}) data sets},
	url = {http://dx.doi.org/10.6084/m9.figshare.853801},
	author = {Hao, Z. and {AghaKouchak}, A. and Nakhjiri, N. and Farahmand, A},
	date = {2014},
}

@article{rayner_eye_1998,
	title = {Eye movements in reading and information processing: 20 years of research.},
	volume = {124},
	doi = {10.1037/0033-2909.124.3.372},
	shorttitle = {Eye movements in reading and information processing},
	pages = {372--422},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {Rayner, Keith},
	date = {1998},
}

@article{bilge_framing_2017,
	title = {Framing the figure: Mental rotation revisited in light of cognitive strategies},
	volume = {45},
	issn = {1532-5946},
	url = {https://doi.org/10.3758/s13421-016-0648-1},
	doi = {10.3758/s13421-016-0648-1},
	pages = {63--80},
	number = {1},
	journaltitle = {Memory \& Cognition},
	shortjournal = {Mem Cogn},
	author = {Bilge, A. Reyyan and Taylor, Holly A.},
	urldate = {2021-12-21},
	date = {2017-01-01},
}

@inproceedings{ge_extended-depth_2021,
	title = {Extended-Depth Light Field Display Based on Controlling-Light Structure in Cross Arrangement},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_6},
	series = {Communications in Computer and Information Science},
	pages = {56--65},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Ge, Fan and Sang, Xinzhu},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Controlling-light structure, Depth range, Light field display},
}

@article{hunt_exploring_2015,
	title = {Exploring the Relationship Between Mathematics Anxiety and Performance: An Eye-Tracking Approach: Math anxiety, performance, and eye-tracking},
	volume = {29},
	issn = {08884080},
	doi = {10.1002/acp.3099},
	pages = {226--231},
	number = {2},
	author = {Hunt, Thomas E. and Clark-Carter, David and Sheffield, David},
	urldate = {2023-06-24},
	date = {2015},
}

@inproceedings{bozkir_exploiting_2021,
	title = {Exploiting Object-of-Interest Information to Understand Attention in {VR} Classrooms},
	doi = {10.1109/VR50410.2021.00085},
	pages = {597--605},
	booktitle = {2021 {IEEE} Virtual Reality and 3D User Interfaces ({VR})},
	author = {Bozkir, Efe and Stark, Philipp and Gao, Hong and Hasenbein, Lisa and Hahn, Jens-Uwe and Kasneci, Enkelejda and Göllner, Richard},
	date = {2021-03},
	keywords = {Applied computing-Education-Computer -assisted instruction, Applied computing-Education-Interactive learning environments, Avatars, Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality, Education, Headphones, Human-centered computing-Human computer interaction ({HCI})-Empirical studies in {HCI}, Immersive experience, Pandemics, Three-dimensional displays, Visualization},
}

@article{hawes_effects_2015,
	title = {Effects of mental rotation training on children’s spatial and mathematics performance: A randomized controlled study},
	volume = {4},
	issn = {2211-9493},
	url = {http://www.sciencedirect.com/science/article/pii/S2211949315000083},
	doi = {10.1016/j.tine.2015.05.001},
	pages = {60 -- 68},
	number = {3},
	journaltitle = {Trends in Neuroscience and Education},
	author = {Hawes, Zachary and Moss, Joan and Caswell, Beverly and Poliszczuk, Daniel},
	date = {2015},
	keywords = {Computerized cognitive training, Mathematics education, Mental rotation, {STEM}, Spatial thinking, Spatial training},
}

@article{tomasino_effects_2016,
	title = {Effects of stimulus type and strategy on mental rotation network: An activation likelihood estimation meta-analysis},
	volume = {9},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/article/10.3389/fnhum.2015.00693},
	doi = {10.3389/fnhum.2015.00693},
	pages = {693},
	journaltitle = {Frontiers in Human Neuroscience},
	author = {Tomasino, Barbara and Gremese, Michele},
	date = {2016},
}

@article{larsen_deconstructing_2014,
	title = {Deconstructing mental rotation},
	volume = {40},
	issn = {0096-1523},
	doi = {10.1037/a0035648},
	pages = {1072--1091},
	number = {3},
	journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Larsen, Axel},
	urldate = {2023-09-13},
	date = {2014},
	note = {Publisher: American Psychological Association},
	keywords = {Adult, Attention, Color Perception, Discrimination Learning, Eye Movements, Humans, Imagination, Male, Mental Rotation, Ocular\}, Orientation, Psychophysics, Reaction Time, Saccades, Short Term Memory, Short-Term\}, Visual Memory, Visual\}, eye movements, mental rotation, random walk, visual working memory, \{Fixation, \{Memory, \{Pattern Recognition},
}

@inproceedings{li_convolutional_2021,
	title = {Convolutional Neural Networks for Face Illumination Transfer},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_10},
	series = {Communications in Computer and Information Science},
	pages = {103--120},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Li, Zhonglan and Jin, Xin and Li, Xiaodong and Li, Yannan},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Convolutional neural network ({CNN}), Face image, Illumination transfer},
}

@article{ellis_distortions_1987,
	title = {Distortions of perceived visual out of pictures},
	volume = {42},
	issn = {1532-5962},
	doi = {10.3758/BF03207985},
	pages = {535--544},
	number = {6},
	journaltitle = {Perception \& Psychophysics},
	author = {Ellis, Stephen R. and Smith, Stephen and {McGreevy}, Michael W.},
	urldate = {2023-09-19},
	date = {1987-11-01},
}

@article{just_cognitive_1985,
	title = {Cognitive coordinate systems: accounts of mental rotation and individual differences in spatial ability},
	volume = {92},
	issn = {0033295X},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3deue%26AN%3d508116378%26site%3dehost-live},
	doi = {10.1037/0033-295X.92.2.137},
	shorttitle = {Cognitive coordinate systems},
	pages = {137--172},
	journaltitle = {Psychological Review},
	author = {Just, Marcel Adam and Carpenter, Patricia A.},
	urldate = {2022-02-01},
	date = {1985-04},
	keywords = {Human information processing, Individual differences, Mental rotation, Reaction time, Space perception},
}

@article{gardony_cognitive_2017,
	title = {Cognitive strategies in the mental rotation task revealed by {EEG} spectral power},
	volume = {118},
	issn = {0278-2626},
	url = {https://www.sciencedirect.com/science/article/pii/S0278262616302147},
	doi = {10.1016/j.bandc.2017.07.003},
	pages = {1--18},
	journaltitle = {Brain and Cognition},
	author = {Gardony, Aaron L. and Eddy, Marianna D. and Brunyé, Tad T. and Taylor, Holly A.},
	date = {2017},
}

@article{allison_combined_1996,
	title = {Combined head and eye tracking system for dynamic testing of the vestibular system},
	volume = {43},
	doi = {10.1109/10.541249},
	pages = {1073--1082},
	number = {11},
	journaltitle = {{IEEE} Transactions on Biomedical Engineering},
	author = {Allison, R.S. and Eizenman, M. and Cheung, B.S.K.},
	date = {1996},
}

@online{noauthor_average_nodate,
	title = {Average duration of a single eye blink - Human Homo sapiens - {BNID} 100706},
	url = {https://bionumbers.hms.harvard.edu/bionumber.aspx?id=100706&ver=0},
	urldate = {2023-09-08},
}

@inproceedings{tang_ar_2021,
	title = {{AR} Application Research Based on {ORB}-{SLAM}},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_8},
	series = {Communications in Computer and Information Science},
	pages = {78--88},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Tang, Baihui and Liu, Zhengyi and Cao, Sanxing},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Augmented reality, Feature point description, Feature point extraction, {ORB} feature},
}

@incollection{cooper_chronometric_1973,
	title = {{CHRONOMETRIC} {STUDIES} {OF} {THE} {ROTATION} {OF} {MENTAL} {IMAGES}},
	isbn = {978-0-12-170150-5},
	pages = {75--176},
	booktitle = {Visual Information Processing},
	publisher = {Academic Press},
	author = {Cooper, Lynn A. and Shepard, Roger N.},
	editor = {Chase, {WILLIAM} G.},
	urldate = {2023-09-07},
	date = {1973-01-01},
	doi = {10.1016/B978-0-12-170150-5.50009-3},
}

@inproceedings{vuori_can_2004,
	location = {New York, {NY}, {USA}},
	title = {Can eye movements be quantitatively applied to image quality studies?},
	isbn = {978-1-58113-857-3},
	url = {https://doi.org/10.1145/1028014.1028067},
	doi = {10.1145/1028014.1028067},
	series = {{NordiCHI} '04},
	pages = {335--338},
	booktitle = {Proceedings of the third Nordic conference on Human-computer interaction},
	publisher = {Association for Computing Machinery},
	author = {Vuori, Tero and Olkkonen, Maria and Pölönen, Monika and Siren, Ari and Häkkinen, Jukka},
	urldate = {2022-12-12},
	date = {2004},
	keywords = {eye movements, image quality, psychometric scaling},
}

@article{pittalis_coding_2013,
	title = {Coding and decoding representations of 3D shapes},
	volume = {32},
	issn = {0732-3123},
	doi = {10.1016/j.jmathb.2013.08.004},
	pages = {673--689},
	number = {3},
	journaltitle = {The Journal of Mathematical Behavior},
	author = {Pittalis, Marios and Christou, Constantinos},
	urldate = {2023-06-16},
	date = {2013-09-01},
}

@article{figueredo_assortative_2009,
	title = {Assortative pairing and life history strategy – a cross-cultural study},
	volume = {20},
	doi = {10.1007/s12110-009-9068-2},
	pages = {317--330},
	journaltitle = {Human Nature},
	author = {Figueredo, A. J. and Wolf, P. S. A.},
	date = {2009},
}

@article{hardiess_allocation_2015,
	title = {Allocation of cognitive resources in comparative visual search – Individual and task dependent effects},
	volume = {113},
	issn = {0042-6989},
	doi = {10.1016/j.visres.2015.05.017},
	pages = {71--77},
	journaltitle = {Vision Research},
	author = {Hardiess, Gregor and Mallot, Hanspeter A.},
	urldate = {2023-09-07},
	date = {2015-08-01},
	keywords = {Comparative visual search, Decision making, Gaze movement, Individual trade-off, Working memory},
}

@article{hegarty_ability_2018,
	title = {Ability and sex differences in spatial thinking: What does the mental rotation test really measure?},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-017-1347-z},
	doi = {10.3758/s13423-017-1347-z},
	shorttitle = {Ability and sex differences in spatial thinking},
	pages = {1212--1219},
	number = {3},
	journaltitle = {Psychonomic Bulletin \& Review},
	author = {Hegarty, Mary},
	urldate = {2021-12-21},
	date = {2018-06},
	langid = {english},
}

@article{peters_redrawn_1995,
	title = {A Redrawn Vandenberg and Kuse Mental Rotations Test - Different Versions and Factors That Affect Performance},
	volume = {28},
	issn = {0278-2626},
	url = {https://www.sciencedirect.com/science/article/pii/S0278262685710329},
	doi = {10.1006/brcg.1995.1032},
	pages = {39--58},
	number = {1},
	journaltitle = {Brain and Cognition},
	author = {Peters, M. and Laeng, B. and Latham, K. and Jackson, M. and Zaiyouna, R. and Richardson, C.},
	urldate = {2021-11-24},
	date = {1995-06},
	langid = {english},
}

@article{ganis_new_2015,
	title = {A New Set of Three-Dimensional Shapes for Investigating Mental Rotation Processes: Validation Data and Stimulus Set},
	volume = {3},
	url = {https://openpsychologydata.metajnl.com/articles/10.5334/jopd.ai/#},
	doi = {10.5334/jopd.ai},
	journaltitle = {Journal of Open Psychology Data},
	author = {Ganis, Giorgio and Kievit, Rogier},
	date = {2015},
}

@article{pylyshyn_what_1973,
	title = {What the mind's eye tells the mind's brain: A critique of mental imagery},
	volume = {80},
	issn = {1939-1455},
	doi = {10.1037/h0034650},
	shorttitle = {What the mind's eye tells the mind's brain},
	pages = {1--24},
	number = {1},
	journaltitle = {Psychological Bulletin},
	author = {Pylyshyn, Zenon W.},
	date = {1973},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Cognitive Processes, Imagery, Memory},
}

@article{xue_uncovering_2017,
	title = {Uncovering the cognitive processes underlying mental rotation: an eye-movement study},
	volume = {7},
	rights = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-10683-6},
	doi = {10.1038/s41598-017-10683-6},
	shorttitle = {Uncovering the cognitive processes underlying mental rotation},
	abstract = {Mental rotation is an important paradigm for spatial ability. Mental-rotation tasks are assumed to involve five or three sequential cognitive-processing states, though this has not been demonstrated experimentally. Here, we investigated how processing states alternate during mental-rotation tasks. Inference was carried out using an advanced statistical modelling and data-driven approach – a discriminative hidden Markov model ({dHMM}) trained using eye-movement data obtained from an experiment consisting of two different strategies: (I) mentally rotate the right-side figure to be aligned with the left-side figure and ({II}) mentally rotate the left-side figure to be aligned with the right-side figure. Eye movements were found to contain the necessary information for determining the processing strategy, and the {dHMM} that best fit our data segmented the mental-rotation process into three hidden states, which we termed encoding and searching, comparison, and searching on one-side pair. Additionally, we applied three classification methods, logistic regression, support vector model and {dHMM}, of which {dHMM} predicted the strategies with the highest accuracy (76.8\%). Our study did confirm that there are differences in processing states between these two of mental-rotation strategies, and were consistent with the previous suggestion that mental rotation is discrete process that is accomplished in a piecemeal fashion.},
	pages = {10076},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Xue, Jiguo and Li, Chunyong and Quan, Cheng and Lu, Yiming and Yue, Jingwei and Zhang, Chenggang},
	urldate = {2022-11-25},
	date = {2017-08-30},
	langid = {english},
	keywords = {Cognitive control, Pattern vision, Perception, Saccades},
}

@article{bailenson_use_2008,
	title = {The Use of Immersive Virtual Reality in the Learning Sciences: Digital Transformations of Teachers, Students, and Social Context},
	volume = {17},
	url = {https://doi.org/10.1080/10508400701793141},
	doi = {10.1080/10508400701793141},
	abstract = {This article illustrates the utility of using virtual environments to transform social interaction via behavior and context, with the goal of improving learning in digital environments. We first describe the technology and theories behind virtual environments and then report data from 4 empirical studies. In Experiment 1, we demonstrated that teachers with augmented social perception (i.e., receiving visual warnings alerting them to students not receiving enough teacher eye gaze) were able to spread their attention more equally among students than teachers without augmented perception. In Experiments 2 and 3, we demonstrated that by breaking the rules of spatial proximity that exist in physical space, students can learn more by being in the center of the teacher's field of view (compared to the periphery) and by being closer to the teacher (compared to farther away). In Experiment 4, we demonstrated that inserting virtual co-learners who were either model students or distracting students changed the learning abilities of experiment participants who conformed to the virtual co-learners. Results suggest that virtual environments will have a unique ability to alter the social dynamics of learning environments via transformed social interaction. We would like to thank Roy Pea, Byron Reeves, and the Stanford {LIFE} lab for helpful suggestions and Sandra Okita and Dan Schwartz for suggestions as well as for detailed comments on an earlier draft of this article. This work was supported in part by National Science Foundation Grant 0527377.},
	pages = {102--141},
	number = {1},
	journaltitle = {Journal of the Learning Sciences},
	author = {Bailenson, Jeremy N. and Yee, Nick and Blascovich, Jim and Beall, Andrew C. and Lundblad, Nicole and Jin, Michael},
	date = {2008},
}

@article{lochhead_immersive_2022,
	title = {The Immersive Mental Rotations Test: Evaluating Spatial Ability in Virtual Reality},
	volume = {3},
	issn = {2673-4192},
	url = {https://www.frontiersin.org/articles/10.3389/frvir.2022.820237},
	doi = {10.3389/frvir.2022.820237},
	shorttitle = {The Immersive Mental Rotations Test},
	abstract = {Advancements in extended reality ({XR}) have inspired new uses and users of advanced visualization interfaces, transforming geospatial data visualization and consumption by enabling interactive 3D geospatial data experiences in 3D. Conventional metrics (e.g., mental rotations test ({MRT})) are often used to assess and predict the appropriateness of these visualizations without accounting for the effect the interface has on those metrics. We developed the Immersive {MRT} ({IMRT}) to evaluate the impact that virtual reality ({VR}) based visualizations and 3D virtual environments have on mental rotation performance. Consistent with previous work, the results of our pilot study suggest that mental rotation tasks are performed more accurately and rapidly with stereo 3D stimuli than with 2D images of those stimuli.},
	journaltitle = {Frontiers in Virtual Reality},
	author = {Lochhead, Ian and Hedley, Nick and Çöltekin, Arzu and Fisher, Brian},
	urldate = {2022-09-07},
	date = {2022},
}

@article{beatty_task-evoked_1982,
	title = {Task-evoked pupillary responses, processing load, and the structure of processing resources},
	volume = {91},
	issn = {0033-2909},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpdh%26AN%3d1982-11578-001%26site%3dehost-live},
	doi = {10.1037/0033-2909.91.2.276},
	abstract = {A physiological measure of processing load or 'mental effort' required to perform a cognitive task should accurately reflect within-task, between-task, and between-individual variations in processing demands. The present article reviews all available experimental data on task-evoked pupillary response. It is concluded that the task-evoked pupillary response fulfills the criteria. Alternative explanations are considered and rejected. Implications for neurophysiological and cognitive theories of processing resources are discussed. (47 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {276--292},
	number = {2},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychological Bulletin},
	author = {Beatty, Jackson},
	urldate = {2022-12-08},
	date = {1982-03},
	note = {Publisher: American Psychological Association},
	keywords = {Attention, Brain, Cognitive Processes, Human Channel Capacity, Humans, Mental Processes, Pupil Dilation, Pupillary, Reflex, Reflex, Pupillary, mental load \& processing demands, mental load {\textbackslash}\& processing demands, task-evoked pupillary response},
}

@article{desrocher_stimulus_1995,
	title = {Stimulus and Sex-Differences in Performance of Mental Rotation - Evidence from Event-Related Potentials},
	volume = {28},
	issn = {0278-2626},
	url = {https://www.sciencedirect.com/science/article/pii/S0278262685710317},
	doi = {10.1006/brcg.1995.1031},
	abstract = {We examined stimulus and sex differences in reaction time ({RT}) and event-related potentials ({ERPs}) during mental rotation of letters and abstract designs ({PMA} figures). {RTs} replicated stimulus and angle effects found in previous studies, but no sex differences were found for either set of stimuli. {ERP} latency data showed women began stimulus evaluation earlier, and {PMA} rotations began later over smaller angles, whereas letter rotations began later over larger angles. {ERP} amplitude data replicated hemisphere, electrode, and angle effects found in earlier studies. Amplitude measures also showed greater involvement of anterior cortical areas for evaluation of letter figures and posterior right temporal lobe for {PMA} figures, and greater positivity of womens waveforms than mens over late evaluation and early rotation components.},
	pages = {14--38},
	number = {1},
	journaltitle = {Brain and Cognition},
	shortjournal = {Brain and Cognition},
	author = {Desrocher, M. E. and Smith, M. L. and Taylor, M. J.},
	urldate = {2023-06-16},
	date = {1995-06-01},
	langid = {english},
}

@article{parsons_sex_2004,
	title = {Sex differences in mental rotation and spatial rotation in a virtual environment},
	volume = {42},
	issn = {0028-3932},
	url = {https://www.sciencedirect.com/science/article/pii/S0028393203002380},
	doi = {10.1016/j.neuropsychologia.2003.08.014},
	abstract = {The visuospatial ability referred to as mental rotation has been shown to produce one of the largest and most consistent sex differences, in favor of males, in the cognitive literature. The current study utilizes both a paper-and-pencil version of the mental rotations test ({MRT}) and a virtual environment for investigating rotational ability among 44 adult subjects. Results replicate sex differences traditionally seen on paper-and-pencil measures, while no sex effects were observed in the virtual environment. These findings are discussed in terms of task demands and motor involvement. Sex differences were also seen in the patterns of correlations between rotation tasks and other neuropsychological measures. Current results suggest men may rely more on left hemisphere processing than women when engaged in rotational tasks.},
	pages = {555--562},
	number = {4},
	journaltitle = {Neuropsychologia},
	author = {Parsons, Thomas D. and Larson, Peter and Kratz, Kris and Thiebaux, Marcus and Bluestein, Brendon and Buckwalter, J. Galen and Rizzo, Albert A.},
	date = {2004},
	keywords = {Cognition, Gender differences, Mental rotation task, Neuropsychological assessment, Neuropsychology, Virtual reality},
}

@article{rao_sensorimotor_2018,
	title = {Sensorimotor Learning during a Marksmanship Task in Immersive Virtual Reality},
	volume = {9},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00058},
	doi = {10.3389/fpsyg.2018.00058},
	abstract = {Sensorimotor learning refers to improvements that occur through practice in the performance of sensory-guided motor behaviors. Leveraging novel technical capabilities of an immersive virtual environment, we probed the component kinematic processes that mediate sensorimotor learning. Twenty naïve subjects performed a simulated marksmanship task modeled after Olympic Trap Shooting standards. We measured movement kinematics and shooting performance as participants practiced 350 trials while receiving trial-by-trial feedback about shooting success. Spatiotemporal analysis of motion tracking elucidated the ballistic and refinement phases of hand movements. We found systematic changes in movement kinematics that accompanied improvements in shot accuracy during training, though reaction and response times did not change over blocks. In particular, we observed longer, slower, and more precise ballistic movements that replaced effort spent on corrections and refinement. Collectively, these results leverage developments in immersive virtual reality technology to quantify and compare the kinematics of movement during early learning of full-body sensorimotor orienting.},
	journaltitle = {Frontiers in Psychology},
	author = {Rao, Hrishikesh M. and Khanna, Rajan and Zielinski, David J. and Lu, Yvonne and Clements, Jillian M. and Potter, Nicholas D. and Sommer, Marc A. and Kopper, Regis and Appelbaum, Lawrence G.},
	urldate = {2022-09-12},
	date = {2018},
}

@incollection{pietroszek_raycasting_2018,
	location = {Cham},
	title = {Raycasting in Virtual Reality},
	isbn = {978-3-319-08234-9},
	url = {https://doi.org/10.1007/978-3-319-08234-9_180-1},
	pages = {1--3},
	booktitle = {Encyclopedia of Computer Graphics and Games},
	publisher = {Springer International Publishing},
	author = {Pietroszek, Krzysztof},
	editor = {Lee, Newton},
	date = {2018},
}

@article{kahneman_pupil_1966,
	title = {Pupil diameter and load on memory},
	volume = {154},
	issn = {0036-8075},
	doi = {10.1126/science.154.3756.1583},
	abstract = {During a short-term memory task, pupil diameter is a measure of the amount of material which is under active processing at any time. The pupil dilates as the material is presented and constricts during report. The rate of change of these functions is related to task difficulty.},
	pages = {1583--1585},
	number = {3756},
	journaltitle = {Science (New York, N.Y.)},
	shortjournal = {Science},
	author = {Kahneman, Daniel and Beatty, Jackson},
	date = {1966-12-23},
	pmid = {5924930},
	keywords = {Adult, Female, Humans, Memory, Pupil},
}

@inproceedings{li_performance_2021,
	title = {Performance Evaluation of 3D Light Field Display Based on Mental Rotation Tasks},
	isbn = {978-981-336-549-0},
	doi = {10.1007/978-981-33-6549-0_4},
	series = {Communications in Computer and Information Science},
	pages = {33--44},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Li, Jingwen and Wang, Peng and Chen, Duo and Qi, Shuai and Sang, Xinzhu and Yan, Binbin},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {2D displays, 3D light field display, Mental rotation, Performance evaluation, Viewing angle},
}

@article{bochynska_more_2021,
	title = {More mental rotation time does not imply more mental effort: Pupillary diameters do not change with angular distance},
	volume = {148},
	issn = {0278-2626},
	url = {https://www.sciencedirect.com/science/article/pii/S0278262620302736},
	doi = {10.1016/j.bandc.2020.105670},
	shorttitle = {More mental rotation time does not imply more mental effort},
	abstract = {The ability to mentally rotate objects in space is a fundamental cognitive capacity. Previous studies showed that the time to rotate the image of a figure to match another increases progressively with angular disparity. It remains unclear whether this increase in response time with angular disparity could reflect increased processing operations or more cognitive effort instead of a sustained use of a ‘rotate’ mechanism without a change in workload. We collected response times as well as pupillary responses that index cognitive workload and activity in the brainstem’s locus coeruleus, from a sample of 38 young adults performing a chronometric mental rotations task. The results showed the expected increase in response times but no increase in pupil diameters between 60, 120, and 180 degrees of rotation, suggesting no significant changes in arousal levels when rotating figures near and far. This indicates that during mental rotation the load on cognitive resources remains constant irrespective of angular distance.},
	pages = {105670},
	journaltitle = {Brain and Cognition},
	shortjournal = {Brain and Cognition},
	author = {Bochynska, Agata and Postma, Albert and Vulchanova, Mila and Laeng, Bruno},
	urldate = {2021-10-07},
	date = {2021-03-01},
	langid = {english},
	keywords = {Cognitive effort, Mental rotation, Pupillometry, Spatial cognition, Visual perception},
}

@article{mathot_methods_2022,
	title = {Methods in cognitive pupillometry: Design, preprocessing, and statistical analysis},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-022-01957-7},
	doi = {10.3758/s13428-022-01957-7},
	shorttitle = {Methods in cognitive pupillometry},
	abstract = {Cognitive pupillometry is the measurement of pupil size to investigate cognitive processes such as attention, mental effort, working memory, and many others. Currently, there is no commonly agreed-upon methodology for conducting cognitive-pupillometry experiments, and approaches vary widely between research groups and even between different experiments from the same group. This lack of consensus makes it difficult to know which factors to consider when conducting a cognitive-pupillometry experiment. Here we provide a comprehensive, hands-on guide to methods in cognitive pupillometry, with a focus on trial-based experiments in which the measure of interest is the task-evoked pupil response to a stimulus. We cover all methodological aspects of cognitive pupillometry: experimental design, preprocessing of pupil-size data, and statistical techniques to deal with multiple comparisons when testing pupil-size data. In addition, we provide code and toolboxes (in Python) for preprocessing and statistical analysis, and we illustrate all aspects of the proposed workflow through an example experiment and example scripts.},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Mathôt, Sebastiaan and Vilotijević, Ana},
	urldate = {2022-09-09},
	date = {2022-08-26},
	langid = {english},
	keywords = {Cognitive neuroscience, Cognitive psychology, Data analysis, Experimental design, Pupillometry},
}

@article{vandenberg_mental_1978,
	title = {Mental Rotations, a Group Test of Three-Dimensional Spatial Visualization},
	volume = {47},
	issn = {0031-5125},
	url = {https://doi.org/10.2466/pms.1978.47.2.599},
	doi = {10.2466/pms.1978.47.2.599},
	abstract = {A new paper-and-pencil test of spatial visualization was constructed from the figures used in the Chronometric study of Shepard and Metzler (1971). In large samples, the new test displayed substantial internal consistency (Kuder-Richardson 20 = .88), a test-retest reliability (.83), and consistent sex differences over the entire range of ages investigated. Correlations with other measures indicated strong association with tests of spatial visualization and virtually no association with tests of verbal ability.},
	pages = {599--604},
	number = {2},
	journaltitle = {Perceptual and Motor Skills},
	shortjournal = {Percept Mot Skills},
	author = {Vandenberg, Steven G. and Kuse, Allan R.},
	urldate = {2022-03-08},
	date = {1978},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@inproceedings{salvucci_identifying_2000,
	location = {New York, {NY}, {USA}},
	title = {Identifying Fixations and Saccades in Eye-Tracking Protocols},
	isbn = {1-58113-280-8},
	url = {https://doi.org/10.1145/355017.355028},
	doi = {10.1145/355017.355028},
	pages = {71--78},
	booktitle = {Proceedings of the 2000 Symposium on Eye Tracking Research \& Applications},
	publisher = {{ACM}},
	author = {Salvucci, Dario D. and Goldberg, Joseph H.},
	date = {2000},
	note = {Number of pages: 8
Place: Palm Beach Gardens, {FL}, {USA}},
	keywords = {data analysis algorithms, eye tracking, fixation identification},
}

@article{lundberg_local_2020,
	title = {From local explanations to global understanding with explainable {AI} for trees},
	volume = {2},
	rights = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0138-9},
	doi = {10.1038/s42256-019-0138-9},
	abstract = {Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the {US} population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model’s performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
	pages = {56--67},
	number = {1},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and {DeGrave}, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	urldate = {2022-09-09},
	date = {2020-01},
	langid = {english},
	keywords = {Computer science, Medical research, Software},
}

@article{alghamdi_fixation_2019,
	title = {Fixation Detection with Ray-casting in Immersive Virtual Reality},
	volume = {10},
	issn = {2156-5570},
	url = {https://thesai.org/Publications/ViewPaper?Volume=10&Issue=7&Code=IJACSA&SerialNo=10},
	doi = {10.14569/IJACSA.2019.0100710},
	abstract = {This paper demonstrates the application of a proposed eye fixation detection algorithm to eye movement recorded during eye gaze input within immersive Virtual Reality and compares it with the standard frame-by-frame analysis for validation. Pearson correlations and a sample paired t-test indicated strong correlations between the two analysis methods in terms of fixation duration. The results showed that the principle of eye movement event detection in 2D can be applied successfully in a 3D environment and ensures efficient detection when combined with ray-casting and event time.},
	number = {7},
	journaltitle = {International Journal of Advanced Computer Science and Applications ({IJACSA})},
	author = {Alghamdi, Najood and Alhalabi, Wadee},
	urldate = {2022-05-06},
	date = {2019},
	langid = {english},
}

@article{rayner_eye_2009,
	title = {Eye movements and attention in reading, scene perception, and visual search},
	volume = {62},
	issn = {1747-0226},
	doi = {10.1080/17470210902816461},
	abstract = {Eye movements are now widely used to investigate cognitive processes during reading, scene perception, and visual search. In this article, research on the following topics is reviewed with respect to reading: (a) the perceptual span (or span of effective vision), (b) preview benefit, (c) eye movement control, and (d) models of eye movements. Related issues with respect to eye movements during scene perception and visual search are also reviewed. It is argued that research on eye movements during reading has been somewhat advanced over research on eye movements in scene perception and visual search and that some of the paradigms developed to study reading should be more widely adopted in the study of scene perception and visual search. Research dealing with "real-world" tasks and research utilizing the visual-world paradigm are also briefly discussed.},
	pages = {1457--1506},
	number = {8},
	journaltitle = {Quarterly Journal of Experimental Psychology (2006)},
	shortjournal = {Q J Exp Psychol (Hove)},
	author = {Rayner, Keith},
	date = {2009-08},
	pmid = {19449261},
	keywords = {Attention, Eye Movements, Fixation, Ocular, Humans, Models, Psychological, Neuropsychological Tests, Ocular\}, Pattern Recognition, Visual, Photic Stimulation, Psychological\}, Reaction Time, Reading, Visual Fields, Visual Perception, Visual\}, \{Fixation, \{Models, \{Pattern Recognition},
}

@article{tang_eye_2023,
	title = {Eye movement characteristics in a mental rotation task presented in virtual reality},
	volume = {17},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2023.1143006},
	doi = {10.3389/fnins.2023.1143006},
	abstract = {{IntroductionEye}-tracking technology provides a reliable and cost-effective approach to characterize mental representation according to specific patterns. Mental rotation tasks, referring to the mental representation and transformation of visual information, have been widely used to examine visuospatial ability. In these tasks, participants visually perceive three-dimensional (3D) objects and mentally rotate them until they identify whether the paired objects are identical or mirrored. In most studies, 3D objects are presented using two-dimensional (2D) images on a computer screen. Currently, visual neuroscience tends to investigate visual behavior responding to naturalistic stimuli rather than image stimuli. Virtual reality ({VR}) is an emerging technology used to provide naturalistic stimuli, allowing the investigation of behavioral features in an immersive environment similar to the real world. However, mental rotation tasks using 3D objects in immersive {VR} have been rarely reported.{MethodsHere}, we designed a {VR} mental rotation task using 3D stimuli presented in a head-mounted display ({HMD}). An eye tracker incorporated into the {HMD} was used to examine eye movement characteristics during the task synchronically. The stimuli were virtual paired objects oriented at specific angular disparities (0, 60, 120, and 180°). We recruited thirty-three participants who were required to determine whether the paired 3D objects were identical or mirrored.{ResultsBehavioral} results demonstrated that the response times when comparing mirrored objects were longer than identical objects. Eye-movement results showed that the percent fixation time, the number of within-object fixations, and the number of saccades for the mirrored objects were significantly lower than that for the identical objects, providing further explanations for the behavioral results.{DiscussionIn} the present work, we examined behavioral and eye movement characteristics during a {VR} mental rotation task using 3D stimuli. Significant differences were observed in response times and eye movement metrics between identical and mirrored objects. The eye movement data provided further explanation for the behavioral results in the {VR} mental rotation task.},
	journaltitle = {Frontiers in Neuroscience},
	author = {Tang, Zhili and Liu, Xiaoyu and Huo, Hongqiang and Tang, Min and Qiao, Xiaofeng and Chen, Duo and Dong, Ying and Fan, Linyuan and Wang, Jinghui and Du, Xin and Guo, Jieyi and Tian, Shan and Fan, Yubo},
	urldate = {2023-06-16},
	date = {2023},
}

@inproceedings{zhu_large-scale_2021,
	title = {A Large-Scale {VR} Panoramic Dataset of {QR} Code and Improved Detecting Algorithm},
	doi = {10.1007/978-981-33-6549-0_12},
	series = {Communications in Computer and Information Science},
	pages = {137--148},
	booktitle = {{VR}/{AR} and 3D Displays},
	publisher = {Springer},
	author = {Zhu, Zehao and Zhai, Guangtao and Zhang, Jiahe and Jia, Jun and Yi, Fuwang},
	editor = {Song, Weitao and Xu, Feng},
	date = {2021},
	note = {Place: Singapore},
	keywords = {Deep learning, {QR} code, {VR} video panoramic detection},
}

@article{just_eye_1976,
	title = {Eye fixations and cognitive processes},
	volume = {8},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/0010028576900153},
	doi = {10.1016/0010-0285(76)90015-3},
	abstract = {This paper presents a theoretical account of the sequence and duration of eye fixation during a number of simple cognitive tasks, such as mental rotation, sentence verification, and quantitative comparison. In each case, the eye fixation behavior is linked to a processing model for the task by assuming that the eye fixates the referent of the symbol being operated on.},
	pages = {441--480},
	number = {4},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Just, Marcel Adam and Carpenter, Patricia A},
	urldate = {2023-06-19},
	date = {1976-10-01},
	langid = {english},
}

@article{rokach_ensemble-based_2010,
	title = {Ensemble-based classifiers},
	volume = {33},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-009-9124-7},
	doi = {10.1007/s10462-009-9124-7},
	abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. Researchers from various disciplines such as statistics and {AI} considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.},
	pages = {1--39},
	number = {1},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Rokach, Lior},
	urldate = {2023-09-14},
	date = {2010-02-01},
	langid = {english},
	keywords = {Boosting, Classification, Ensemble of classifiers, Supervised learning},
}

@article{juhola_effect_1985,
	title = {Effect of sampling frequencies on computation of the maximum velocity of saccadic eye movements},
	volume = {53},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00337023},
	doi = {10.1007/BF00337023},
	abstract = {The maximum velocity of saccades is widely used in the clinical assessment of topographical diagnoses. Several methodological factors affect the maximum velocity results. The sampling frequency, the resolution of the analog-digital converter, and filtering of the signal are the most important factors. The sampling frequency should preferabel be higher than 300 Hz. Frequencies below 200 Hz severily deform the velocity profile. The resolution of the analog-digital converter should be 10 bits or more. A theoretical model was constructed for maximum velocity computation. A case study of electro-oculographic and photoelectric recordings confirmed the theoretical model.},
	pages = {67--72},
	number = {2},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Juhola, M. and Jäntti, V. and Pyykkö, I.},
	urldate = {2023-09-12},
	date = {1985-12-01},
	langid = {english},
	keywords = {Clinical Assessment, Maximum Velocity, Sampling Frequency, Theoretical Model, Velocity Profile},
}

@article{kasneci_your_2022,
	title = {Do your eye movements reveal your performance on an {IQ} test? A study linking eye movements and socio-demographic information to fluid intelligence},
	volume = {17},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0264316},
	doi = {10.1371/journal.pone.0264316},
	shorttitle = {Do your eye movements reveal your performance on an {IQ} test?},
	abstract = {Understanding the main factors contributing to individual differences in fluid intelligence is one of the main challenges of psychology. A vast body of research has evolved from the theoretical framework put forward by Cattell, who developed the Culture-Fair {IQ} Test ({CFT} 20-R) to assess fluid intelligence. In this work, we extend and complement the current state of research by analysing the differential and combined relationship between eye-movement patterns and socio-demographic information and the ability of a participant to correctly solve a {CFT} item. Our work shows that a participant’s eye movements while solving a {CFT} item contain discriminative information and can be used to predict whether the participant will succeed in solving the test item. Moreover, the information related to eye movements complements the information provided by socio-demographic data when it comes to success prediction. In combination, both types of information yield a significantly higher predictive performance than each information type individually. To better understand the contributions of features related to eye movements and socio-demographic information to predict a participant’s success in solving a {CFT} item, we employ state-of-the-art explainability techniques and show that, along with socio-demographic variables, eye-movement data. Especially the number of saccades and the mean pupil diameter, significantly increase the discriminating power. The eye-movement features are likely indicative of processing efficiency and invested mental effort. Beyond the specific contribution to research on how eye movements can serve as a means to uncover mechanisms underlying cognitive processes, the findings presented in this work pave the way for further in-depth investigations of factors predicting individual differences in fluid intelligence.},
	pages = {e0264316},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Kasneci, Enkelejda and Kasneci, Gjergji and Trautwein, Ulrich and Appel, Tobias and Tibus, Maike and Jaeggi, Susanne M. and Gerjets, Peter},
	urldate = {2022-06-22},
	date = {2022-03},
	langid = {english},
	keywords = {Attention, Eye movements, Forecasting, Human intelligence, Intelligence, Intelligence tests, Problem solving, Sports},
}

@incollection{gao_digital_2021,
	location = {New York, {NY}, {USA}},
	title = {Digital Transformations of Classrooms in Virtual Reality},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445596},
	abstract = {With rapid developments in consumer-level head-mounted displays and computer graphics, immersive {VR} has the potential to take online and remote learning closer to real-world settings. However, the effects of such digital transformations on learners, particularly for {VR}, have not been evaluated in depth. This work investigates the interaction-related effects of sitting positions of learners, visualization styles of peer-learners and teachers, and hand-raising behaviors of virtual peer-learners on learners in an immersive {VR} classroom, using eye tracking data. Our results indicate that learners sitting in the back of the virtual classroom may have difficulties extracting information. Additionally, we find indications that learners engage with lectures more efficiently if virtual avatars are visualized with realistic styles. Lastly, we find different eye movement behaviors towards different performance levels of virtual peer-learners, which should be investigated further. Our findings present an important baseline for design decisions for {VR} classrooms.},
	pages = {1--10},
	number = {483},
	booktitle = {Proceedings of the 2021 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Gao, Hong and Bozkir, Efe and Hasenbein, Lisa and Hahn, Jens-Uwe and Göllner, Richard and Kasneci, Enkelejda},
	urldate = {2021-10-14},
	date = {2021-05-06},
	keywords = {avatars, education, eye tracking, immersive virtual reality, perception},
}

@article{tapley_investigation_1977,
	title = {An investigation of sex differences in spatial ability: Mental rotation of three-dimensional objects},
	volume = {31},
	issn = {0008-4255},
	doi = {10.1037/h0081655},
	shorttitle = {An investigation of sex differences in spatial ability},
	abstract = {Presented 2-dimensional computer-generated representations of 3-dimensional objects in pairs to 20 male and 20 female right-handed undergraduates. Ss were given 15 sec to make a same-different judgment of the objects, one of which was rotated 0°, 40°, 80°, 120°, or 160° from the other. Ss were also assessed on 2 standard spatial ability tests (the Spatial Relations subtest of the Differential Aptitude Tests, Form L, and the Standardized Road-Map Test of Direction Sense) and a verbal-imagery questionnaire. Analyses of the data showed that men were more accurate than women, and that the slope of the function relating response time to degree of rotation was steeper in women. There was a significant linear relation between performance and the degree of rotation. Rate of rotation and accuracy correlated with the other tests of spatial ability. Response time slope correlated with imagery in men but not in women, suggesting that frequent use of visual imagery was related to mental rotation rate in men, but not in women. There were no clear relations between performance and the strategy Ss professed to use in doing the mental rotation. (French summary) (18 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {122--130},
	journaltitle = {Canadian Journal of Psychology/Revue canadienne de psychologie},
	author = {Tapley, S. Marion and Bryden, M. P.},
	date = {1977},
	note = {Place: Canada
Publisher: University of Toronto Press},
	keywords = {Human Sex Differences, Imagery, Reaction Time, Spatial Orientation (Perception), Spatial Perception},
}

@article{aston-jones_integrative_2005,
	title = {An integrative theory of locus coeruleus-norepinephrine function: Adaptive gain and optimal performance},
	volume = {28},
	issn = {0147-006X},
	doi = {10.1146/annurev.neuro.28.061604.135709},
	shorttitle = {An integrative theory of locus coeruleus-norepinephrine function},
	pages = {403--450},
	journaltitle = {Annual review of neuroscience},
	shortjournal = {Annu Rev Neurosci},
	author = {Aston-Jones, Gary and Cohen, Jonathan D.},
	date = {2005},
	note = {tex.eprinttype: pmid},
	keywords = {Action Potentials, Animals, Brain Mapping, Cognition, Computer\}, Humans, Locus Coeruleus, Norepinephrine, Physiological\}, Systems Integration, Time Factors, \{Adaptation, \{Neural Networks},
}

@article{sipatchin_accuracy_2020,
	title = {Accuracy and precision of the {HTC} {VIVE} {PRO} eye tracking in head-restrained and head-free conditions},
	volume = {61},
	issn = {1552-5783},
	url = {https://api.semanticscholar.org/CorpusID:229076827},
	abstract = {Eye tracking opens up a variety of novel functions in virtual reality. But, many of those functions, such as gaze-contingent content presentation, require a specific level of eye tracking accuracy. The current study evaluates accuracy and precision of gaze estimates over the whole visual field in head-restrained, as well as head-free conditions.    Accuracy and precision were tested at 25 sample positions spanning +- 25° horizontally and +- 25° vertically in two separate conditions: head-restrained, as well as head-free. In the first condition, target position was fixed to headset, and subjects were asked to saccade to an appearing target, while keeping the head still. In the head-free condition targets were positioned in a world-fixed coordinate system, and subjects were motivated to move the gaze together with the head in a natural way towards an appearing target and fixate it. In both conditions the displayed targets were presented in a randomized order with 5 repetitions. Eleven subjects were tested in the first condition; ten subjects were tested in the second condition.In the head-fixed condition accuracy was calculated as the mean offset between the target-eye vector and gaze-target vector. Precision was calculated as the {RMS} (Root Mean Square) of successive measurements. In the head-free condition, phases with head movement were compared to phases of static head in accuracy and precision within each trial.    Average accuracy of the eye-tracker in the head-fixed condition was 4.16°, {SD}: 3.23 while the precision had a mean of 2.17°, {SD}: 0.75. Comparing accuracy and precision horizontally in a one-way {ANOVA}, revealed that accuracy differed between the centre and periphery (F (4,50) = 3.35, p = 0.02). Under head-movement one-way {ANOVA} revealed a significant difference in precision between phases with moving head, compared to phases of static head (F (1,18) = 253.03, p \&lt; 0.0001; meanmoving: 6.21°, {SD}: 0.77; meanstatic 1.80°, {SD}: 0.42).    While providing a powerful tool for many novel functionalities the eye tracking of the {HTC} Pro Eye’s accuracy and precision have to be taken into account in experiment planning, specifically when including tracking in the periphery, or head movements.  This is a 2020 {ARVO} Annual Meeting abstract.},
	pages = {5071},
	number = {7},
	journaltitle = {Investigative Ophthalmology \& Visual Science},
	shortjournal = {Investigative Ophthalmology \& Visual Science},
	author = {Sipatchin, Alexandra and Wahl, Siegfried and Rifai, Katharina},
	date = {2020-06},
}

@inproceedings{agtzidis_360-degree_2019,
	location = {New York, {NY}, {USA}},
	title = {360-Degree Video Gaze Behaviour: A Ground-Truth Data Set and a Classification Algorithm for Eye Movements},
	isbn = {978-1-4503-6889-6},
	doi = {10.1145/3343031.3350947},
	pages = {1007--1015},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Agtzidis, Ioannis and Startsev, Mikhail and Dorr, Michael},
	date = {2019},
	keywords = {eye movement classification, hmd, smooth pursuit},
}

@article{johnston_exploring_2018,
	title = {Exploring Pedagogical Foundations of Existing Virtual Reality Educational Applications: A Content Analysis Study},
	volume = {46},
	url = {https://doi.org/10.1177/0047239517745560},
	doi = {10.1177/0047239517745560},
	abstract = {New virtual reality ({VR}) applications for education appear frequently in the marketplace but rarely contain explicit pedagogies. The research objective of this study was to identify and categorize principles and practices of pedagogy that are evident but not articulated in selected {VR} applications for education. Analysis of public content for the {VR} applications showed most were experiential while others were categorized as discovery learning, constructivism, situated cognition, direct instruction, or unclassified approaches. Educators and {VR} designers could use explicit pedagogical frameworks to support faculty development, construct extended, and congruent curricular options that stimulate reflections, build insights, and insure innovative and measurable outcomes.},
	pages = {414--439},
	number = {4},
	journaltitle = {Journal of Educational Technology Systems},
	author = {Johnston, Elizabeth and Olivas, Gerald and Steele, Patricia and Smith, Cassandra and Bailey, Liston},
	date = {2018},
}

@article{wang_relating_2023,
	title = {Relating visual and pictorial space: Binocular disparity for distance, motion parallax for direction},
	volume = {31},
	doi = {10.1080/13506285.2023.2203528},
	shorttitle = {Relating visual and pictorial space},
	abstract = {Interacting with people and three-dimensional objects depicted on a screen is perceptually different from interacting with them in real life. This difference resides in their corresponding perceptual spaces: The former involves pictorial space, and the latter, visual space. Studies have examined the perceptual geometry of pictorial or visual space, but rarely their connection. The current study connected the two spaces using a pointing task and investigated how binocular disparity and motion parallax affect this connection. In a virtual environment, a pointing virtual character was displayed within a frame and the participants rotated him to point at targets in visual space. What binocular disparity and motion parallax specified was independently manipulated, either the two-dimensional surface or its depicted three-dimensional content. In Experiment 1, we changed the virtual character's distance to the screen and found that binocular disparity determines the distance relationship between visual and pictorial space, but also introduces a relief depth expansion of the perceived virtual character. In Experiment 2, we changed the participants' viewing angle relative to the screen and found that motion parallax determines the directional relationship between visual and pictorial space. We discuss the theoretical and practical implications of our results in the context of video-mediated telecommunication.},
	pages = {1--19},
	journaltitle = {Visual Cognition},
	shortjournal = {Visual Cognition},
	author = {Wang, Xiaoye and Troje, Nikolaus},
	date = {2023-04-24},
}

@article{hasenbein_learning_2022,
	title = {Learning with simulated virtual classmates: Effects of social-related configurations on students’ visual attention and learning experiences in an immersive virtual reality classroom},
	volume = {133},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563222001042},
	doi = {10.1016/j.chb.2022.107282},
	shorttitle = {Learning with simulated virtual classmates},
	abstract = {Immersive virtual reality ({IVR}) provides great potential to experimentally investigate effects of peers on student learning in class and to strategically deploy virtual peer learners to improve learning. The present study examined how three social-related classroom configurations (i.e., students' position in the classroom, visualization style of virtual avatars, and virtual classmates' performance-related behavior) affect students' visual attention toward information presented in the {IVR} classroom using a large-scale eye-tracking data set of N = 274 sixth graders. {ANOVA} results showed that the {IVR} configurations were systematically associated with differences in learners' visual attention on classmates or the instructional content and their overall gaze distribution in the {IVR} classroom (Cohen's d ranging from 0.28 to 2.04 for different {IVR} configurations and gaze features). Gaze-based attention on classmates was negatively related to students' interest in the {IVR} lesson (d = 0.28); specifically, the more boys were among the observed peers, the lower students' situational self-concept (d = 0.24). In turn, gaze-based attention on the instructional content was positively related to students' performance after the {IVR} lesson (d = 0.26). Implications for the future use of {IVR} classrooms in educational research and practice are discussed.},
	pages = {107282},
	journaltitle = {Computers in Human Behavior},
	shortjournal = {Computers in Human Behavior},
	author = {Hasenbein, Lisa and Stark, Philipp and Trautwein, Ulrich and Queiroz, Anna Carolina Muller and Bailenson, Jeremy and Hahn, Jens-Uwe and Göllner, Richard},
	urldate = {2022-12-19},
	date = {2022-08-01},
	langid = {english},
	keywords = {Classroom simulation, Eye-tracking, Immersive virtual reality, Network analysis, Peer effects, Visual attention},
}

@article{slater_enhancing_2016,
	title = {Enhancing Our Lives with Immersive Virtual Reality},
	volume = {3},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/article/10.3389/frobt.2016.00074},
	doi = {10.3389/frobt.2016.00074},
	pages = {74},
	journaltitle = {Frontiers in Robotics and {AI}},
	author = {Slater, Mel and Sanchez-Vives, Maria V.},
	date = {2016},
}

@article{clay_eye_2019,
	title = {Eye Tracking in Virtual Reality},
	volume = {12},
	issn = {1995-8692},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7903250/},
	doi = {10.16910/jemr.12.1.3},
	abstract = {The intent of this paper is to provide an introduction into the bourgeoning field of eye tracking in Virtual Reality ({VR}). {VR} itself is an emerging technology on the consumer market, which will create many new opportunities in research. It offers a lab environment with high immersion and close alignment with reality. An experiment which is using {VR} takes place in a highly controlled environment and allows for a more in-depth amount of information to be gathered about the actions of a subject. Techniques for eye tracking were introduced more than a century ago and are now an established technique in psychological experiments, yet recent development makes it versatile and affordable. In combination, these two techniques allow unprecedented monitoring and control of human behavior in semi-realistic conditions. This paper will explore the methods and tools which can be applied in the implementation of experiments using eye tracking in {VR} following the example of one case study. Accompanying the technical descriptions, we present research that displays the effectiveness of the technology and show what kind of results can be obtained when using eye tracking in {VR}. It is meant to guide the reader through the process of bringing {VR} in combination with eye tracking into the lab and to inspire ideas for new experiments.},
	number = {1},
	journaltitle = {Journal of Eye Movement Research},
	shortjournal = {J Eye Mov Res},
	author = {Clay, Viviane and König, Peter and König, Sabine},
	urldate = {2021-10-13},
	date = {2019},
}

@book{yarbus_eye_1967,
	location = {New York, {NY}, {US}},
	edition = {1},
	title = {Eye Movements and Vision},
	isbn = {978-1-4899-5379-7},
	url = {https://doi.org/10.1007/978-1-4899-5379-7},
	pagetotal = {222},
	publisher = {Springer},
	author = {Yarbus, Alfred L.},
	date = {1967},
	langid = {english},
	keywords = {Medical / Clinical Medicine, Medical / Ophthalmology},
}

@article{bailey_virtual_2019,
	title = {Virtual reality's effect on children's inhibitory control, social compliance, and sharing},
	volume = {64},
	issn = {01933973},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0193397318300315},
	doi = {10.1016/j.appdev.2019.101052},
	abstract = {We compared the eﬀects of diﬀerent immersive technologies on four- to six-year-olds' inhibitory control skills, social compliance (i.e., walking upon request), and sharing (i.e., physical stickers) with a children's media character (Grover from Sesame Street©). Children (N = 52) completed an inhibitory control task, Simon Says, with Grover either via {TV} or {VR}. Children using {VR} were less likely to suppress a dominant motoric response during Simon Says (i.e., not imitating Grover's actions at the appropriate time) compared to children using {TV}. More children in the {VR} condition approached Grover, and they shared a greater number of stickers with Grover compared to the {TV} condition (among those that shared). There were no diﬀerences between conditions for emotional or physical distress or children's enjoyment of the experience. These preliminary ﬁndings suggest that {VR} may elicit diﬀerential cognitive and social responses compared to less immersive technology.},
	pages = {101052},
	journaltitle = {Journal of Applied Developmental Psychology},
	shortjournal = {Journal of Applied Developmental Psychology},
	author = {Bailey, Jakki O. and Bailenson, Jeremy N. and Obradović, Jelena and Aguiar, Naomi R.},
	urldate = {2022-01-14},
	date = {2019-07},
	langid = {english},
}

@article{wraga_implicit_2003,
	title = {Implicit transfer of motor strategies in mental rotation},
	volume = {52},
	issn = {1090-2147},
	doi = {10.1016/S0278-2626(03)00033-2},
	abstract = {Recent research indicates that motor areas are activated in some types of mental rotation. Many of these studies have required participants to perform egocentric transformations of body parts or whole bodies; however, motor activation also has been found with nonbody objects when participants explicitly relate the objects to their hands. The current study (n=16 right-handed males; aged 18-39 yrs) used positron emission tomography ({PET}) to examine whether such egocentric motor strategies can be transferred implicitly from one type of mental rotation to another. Two groups of participants were tested. In the Hand-Object group, participants performed imaginal rotations of pictures of hands; following this, they then made similar judgments of pictures of Shepard-Metzler objects. The Object-Object group performed the rotation task for two sets of Shepard-Metzler objects only. When the second condition in each group (which always required rotating Shepard-Metzler objects) was compared, motor areas (Area 6 and M1) were found to be activated only in the Hand-Object group. These findings suggest that motor strategies can be covertly transferred to imaginal transformations of nonbody objects. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {135--143},
	number = {2},
	journaltitle = {Brain and Cognition},
	author = {Wraga, Maryjane and Thompson, William L. and Alpert, Nathaniel M. and Kosslyn, Stephen M.},
	date = {2003},
	note = {Place: Netherlands
Publisher: Elsevier Science},
	keywords = {Body Awareness, Mental Rotation, Motor Cortex, Motor Processes, Spatial Orientation (Perception), Transfer (Learning)},
}

@software{mathot_python_2022,
	title = {Python {DataMatrix}},
	url = {https://github.com/open-cogsci/python-datamatrix},
	abstract = {An intuitive, Pythonic way to work with tabular data},
	publisher = {Cogsci.nl},
	author = {Mathôt, Sebastiaan},
	urldate = {2022-11-29},
	date = {2022-08-26},
	note = {original-date: 2016-01-03T13:10:59Z},
	keywords = {analysis, data-analysis, data-structures, python, scientific-computing},
}

@article{mathot_simple_2013,
	title = {A simple way to reconstruct pupil size during eye blinks},
	doi = {10.6084/m9.figshare.688001},
	abstract = {Here I describe a simple way to reconstruct pupil-size data during eye blinks. Blinks are detected using a velocity threshold and reconstructed using cubic-spline interpolation. Although it is debatable whether this reconstruction procedure is theoretically meaningful, it has considerable practical benefits during analysis and data presentation, compared to treating blinks as periods of missing data.},
	journaltitle = {{FigShare}},
	shortjournal = {{FigShare}},
	author = {Mathôt, Sebastiaan},
	date = {2013-04-24},
}

@article{shiferaw_stationary_2018,
	title = {Stationary gaze entropy predicts lane departure events in sleep-deprived drivers},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-20588-7},
	doi = {10.1038/s41598-018-20588-7},
	abstract = {Performance decrement associated with sleep deprivation is a leading contributor to traffic accidents and fatalities. While current research has focused on eye blink parameters as physiological indicators of driver drowsiness, little is understood of how gaze behaviour alters as a result of sleep deprivation. In particular, the effect of sleep deprivation on gaze entropy has not been previously examined. In this randomised, repeated measures study, 9 (4 male, 5 female) healthy participants completed two driving sessions in a fully instrumented vehicle (1 after a night of sleep deprivation and 1 after normal sleep) on a closed track, during which eye movement activity and lane departure events were recorded. Following sleep deprivation, the rate of fixations reduced while blink rate and duration as well as saccade amplitude increased. In addition, stationary and transition entropy of gaze also increased following sleep deprivation as well as with amount of time driven. An increase in stationary gaze entropy in particular was associated with higher odds of a lane departure event occurrence. These results highlight how fatigue induced by sleep deprivation and time-on-task effects can impair drivers’ visual awareness through disruption of gaze distribution and scanning patterns.},
	pages = {2220},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Shiferaw, Brook A. and Downey, Luke A. and Westlake, Justine and Stevens, Bronwyn and Rajaratnam, Shantha M. W. and Berlowitz, David J. and Swann, Phillip and Howard, Mark E.},
	urldate = {2023-11-07},
	date = {2018-02-02},
	langid = {english},
	keywords = {Human behaviour, Sleep deprivation},
}

@article{shic_limited_2011,
	title = {Limited activity monitoring in toddlers with autism spectrum disorder},
	volume = {1380},
	issn = {0006-8993},
	url = {https://www.sciencedirect.com/science/article/pii/S0006899310025874},
	doi = {10.1016/j.brainres.2010.11.074},
	series = {The Emerging Neuroscience of Autism Spectrum Disorders},
	abstract = {This study used eye-tracking to examine how 20-month-old toddlers with autism spectrum disorder ({ASD}) (n=28), typical development ({TD}) (n=34), and non-autistic developmental delays ({DD}) (n=16) monitored the activities occurring in a context of an adult–child play interaction. Toddlers with {ASD}, in comparison to control groups, showed less attention to the activities of others and focused more on background objects (e.g., toys). In addition, while all groups spent the same time overall looking at people, toddlers with {ASD} looked less at people's heads and more at their bodies. In {ASD}, these patterns were associated with cognitive deficits and greater autism severity. These results suggest that the monitoring of the social activities of others is disrupted early in the developmental progression of autism, limiting future avenues for observational learning.},
	pages = {246--254},
	journaltitle = {Brain Research},
	shortjournal = {Brain Research},
	author = {Shic, Frederick and Bradshaw, Jessica and Klin, Ami and Scassellati, Brian and Chawarska, Katarzyna},
	urldate = {2024-04-08},
	date = {2011-03-22},
	keywords = {Activity monitoring, Autism, Eye-tracking, Joint attention, Observational learning, Social learning},
}

@article{nahlik_adapting_2022,
	title = {Adapting gaze-transition entropy analysis to compare participants’ problem solving approaches for chemistry word problems},
	volume = {23},
	url = {https://pubs.rsc.org/en/content/articlelanding/2022/rp/d2rp00066k},
	doi = {10.1039/D2RP00066K},
	pages = {714--724},
	number = {3},
	journaltitle = {Chemistry Education Research and Practice},
	author = {Nahlik, Philip and L. Daubenmire, Patrick},
	urldate = {2024-01-21},
	date = {2022},
	langid = {english},
	note = {Publisher: Royal Society of Chemistry},
}

@article{mikhailenko_eye-tracking_2022,
	title = {Eye-Tracking in Immersive Virtual Reality for Education: A Review of the Current Progress and Applications},
	volume = {7},
	issn = {2504-284X},
	url = {https://www.frontiersin.org/articles/10.3389/feduc.2022.697032},
	doi = {https://doi.org/10.3389/feduc.2022.697032},
	shorttitle = {Eye-Tracking in Immersive Virtual Reality for Education},
	abstract = {The concept of using eye-tracking in virtual reality for education has been researched in various fields over the past years. With this review, we aim to discuss the recent advancements and applications in this area, explain the technological aspects, highlight the advantages of this approach and inspire interest in the field. Eye-tracking has already been used in science for many decades and now has been substantially reinforced by the addition of virtual and augmented reality technologies. The first part of the review is a general overview of eye-tracking concepts, technical parts, and their applications. In the second part, the focus shifted toward the application of eye-tracking in virtual reality. The third part, first the description of the recently emerged concept of eye-tracking in virtual reality is given, followed by the current applications to education and studying, which has not been thoroughly described before. We describe the main findings, technological aspects, and advantages of this approach.},
	journaltitle = {Frontiers in Education},
	author = {Mikhailenko, Maria and Maksimenko, Nadezhda and Kurushkin, Mikhail},
	urldate = {2023-03-23},
	date = {2022},
}

@inproceedings{krejtz_entropy_2022,
	location = {New York, {NY}, {USA}},
	title = {Entropy of eye movements while reading code or text},
	isbn = {978-1-4503-9289-1},
	url = {https://dl.acm.org/doi/10.1145/3524488.3527365},
	doi = {10.1145/3524488.3527365},
	series = {{EMIP} '22},
	abstract = {A new gaze-based analysis method is presented based on word span entropy, suitable for comparison of eye movements collected during reading of code or text. Word span entropy is derived from gaze transition entropy but differs in that the transition matrix represents word span instead of gaze transition between Areas Of Interest ({AOIs}). Empirical evidence shows that, as expected, an increase in word span entropy is related to a shorter response time, especially when reading code, showcasing the metric's analytical utility.},
	pages = {8--14},
	booktitle = {Proceedings of the Tenth International Workshop on Eye Movements in Programming},
	publisher = {Association for Computing Machinery},
	author = {Krejtz, Krzysztof and Duchowski, Andrew T. and Wisiecka, Katarzyna and Krejtz, Izabela},
	urldate = {2024-01-21},
	date = {2022-11-28},
	keywords = {code reading, eye tracking, gaze entropy, text reading},
}

@inproceedings{ebeid_real-time_2018,
	location = {New York, {NY}, {USA}},
	title = {Real-time gaze transition entropy},
	isbn = {978-1-4503-5706-7},
	url = {https://dl.acm.org/doi/10.1145/3204493.3208340},
	doi = {10.1145/3204493.3208340},
	series = {{ETRA} '18},
	abstract = {In this video, we introduce a real-time algorithm that computes gaze transition entropy. This approach can be employed in detecting higher level cognitive states such as situation awareness. We first compute fixations using our real-time version of a well established velocity threshold based algorithm. We then compute the gaze transition entropy for a content independent grid of areas of interest in real-time using an update processing window approach. We test for Markov property after each update to test whether Markov assumption holds. Higher entropy corresponds to increased eye movement and more frequent monitoring of the visual field. In contrast, lower entropy corresponds to fewer eye movements and less frequent monitoring. Based on entropy levels, the system could then alert the user accordingly and plausibly offer an intervention. We developed an example application to demonstrate the use of the online calculation of gaze transition entropy in a practical scenario.},
	pages = {1--3},
	booktitle = {Proceedings of the 2018 {ACM} Symposium on Eye Tracking Research \& Applications},
	publisher = {Association for Computing Machinery},
	author = {Ebeid, Islam Akef and Gwizdka, Jacek},
	urldate = {2024-01-21},
	date = {2018-06-14},
	keywords = {driving, eye movements, markov model, shannon entropy, transition matrix, within-person differences},
}

@incollection{bailey_chapter_2017,
	location = {San Diego},
	title = {Chapter 9 - Immersive Virtual Reality and the Developing Child},
	isbn = {978-0-12-809481-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128094815000092},
	abstract = {Immersive virtual reality ({IVR}) pushes the limit of mediated experiences and digital representation by blocking out the physical world, and placing users directly into vivid and personal scenarios. This chapter provides an overview of {IVR} technology, and the way it relates to cognitive development, particularly during early childhood. We provide technological (i.e., tracking, rendering, embodiment of senses) and psychological (i.e., immersion vs. presence) definitions of {IVR}, and describe its unique attributes (i.e., the type of environments and the digital representation users can experience). We discuss how these unique attributes of {IVR} relate to topics of cognitive development such as executive functioning. Finally, we present trends in empirical research on children and {IVR} (i.e., areas of research, ages studied, sample sizes) and provide future research directions.},
	pages = {181--200},
	booktitle = {Cognitive Development in Digital Contexts},
	publisher = {Academic Press},
	author = {Bailey, Jakki O. and Bailenson, Jeremy N.},
	editor = {Blumberg, Fran C. and Brooks, Patricia J.},
	urldate = {2024-01-21},
	date = {2017-01-01},
	doi = {10.1016/B978-0-12-809481-5.00009-2},
	keywords = {Children, Immersive virtual reality, Technology},
}

@article{mangalmurti_using_2020,
	title = {Using virtual reality to define the mechanisms linking symptoms with cognitive deficits in attention deficit hyperactivity disorder},
	volume = {10},
	doi = {10.1038/s41598-019-56936-4},
	number = {529},
	journaltitle = {Scientific Reports},
	author = {Mangalmurti, Aman and Kistler, William D. and Quarrie, Barrington and Sharp, Wendy and Persky, Susan and Shaw, Philip},
	date = {2020},
}

@article{stark_impact_2024,
	title = {The Impact of Presentation Modes on Mental Rotation Processing: A Comparative Analysis of Eye Movements and Performance},
	doi = {10.1038/s41598-024-60370-6},
	journaltitle = {Scientific Reports},
	author = {Stark, Philipp and Bozkir, Efe and Sójka, Weronika and Huff, Markus and Kasneci, Enkelejda and Göllner, Richard},
	date = {2024},
}

@inproceedings{aronson_eye-hand_2018,
	title = {Eye-Hand Behavior in Human-Robot Shared Manipulation},
	abstract = {Shared autonomy systems enhance people’s abilities to perform activities of daily living using robotic manipulators. Recent systems succeed by first identifying their operators’ intentions, typically by analyzing the user’s joystick input. To enhance this recognition, it is useful to characterize people’s behavior while performing such a task. Furthermore, eye gaze is a rich source of information for understanding operator intention. The goal of this paper is to provide novel insights into the dynamics of control behavior and eye gaze in human-robot shared manipulation tasks. To achieve this goal, we conduct a data collection study that uses an eye tracker to record eye gaze during a human-robot shared manipulation activity, both with and without shared autonomy assistance. We process the gaze signals from the study to extract gaze features like saccades, fixations, smooth pursuits, and scan paths. We analyze those features to identify novel patterns of gaze behaviors and highlight where these patterns are similar to and different from previous findings about eye gaze in human-only manipulation tasks. The work described in this paper lays a foundation for a model of natural human eye gaze in human-robot shared manipulation.},
	eventtitle = {2018 13th {ACM}/{IEEE} International Conference on Human-Robot Interaction ({HRI})},
	pages = {4--13},
	booktitle = {2018 13th {ACM}/{IEEE} International Conference on Human-Robot Interaction ({HRI})},
	author = {Aronson, Reuben M. and Santini, Thiago and Kübler, Thomas C. and Kasneci, Enkelejda and Srinivasa, Siddhartha and Admoni, Henny},
	date = {2018-03},
	keywords = {Feature extraction, Pupils, Robot control, Robots, Signal processing algorithms, Task analysis, Tracking, eye gaze, eye tracking, human-robot interaction, nonverbal communication, shared autonomy},
}

@incollection{curtin_network_2018,
	location = {Oxford},
	title = {Network Analysis},
	isbn = {978-0-12-804793-4},
	url = {10.1016/B978-0-12-409548-9.09599-3},
	abstract = {Network analysis holds a prominent place in geographic information systems ({GIS}), which is a reflection of the earliest {GIS} data models that were implemented, its broad usefulness across domain areas, and its potential for future development. A network is a fundamental representation of space which can accurately represent a wide range of relationships. There is a scientific foundation for network analysis in the mathematical subdisciplines of graph theory and topology, along with referencing schemes that allow for geographic locations to be specified by their positions on a network. Within {GIS}, analytical methods for network applications have been developed that allow one to describe and measure networks, associate network events spatially, locate facilities on networks, and examine movement across networks. The field of network spatial statistics is quickly developing ways to infer the significance of network events with regard to their spatial and spatio-temporal properties. Interest in network analysis in {GIS} is high within many applications areas including transportation planning, utilities management, environmental analysis, social network analysis, and many more. Network analysis in {GIS} is an active research area today and is likely to see significant development of new methods for the foreseeable future.},
	pages = {153--161},
	booktitle = {Comprehensive Geographic Information Systems},
	publisher = {Elsevier},
	author = {Curtin, Kevin M.},
	editor = {Huang, Bo},
	date = {2018-01-01},
	keywords = {Flows, {GIScience}, Geocomputation, Geographic information systems ({GIS}), Linear referencing, Migration, Network spatial statistics, Network theory, Quantitative methods, Research methods, Social networks, Spatial databases, Spatial modeling, Spatiotemporal network analysis, Transportation},
}

@article{gollner_its_2018,
	title = {It’s not only who you are but who you are with: High school composition and individuals’ attainment over the life course},
	volume = {29},
	issn = {1467-9280},
	doi = {10.1177/0956797618794454},
	shorttitle = {It’s not only who you are but who you are with},
	abstract = {We examined life-course effects of attending selective schools using a longitudinal study of U.S. high school students begun in 1960 (Ns ranging from 1,952 to 377,015). The effects, measured 11 and 50 years after the initial assessment, differed significantly across the two indicators of school selectivity that were used. School average socioeconomic background was positively related to students’ educational expectations, educational attainment, income, and occupational prestige at the 11-year follow-up (0.15 ≤ β ≤ 0.39; all ps ps {\textless} .05) when schools’ socioeconomic background was controlled for. All associations were mediated by students’ educational expectations. With the exception of income, these effects were consistent 50 years after high school, pointing to the long reach of beneficial learning resources and negative social comparison processes when attending selective schools. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {1785--1796},
	number = {11},
	journaltitle = {Psychological Science},
	author = {Göllner, Richard and Damian, Rodica Ioana and Nagengast, Benjamin and Roberts, Brent W. and Trautwein, Ulrich},
	date = {2018},
	keywords = {Academic Achievement, High School Students, School Based Intervention, School Environment, Student Attitudes},
}

@inproceedings{khamis_vrpursuits_2018,
	location = {New York, {NY}, {USA}},
	title = {{VRPursuits}: Interaction in virtual reality using smooth pursuit eye movements},
	isbn = {978-1-4503-5616-9},
	doi = {10.1145/3206505.3206522},
	booktitle = {Proceedings of the 2018 international conference on advanced visual interfaces},
	publisher = {{ACM}},
	author = {Khamis, Mohamed and Oechsner, Carl and Alt, Florian and Bulling, Andreas},
	date = {2018},
	keywords = {eye tracking, gaze interaction, pursuits, virtual reality},
}

@article{krejtz_eye_2018,
	title = {Eye tracking cognitive load using pupil diameter and microsaccades with fixed gaze},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0203629},
	doi = {10.1371/journal.pone.0203629},
	abstract = {Pupil diameter and microsaccades are captured by an eye tracker and compared for their suitability as indicators of cognitive load (as beset by task difficulty). Specifically, two metrics are tested in response to task difficulty: (1) the change in pupil diameter with respect to inter- or intra-trial baseline, and (2) the rate and magnitude of microsaccades. Participants performed easy and difficult mental arithmetic tasks while fixating a central target. Inter-trial change in pupil diameter and microsaccade magnitude appear to adequately discriminate task difficulty, and hence cognitive load, if the implied causality can be assumed. This paper’s contribution corroborates previous work concerning microsaccade magnitude and extends this work by directly comparing microsaccade metrics to pupillometric measures. To our knowledge this is the first study to compare the reliability and sensitivity of task-evoked pupillary and microsaccadic measures of cognitive load.},
	pages = {e0203629},
	number = {9},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Krejtz, Krzysztof and Duchowski, Andrew T. and Niedzielska, Anna and Biele, Cezary and Krejtz, Izabela},
	urldate = {2022-10-13},
	date = {2018-09-14},
	langid = {english},
	keywords = {Attention, Eye movements, Eyes, Pupil, Questionnaires, Signal filtering, Vision, Working memory},
}

@inproceedings{lang_synthesizing_2018,
	location = {New York, {NY}, {USA}},
	title = {Synthesizing personalized training programs for improving driving habits via virtual reality},
	doi = {10.1109/VR.2018.8448290},
	pages = {297--304},
	booktitle = {2018 {IEEE} conference on virtual reality and 3D user interfaces ({VR})},
	publisher = {{IEEE}},
	author = {Lang, Yining and Wei, Liang and Xu, Fang and Zhao, Yibiao and Yu, Lap-Fai},
	date = {2018},
}

@article{langbehn_blink_2018,
	title = {In the blink of an eye: Leveraging blink-induced suppression for imperceptible position and orientation redirection in virtual reality},
	volume = {37},
	issn = {0730-0301},
	doi = {10.1145/3197517.3201335},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Langbehn, Eike and Steinicke, Frank and Lappe, Markus and Welch, Gregory F. and Bruder, Gerd},
	date = {2018},
	keywords = {eye blinks, psychophysical experiments, redirected walking, virtual reality},
}

@article{oh_systematic_2018,
	title = {A Systematic Review of Social Presence: Definition, Antecedents, and Implications},
	volume = {5},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2018.00114},
	doi = {10.3389/frobt.2018.00114},
	shorttitle = {A Systematic Review of Social Presence},
	abstract = {Social presence, or the feeling of being there with a “real” person, is a crucial component of interactions that take place in virtual reality. This paper reviews the concept, antecedents, and implications of social presence, with a focus on the literature regarding the predictors of social presence. The article begins by exploring the concept of social presence, distinguishing it from two other dimensions of presence—telepresence and self-presence. After establishing the definition of social presence, the article offers a systematic review of 222 separate findings identified from 150 studies that investigate the factors (i.e., immersive qualities, contextual differences, and individual psychological traits) that predict social presence. Finally, the paper discusses the implications of heightened social presence and when it does and does not enhance one’s experience in a virtual environment.},
	journaltitle = {Frontiers in Robotics and {AI}},
	shortjournal = {Front. Robot. {AI}},
	author = {Oh, Catherine S. and Bailenson, Jeremy N. and Welch, Gregory F.},
	urldate = {2024-04-25},
	date = {2018-10-15},
	keywords = {Immersion, Social presence, computer-mediated communication, presence, virtual environments, virtual reality},
}

@article{parong_learning_2018,
	title = {Learning science in immersive virtual reality},
	volume = {110},
	issn = {1939-2176},
	doi = {10.1037/edu0000241},
	abstract = {The goals of the study were (a) to compare the instructional effectiveness of immersive virtual reality ({VR}) versus a desktop slideshow as media for teaching scientific knowledge, and (b) to examine the efficacy of adding a generative learning strategy to a {VR} lesson. In Experiment 1, college students viewed a biology lesson about how the human body works either in immersive {VR} or via a self-directed {PowerPoint} slideshow on a desktop computer. Based on interest theory, it was predicted that students who learned in immersive {VR} would report more positive ratings of interest and motivation and would score higher on a posttest covering material in the lesson. In contrast, based on the cognitive theory of multimedia learning, it was predicted that students who learned with a well-designed slideshow would score higher on a posttest, although they might not report higher levels of interest and motivation. The results showed that students who viewed the slideshow performed significantly better on the posttest than the {VR} group, but reported lower motivation, interest, and engagement ratings. In Experiment 2, students either viewed a segmented {VR} lesson and produced a written summary after each segment or viewed the original, continuous {VR} lesson as in Experiment 1. Students who summarized the lesson after each segment performed significantly better on the posttest and the groups did not differ on reported interest, engagement, and motivation. These results support the cognitive theory of multimedia learning and demonstrate the value of generative learning strategies in immersive {VR} environments. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {785--797},
	number = {6},
	journaltitle = {Journal of Educational Psychology},
	author = {Parong, Jocelyn and Mayer, Richard E.},
	date = {2018},
	keywords = {Learning, Learning Strategies, Multimedia, Science Education, Simulation, Test Construction, Virtual Reality},
}

@article{sporns_graph_2018,
	title = {Graph theory methods: applications in brain networks},
	volume = {20},
	issn = {null},
	url = {https://doi.org/10.31887/DCNS.2018.20.2/osporns},
	doi = {10.31887/DCNS.2018.20.2/osporns},
	shorttitle = {Graph theory methods},
	abstract = {Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.},
	pages = {111--121},
	number = {2},
	journaltitle = {Dialogues in Clinical Neuroscience},
	author = {Sporns, Olaf},
	urldate = {2024-01-11},
	date = {2018-06-30},
	keywords = {connectome, functional {MRI}, graph theory, neuroanatomy, neuroimaging},
}

@incollection{stein_uncanny_2018,
	location = {Cham},
	title = {Uncanny Valley in Virtual Reality},
	isbn = {978-3-319-08234-9},
	url = {https://doi.org/10.1007/978-3-319-08234-9_177-1},
	pages = {1--3},
	booktitle = {Encyclopedia of Computer Graphics and Games},
	publisher = {Springer International Publishing},
	author = {Stein, Christian},
	editor = {Lee, Newton},
	urldate = {2024-04-30},
	date = {2018},
	langid = {english},
}

@article{zhang_continuous_2018,
	title = {Continuous authentication using eye movement response of implicit visual stimuli},
	volume = {1},
	issn = {2474-9567},
	doi = {10.1145/3161410},
	pages = {177:1--177:22},
	number = {4},
	journaltitle = {{ACM} Interact. Mob. Wearable Ubiquitous Technology},
	author = {Zhang, Yongtuo and Hu, Wen and Xu, Weitao and Chou, Chun Tung and Hu, Jiankun},
	date = {2018},
}

@incollection{berkman_presence_2019,
	location = {Cham},
	title = {Presence and Immersion in Virtual Reality},
	isbn = {978-3-319-08234-9},
	url = {https://doi.org/10.1007/978-3-319-08234-9_162-1},
	pages = {1--10},
	booktitle = {Encyclopedia of Computer Graphics and Games},
	publisher = {Springer International Publishing},
	author = {Berkman, Mehmet Ilker and Akan, Ecehan},
	editor = {Lee, Newton},
	urldate = {2024-04-13},
	date = {2019},
	langid = {english},
}

@article{billingsley_systematic_2019,
	title = {A Systematic Literature Review of Using Immersive Virtual Reality Technology in Teacher Education},
	volume = {30},
	issn = {1093-023X},
	url = {https://www.learntechlib.org/p/176261},
	abstract = {Virtual reality ({VR}) technology offers promise in enriching learning opportunities in teacher education programs. The purpose of this study was to investigate the ways immersive virtual reality has been used in teacher-training programs. This systematic review of literature examined eight studies where immersive {VR} was utilized to increase learning opportunities during courses that prepared pre-service teachers or in-service teachers taking advanced coursework in education. Included studies were analyzed in terms of (a) participant description, (b) description of the intervention and purpose of the study, (c) study methodology, (d) dependent variable, and (e) outcomes of the study. Finally, a discussion about the efficacy of using immersive {VR} to prepare teachers is offered that confirms that these types of technologies can enrich and enhance learning opportunities for future teachers, but research in this area is greatly lacking.},
	pages = {65--90},
	number = {1},
	journaltitle = {Journal of Interactive Learning Research},
	author = {Billingsley, Glenna and Smith, Scott and Smith, Shaunna and Meritt, Julia},
	date = {2019},
}

@article{blume_students_2019,
	title = {Do students learn better when seated close to the teacher? A virtual classroom study considering individual levels of inattention and hyperactivity-impulsivity},
	volume = {61},
	issn = {0959-4752},
	doi = {10.1016/j.learninstruc.2018.10.004},
	pages = {138--147},
	journaltitle = {Learning and Instruction},
	author = {Blume, Friederike and Göllner, Richard and Moeller, Korbinian and Dresler, Thomas and Ehlis, Ann-Christine and Gawrilow, Caterina},
	date = {2019},
	keywords = {{ADHD} symptoms, Learning, School, Virtual reality},
}

@inproceedings{bozkir_assessment_2019,
	location = {New York, {NY}, {USA}},
	title = {Assessment of driver attention during a safety critical situation in {VR} to generate {VR}-based training},
	isbn = {978-1-4503-6890-2},
	doi = {10.1145/3343036.3343138},
	booktitle = {{ACM} symposium on applied perception 2019},
	publisher = {{ACM}},
	author = {Bozkir, Efe and Geisler, David and Kasneci, Enkelejda},
	date = {2019},
	keywords = {Driver Assistance, Driver Attention, Driver Training, Eye Tracking, Pedestrian Safety, Virtual Reality},
}

@article{chandrasekera_effect_2019,
	title = {Effect of Degrees of Freedom on the Sense of Presence Generated by Virtual Reality ({VR}) Head-Mounted Display Systems: A Case Study on the Use of {VR} in Early Design Studios},
	volume = {47},
	issn = {0047-2395},
	url = {https://doi.org/10.1177/0047239518824862},
	doi = {10.1177/0047239518824862},
	shorttitle = {Effect of Degrees of Freedom on the Sense of Presence Generated by Virtual Reality ({VR}) Head-Mounted Display Systems},
	pages = {513--522},
	number = {4},
	journaltitle = {Journal of Educational Technology Systems},
	author = {Chandrasekera, Tilanka and Fernando, Kinkini and Puig, Luis},
	urldate = {2024-04-13},
	date = {2019-06-01},
	langid = {english},
}

@article{cheng_case_2019,
	title = {A case study of immersive virtual field trips in an elementary classroom: Students’ learning experience and teacher-student interaction behaviors},
	volume = {140},
	issn = {0360-1315},
	doi = {10.1016/j.compedu.2019.103600},
	pages = {103600},
	journaltitle = {Computers \& Education},
	author = {Cheng, Kun Hung and Tsai, Chin Chung},
	date = {2019},
}

@article{concannon_head-mounted_2019,
	title = {Head-Mounted Display Virtual Reality in Post-secondary Education and Skill Training},
	volume = {4},
	issn = {2504-284X},
	url = {https://www.frontiersin.org/articles/10.3389/feduc.2019.00080},
	doi = {10.3389/feduc.2019.00080},
	abstract = {Background: This review focused on how immersive head-mounted display virtual reality ({VR}) was used in post-secondary level education and skill training, with the aim to better understand its state of the art as found from the literature. While numerous studies describe the use of immersive {VR} within a specific educational setting, they are often standalone events not fully detailed regarding their curricular integration. This review aims to analyse these events, with a focus on immersive {VR}’s incorporation into post-secondary education. Objectives: O1) Review the existing literature on the use of immersive {VR} in post-secondary settings, determining where and how it has been used within each educational discipline. This criterion focused on literature featuring the use of immersive {VR}, due to its influence on a user’s perceived levels of presence and imagination. O2) Identify favourable outcomes from the use of immersive {VR} when compared to other learning methods. O3) Determine the conceptual rationale (purpose) for each implementation of immersive {VR} as found throughout the literature. O4) Identify learning theories and recommendations for the utilization of immersive {VR} in post-secondary education. Methods: A literature review was undertaken with searches of Education Research Complete, {ERIC}, {MEDLINE}, {EMBASE}, {IEEE} Xplore, Scopus and Web of Science: Core Collection to locate reports on the use of immersive {VR} in post-secondary curricula. Results: 119 articles were identified, featuring disciplines across Arts and Humanities, Health Sciences, Military and Aerospace, Science and Technology. 35 out of 38 experiments reported to have found a positive outcome for immersive {VR}, after being compared with a non-immersive platform. Each simulation’s purpose included one or more of the following designations: skill training, convenience, engagement, safety, highlighting, interactivity, team building and suggestion. Recommendations for immersive {VR} in post-secondary education emphasize experiential learning and social constructivist approaches, including student-created virtual environments that are mainly led by the students themselves under team collaboration. Conclusion: Immersive {VR} brings convenient, engaging and interactive alternatives to traditional classroom settings as well as offers additional capability over traditional methods. There is a diverse assortment of educational disciplines that have each attempted to harness the power of this technological medium.},
	journaltitle = {Frontiers in Education},
	shortjournal = {Front. Educ.},
	author = {Concannon, Brendan J. and Esmail, Shaniff and Roduta Roberts, Mary},
	urldate = {2024-04-08},
	date = {2019-08-14},
	keywords = {Education, Immersive technology, Virtual Reality \&amp, educational technologies, head-mounted display ({HMD}), simulation, training},
}

@online{games_unreal_2019,
	title = {Unreal engine, version 4.23.1},
	url = {https://www.unrealengine.com},
	author = {Games, Epic},
	date = {2019},
}

@article{gardner_classroom_2019,
	title = {Classroom Interaction Research: The State of the Art},
	volume = {52},
	url = {https://doi.org/10.1080/08351813.2019.1631037},
	doi = {10.1080/08351813.2019.1631037},
	abstract = {{ABSTRACTThis} article reviews classroom interaction research that uses Conversation Analysis and other interactional approaches. Schools as social institutions are crucial in the modern world, preparing people to live in highly complex societies, from preschool through to tertiary education and beyond. Although classrooms vary significantly internationally, research has focused mainly on schools where classes have a teacher in charge of varying numbers of students. While most research has been on English as an additional language ({EAL}) and other second language classrooms, this article will also review work on classrooms where other school subjects are taught, classrooms with younger children, as well as {CA} work that focuses on learning. Data reported are in English.},
	pages = {212--226},
	number = {3},
	journaltitle = {Research on Language and Social Interaction},
	author = {Gardner, Rod},
	date = {2019},
}

@article{hadnett-hunter_effect_2019,
	title = {The effect of task on visual attention in interactive virtual environments},
	volume = {16},
	issn = {1544-3558},
	doi = {10.1145/3352763},
	number = {3},
	journaltitle = {{ACM} Trans. Appl. Percept.},
	author = {Hadnett-Hunter, Jacob and Nicolaou, George and O'Neill, Eamonn and Proulx, Michael},
	date = {2019},
}

@article{hassoumi_improving_2019,
	title = {Improving eye-tracking calibration accuracy using symbolic regression},
	volume = {14},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0213675},
	doi = {10.1371/journal.pone.0213675},
	abstract = {Eye tracking systems have recently experienced a diversity of novel calibration procedures, including smooth pursuit and vestibulo-ocular reflex based calibrations. These approaches allowed collecting more data compared to the standard 9-point calibration. However, the computation of the mapping function which provides planar gaze positions from pupil features given as input is mostly based on polynomial regressions, and little work has investigated alternative approaches. This paper fills this gap by providing a new calibration computation method based on symbolic regression. Instead of making prior assumptions on the polynomial transfer function between input and output records, symbolic regression seeks an optimal model among different types of functions and their combinations. This approach offers an interesting perspective in terms of flexibility and accuracy. Therefore, we designed two experiments in which we collected ground truth data to compare vestibulo-ocular and smooth pursuit calibrations based on symbolic regression, both using a marker or a finger as a target, resulting in four different calibrations. As a result, we improved calibration accuracy by more than 30\%, with reasonable extra computation time.},
	pages = {e0213675},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Hassoumi, Almoctar and Peysakhovich, Vsevolod and Hurter, Christophe},
	urldate = {2021-10-26},
	date = {2019-03-15},
	langid = {english},
	keywords = {Algorithms, Cameras, Eyes, Polynomials, Pupil, Reflexes, Selection markers, Vision},
}

@inproceedings{iskander_exploring_2019,
	title = {Exploring the Effect of Virtual Depth on Pupil Diameter},
	doi = {10.1109/SMC.2019.8913975},
	abstract = {Virtual and Augmented reality ({VR}/{AR}) are being extensively used in many applications that extends from entertainment, training to rehabilitation and treatment of disorders. Studies on the effects of extended use of {VR} immersion has been performed. However, the change of pupil diameter with the change of {VR} simulated depth has not been investigated. Pupil dilation is an indicative measure of cognitive overload. In this paper, we investigate the relationship between {VR} simulated depth and the pupil diameter change. Results showed a significant difference in pupil diameter change with simulated depth and also a strong negative correlation. This indicates that as the depth of the {VR} object increase (distance from the {VR} user increase), the {VR} user's pupil diameter decreases. These results show that change in pupil diameter can be an indicative of change in the depth of the observed virtual object. This can be an effective {VR}/{AR} scene scanning and understanding tool.},
	pages = {1849--1854},
	booktitle = {2019 {IEEE} International Conference on Systems, Man and Cybernetics ({SMC})},
	author = {Iskander, J. and Attia, M. and Saleh, K. and Abobakr, A. and Nahavandi, D. and Hossny, M. and Nahavandi, S.},
	date = {2019},
	keywords = {Brightness, Calibration, Correlation, Headphones, Pupil diameter, Three-dimensional displays, Tools, {VR} immersion, {VR} object, {VR} simulated depth, {VR} user, Visualization, augmented reality, observed virtual object, pupil diameter change, rehabilitation, simulated depth, virtual depth, virtual reality},
}

@article{lee_motion_2019,
	title = {Motion sickness prediction in stereoscopic videos using 3D convolutional neural networks},
	volume = {25},
	doi = {10.1109/TVCG.2019.2899186},
	pages = {1919--1927},
	number = {5},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Lee, Tae Min and Yoon, Jong-Chul and Lee, In-Kwon},
	date = {2019},
}

@article{peng_meta-analysis_2019,
	title = {A meta-analysis on the relation between fluid intelligence and reading/mathematics: Effects of tasks, age, and social economics status},
	volume = {145},
	issn = {1939-1455},
	doi = {10.1037/bul0000182},
	shorttitle = {A meta-analysis on the relation between fluid intelligence and reading/mathematics},
	abstract = {This study aimed to determine the relations between fluid intelligence (Gf) and reading/mathematics and possible moderators. A meta-analysis of 680 studies involving 793 independent samples and more than 370,000 participants found that Gf was moderately related to reading, r = .38, 95\% {CI} [.36, .39], and mathematics, r = .41, 95\% {CI} [.39, 44]. Synthesis on the longitudinal correlations showed that Gf and reading/mathematics predicted each other in the development even after controlling for initial performance. Moderation analyses revealed the following findings: (a) Gf showed stronger relations to mathematics than to reading, (b) within reading or mathematics, Gf showed stronger relations to complex skills than to foundational skills, (c) the relations between Gf and reading/mathematics increased with age, and (d) family social economic status ({SES}) mostly affected the relations between Gf and reading/mathematics in the early development stage. These findings, taken together, are partially in line with the investment theory but are more in line with the intrinsic cognitive load theory, mutualism theory, and the gene–{SES} interaction hypothesis of cognition and learning. More importantly, these findings imply an integration model of these theories from an educational and developmental perspective: Children may rely on Gf to learn reading and mathematics early on, when high family {SES} can boost the effects of Gf on reading/mathematics performance. As children receive more formal schooling and gain more learning experiences, their reading and mathematics improvement may promote their Gf development. During development, the negative effects of low family {SES} on the relations between Gf and reading/mathematics may be offset by education/learning experiences. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {189--236},
	journaltitle = {Psychological Bulletin},
	author = {Peng, Peng and Wang, Tengfei and Wang, {CuiCui} and Lin, Xin},
	date = {2019},
	keywords = {Age Differences, Childhood Development, Intelligence, Mathematical Ability, Mathematics, Reading, Reading Ability, Reading Skills, Socioeconomic Status, Theories},
}

@article{roberts_assessing_2019,
	title = {Assessing the suitability of virtual reality for psychological testing},
	volume = {31},
	issn = {1939-134X},
	doi = {10.1037/pas0000663},
	abstract = {Virtual reality ({VR}) is rapidly becoming an inexpensive, mainstream technology. {VR} technology is superambulatory as it allows participants to be examined under standardized environments and tests anywhere. In addition, it can test participants in different virtual spaces, including environments that are unsafe, inaccessible, costly or difficult to set up, or even nonexistent. We summarize the benefits and potential problems of {VR} technology, but we also move beyond theoretical approaches and present a customizable, open-source {VR} system ({PSY}-{VR}) that allows scalable psychological testing in modifiable {VR} environments. This system allows users to modify the environment using a simple graphical interface, without programming expertise. Moreover, as a proof-of-concept, we compare responses in a typical Flanker task between a real laboratory and a painstakingly matched virtual laboratory. Results indicate that the {VR} responses are comparable to real life testing, demonstrating the utility of {VR} for psychological assessment studies. The predicted rapid advancement of {VR} immersive technologies, as well the ease of their integration with physiological metrics ensures that {VR}-based assessment will be the modus operandi of psychological assessment in the future. This will allow controllable, low-cost assessment on a global scale. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {318--328},
	number = {3},
	journaltitle = {Psychological Assessment},
	author = {Roberts, Adam C. and Yeap, You Win and Seah, Hock Soon and Chan, Elliot and Soh, Chee-Kiong and Christopoulos, George I.},
	date = {2019},
	keywords = {Construct Validity, Environmental Psychology, Psychological Assessment, Standardized Tests, Test Reliability, Virtual Reality},
}

@inproceedings{sidenmark_gaze_2019,
	location = {New York, {NY}, {USA}},
	title = {Gaze behaviour on interacted objects during hand interaction in virtual reality for eye tracking calibration},
	isbn = {978-1-4503-6709-7},
	doi = {10.1145/3314111.3319815},
	booktitle = {Proceedings of the 11th {ACM} symposium on eye tracking research \& applications},
	publisher = {{ACM}},
	author = {Sidenmark, Ludwig and Lundström, Anders},
	date = {2019},
	keywords = {calibration, empirical study, eye tracking, hand-eye coordination, virtual reality},
}

@article{siew_cognitive_2019,
	title = {Cognitive Network Science: A Review of Research on Cognition through the Lens of Network Representations, Processes, and Dynamics},
	volume = {2019},
	issn = {1076-2787},
	url = {https://www.hindawi.com/journals/complexity/2019/2108423/},
	doi = {10.1155/2019/2108423},
	shorttitle = {Cognitive Network Science},
	abstract = {Network science provides a set of quantitative methods to investigate complex systems, including human cognition. Although cognitive theories in different domains are strongly based on a network perspective, the application of network science methodologies to quantitatively study cognition has so far been limited in scope. This review demonstrates how network science approaches have been applied to the study of human cognition and how network science can uniquely address and provide novel insight on important questions related to the complexity of cognitive systems and the processes that occur within those systems. Drawing on the literature in cognitive network science, with a focus on semantic and lexical networks, we argue three key points. (i) Network science provides a powerful quantitative approach to represent cognitive systems. (ii) The network science approach enables cognitive scientists to achieve a deeper understanding of human cognition by capturing how the structure, i.e., the underlying network, and processes operating on a network structure interact to produce behavioral phenomena. (iii) Network science provides a quantitative framework to model the dynamics of cognitive systems, operationalized as structural changes in cognitive systems on different timescales and resolutions. Finally, we highlight key milestones that the field of cognitive network science needs to achieve as it matures in order to provide continued insights into the nature of cognitive structures and processes.},
	pages = {e2108423},
	journaltitle = {Complexity},
	author = {Siew, Cynthia S. Q. and Wulff, Dirk U. and Beckage, Nicole M. and Kenett, Yoed N.},
	urldate = {2024-04-29},
	date = {2019-06-17},
	langid = {english},
}

@inproceedings{simeone_live_2019,
	location = {New York, {NY}, {USA}},
	title = {{LIVE}: The human role in learning in immersive virtual environments},
	isbn = {978-1-4503-6975-6},
	doi = {10.1145/3357251.3357590},
	booktitle = {Symposium on spatial user interaction},
	publisher = {{ACM}},
	author = {Simeone, Adalberto L. and Speicher, Marco and Molnar, Andreea and Wilde, Adriana and Daiber, Florian},
	date = {2019},
}

@misc{alves_cognitive_2020,
	title = {Cognitive Processing dissociation by mental effort manipulation in long demanding tasks},
	url = {https://www.biorxiv.org/content/10.1101/2020.04.25.060814v1},
	doi = {10.1101/2020.04.25.060814},
	abstract = {Individuals uses cognitive resources to modulate performance in demanding tasks and a non-invasive and reliable way of measuring mental effort is pupillometry. This study aimed to test the mental effort related to different processing systems in long tasks with controlled and automatic demands. We conducted two experiments with healthy subjects: in Experiment 1 (n=15), using a metronome to ensure control on task pace, participants performed a serial number generation task (Counting; little to no effort tasks), a random number generation ({RNG}; effortful tasks), and no task (Unfilled interval; no effort at all). In experiment 2, (n=15) participants performed counting tasks with or without additional intermediary beeps produced by a metronome to assess the effect of a possible increase in effort demanded by the distractors. Experiment 1 showed differences between unfilled interval, counting and {RNG}. Experiment 2 showed that the intermediate beep made the counting tasks more demanding than the normal counting tasks. Notable in both experiments was the tendency of participants to demand mental effort at the beginning of the trial. These results indicate that previously effortless automatic tasks can become controlled, or at least more demanding, with a simple experimental manipulation. They also reveal that tasks that require mental effort over a long period will demand more than automatic ones, but even so the peak of this demand is in the initial trial period. Moreover, they reveal the high sensitivity of pupillometry for the measurement of mental effort employing different processing systems and cognitive resource modulation.},
	publisher = {{bioRxiv}},
	author = {Alves, Marcus Vinicius and Tassini, Susanny and Aedo-Jury, Felipe and Bueno, Orlando F. A.},
	urldate = {2023-01-10},
	date = {2020-04-27},
	langid = {english},
	note = {Pages: 2020.04.25.060814
Section: New Results},
}

@inproceedings{batmaz_touch_2020,
	title = {Touch the Wall: Comparison of Virtual and Augmented Reality with Conventional 2D Screen Eye-Hand Coordination Training Systems},
	doi = {10.1109/VR46266.2020.00037},
	shorttitle = {Touch the Wall},
	abstract = {Previous research on eye-hand coordination training systems has investigated user performance on a wall, 2D touchscreens, and in Virtual Reality ({VR}). In this paper, we designed an eye-hand coordination reaction test to investigate and compare user performance in three different virtual environments ({VEs}) – {VR}, Augmented Reality ({AR}), and a 2D touchscreen. {VR} and {AR} conditions also included two feedback conditions – mid-air and passive haptics. Results showed that compared to {AR}, participants were significantly faster and made fewer errors both in 2D and {VR}. However, compared to {VR} and {AR}, throughput performance of the participants was significantly higher in the 2D touchscreen condition. No significant differences were found between the two feedback conditions. The results show the importance of assessing precision and accuracy in eye-hand coordination training and suggest that it is currently not advisable to use {AR} headsets in such systems.},
	eventtitle = {2020 {IEEE} Conference on Virtual Reality and 3D User Interfaces ({VR})},
	pages = {184--193},
	booktitle = {2020 {IEEE} Conference on Virtual Reality and 3D User Interfaces ({VR})},
	author = {Batmaz, Anil Ufuk and Mutasim, Aunnoy K and Malekmakan, Morteza and Sadr, Elham and Stuerzlinger, Wolfgang},
	date = {2020-03},
	keywords = {Haptic interfaces, Human Computer Interaction ({HCI}), Human-centered computing, Mathematical model, Pointing, Task analysis, Throughput, Touch screens, Training, Two dimensional displays, Virtual Reality},
}

@misc{bozkir_differential_2020,
	title = {Differential privacy for eye tracking with temporal correlations},
	url = {https://arxiv.org/abs/2002.08972},
	author = {Bozkir, Efe and Günlü, Onur and Fuhl, Wolfgang and Schaefer, Rafael F. and Kasneci, Enkelejda},
	date = {2020},
}

@inproceedings{bozkir_privacy_2020,
	location = {New York, {NY}, {USA}},
	title = {Privacy preserving gaze estimation using synthetic images via a randomized encoding based framework},
	isbn = {978-1-4503-7134-6},
	doi = {10.1145/3379156.3391364},
	booktitle = {{ACM} symposium on eye tracking research and applications},
	publisher = {{ACM}},
	author = {Bozkir, Efe and Ünal, Ali Burak and Akgün, Mete and Kasneci, Enkelejda and Pfeifer, Nico},
	date = {2020},
	keywords = {eye tracking, gaze estimation, human computer interaction, privacy preserving machine learning, randomized encoding},
}

@inproceedings{chaudhary_privacy-preserving_2020,
	location = {New York, {NY}, {USA}},
	title = {Privacy-preserving eye videos using rubber sheet model},
	isbn = {978-1-4503-7134-6},
	doi = {10.1145/3379156.3391375},
	booktitle = {{ACM} symposium on eye tracking research and applications},
	publisher = {{ACM}},
	author = {Chaudhary, Aayush Kumar and Pelz, Jeff B.},
	date = {2020},
}

@article{cloude_quantifying_2020,
	title = {Quantifying scientific thinking using multichannel data with crystal island: Implications for individualized game-learning analytics},
	volume = {5},
	issn = {2504-284X},
	doi = {10.3389/feduc.2020.572546},
	pages = {217},
	journaltitle = {Frontiers in Education},
	author = {Cloude, Elizabeth B. and Dever, Daryn A. and Wiedbusch, Megan D. and Azevedo, Roger},
	date = {2020},
}

@misc{fuhl_reinforcement_2020,
	title = {Reinforcement learning for the privacy preservation and manipulation of eye tracking data},
	url = {https://arxiv.org/abs/2002.06806},
	author = {Fuhl, Wolfgang and Bozkir, Efe and Kasneci, Enkelejda},
	date = {2020},
}

@article{haj_pupil_2020,
	title = {Pupil Dilation as an Index of Verbal Fluency},
	volume = {133},
	issn = {0002-9556},
	url = {https://www.jstor.org/stable/10.5406/amerjpsyc.133.4.0501},
	doi = {10.5406/amerjpsyc.133.4.0501},
	abstract = {Verbal fluency tasks are widely used as a neuropsychological test of language production. We assessed pupil dilation during a verbal fluency task and during a control task. On the verbal fluency task, we asked 45 healthy participants (mean age = 23.55 years) to generate as many words as possible beginning with the letter “P,” whereas on the control task we asked them to count aloud. In both tasks we recorded pupil dilation with eye-tracking glasses. Results demonstrated that, compared with counting, verbal fluency resulted in a larger pupil dilation. The larger pupil dilation observed during verbal fluency compared with counting can be attributed to the cognitive load of verbal fluency, which involves both linguistic processing and executive function. By highlighting how verbal fluency can increase pupil dilation, our findings pave the way for the physiological assessment of verbal processing in healthy and pathological populations.},
	pages = {501--507},
	number = {4},
	journaltitle = {The American Journal of Psychology},
	author = {Haj, Mohamad El and Lenoble, Quentin and Allain, Philippe},
	urldate = {2021-07-14},
	date = {2020},
}

@inproceedings{kangas_gaze_2020,
	location = {New York, {NY}, {USA}},
	title = {Gaze Tracker Accuracy and Precision Measurements in Virtual Reality Headsets},
	isbn = {978-1-4503-7581-8},
	url = {https://doi.org/10.1145/3382507.3418816},
	doi = {10.1145/3382507.3418816},
	series = {{ICMI} '20},
	abstract = {To effectively utilize a gaze tracker in user interaction it is important to know the quality of the gaze data that it is measuring. We have developed a method to evaluate the accuracy and precision of gaze trackers in virtual reality headsets. The method consists of two software components. The first component is a simulation software that calibrates the gaze tracker and then performs data collection by providing a gaze target that moves around the headset's field-of-view. The second component makes an off-line analysis of the logged gaze data and provides a number of measurement results of the accuracy and precision. The analysis results consist of the accuracy and precision of the gaze tracker in different directions inside the virtual 3D space. Our method combines the measurements into overall accuracy and precision. Visualizations of the measurements are created to see possible trends over the display area. Results from selected areas in the display are analyzed to find out differences between the areas (for example, the middle/outer edge of the display or the upper/lower part of display).},
	pages = {640--644},
	booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
	publisher = {Association for Computing Machinery},
	author = {Kangas, Jari and Koskinen, Olli and Raisamo, Roope},
	date = {2020},
	keywords = {accuracy measurement, eye tracking, precision, virtual reality},
}

@article{meng_eye-dominance-guided_2020,
	title = {Eye-dominance-guided foveated rendering},
	volume = {26},
	doi = {10.1109/TVCG.2020.2973442},
	pages = {1972--1980},
	number = {5},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Meng, Xiaoxu and Du, Ruofei and Varshney, Amitabh},
	date = {2020},
}

@article{pelagatti_closer_2020,
	title = {A closer look at the timecourse of mind wandering: Pupillary responses and behaviour},
	volume = {15},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7192427/},
	doi = {10.1371/journal.pone.0226792},
	shorttitle = {A closer look at the timecourse of mind wandering},
	abstract = {Mind wandering ({MW}) refers to the shift of attention away from a primary task and/or external environment towards thoughts unrelated to the task. Recent evidence has shown that pupillometry can be used as an objective marker of the onset and maintenance of externally-driven {MW} episodes. In the present study we aimed to further investigate pupillary changes associated with the onset and duration of self-reported {MW} episodes. We used a modified version of the joint behavioural-pupillometry paradigm we recently introduced. Participants were asked to perform a monotonous vigilance task which was intermixed with task-irrelevant cue-phrases (visually presented verbal cues); they were instructed to interrupt the task whenever a thought came to mind (self-caught method) and to indicate the trigger of their thought, if any. We found systematic pupil dilation after the presentation of verbal cues reported to have triggered {MW}, compared with other verbal cues presented during a supposedly on-task period (i.e., the period immediately following the resuming of the task after a self-caught interruption and {MW} report). These results confirm that pupil diameter is sensitive to the changes associated with the onset of {MW} and its unfolding over time. Moreover, by computing the latency between the trigger presentation and the task interruption (self-catch), we could also estimate the duration of {MW} episodes triggered by verbal cues. However, a high variability was found, implying very large inter-event variability, which could not be explained by any of the {MW} properties we acquired (including: temporal focus, specificity, emotional valence). Our behavioural and pupillometry findings stress the need for objective measures about the temporal unfolding of {MW} (while most studies focus on arbitrary time-window preceding self-reports of {MW}).},
	pages = {e0226792},
	number = {4},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} One},
	author = {Pelagatti, Claudia and Binda, Paola and Vannucci, Manila},
	urldate = {2023-01-03},
	date = {2020-04-30},
}

@inproceedings{schmitz_directing_2020,
	location = {New York, {NY}, {USA}},
	title = {Directing versus attracting attention: Exploring the effectiveness of central and peripheral cues in panoramic videos},
	doi = {10.1109/VR46266.2020.00024},
	pages = {63--72},
	booktitle = {2020 {IEEE} conference on virtual reality and 3D user interfaces ({VR})},
	publisher = {{IEEE}},
	author = {Schmitz, Anastasia and {MacQuarrie}, Andrew and Julier, Simon and Binetti, Nicola and Steed, Anthony},
	date = {2020},
}

@article{sharma_eye-tracking_2020,
	title = {Eye-tracking and artificial intelligence to enhance motivation and learning},
	volume = {7},
	issn = {2196-7091},
	url = {https://doi.org/10.1186/s40561-020-00122-x},
	doi = {10.1186/s40561-020-00122-x},
	abstract = {The interaction with the various learners in a Massive Open Online Course ({MOOC}) is often complex. Contemporary {MOOC} learning analytics relate with click-streams, keystrokes and other user-input variables. Such variables however, do not always capture users’ learning and behavior (e.g., passive video watching). In this paper, we present a study with 40 students who watched a {MOOC} lecture while their eye-movements were being recorded. We then proposed a method to define stimuli-based gaze variables that can be used for any kind of stimulus. The proposed stimuli-based gaze variables indicate students’ content-coverage (in space and time) and reading processes (area of interest based variables) and attention (i.e., with-me-ness), at the perceptual (following teacher’s deictic acts) and conceptual levels (following teacher discourse). In our experiment, we identified a significant mediation effect of the content coverage, reading patterns and the two levels of with-me-ness on the relation between students’ motivation and their learning performance. Such variables enable common measurements for the different kind of stimuli present in distinct {MOOCs}. Our long-term goal is to create student profiles based on their performance and learning strategy using stimuli-based gaze variables and to provide students gaze-aware feedback to improve overall learning process. One key ingredient in the process of achieving a high level of adaptation in providing gaze-aware feedback to the students is to use Artificial Intelligence ({AI}) algorithms for prediction of student performance from their behaviour. In this contribution, we also present a method combining state-of-the-art {AI} technique with the eye-tracking data to predict student performance. The results show that the student performance can be predicted with an error of less than 5\%.},
	pages = {13},
	number = {1},
	journaltitle = {Smart Learning Environments},
	shortjournal = {Smart Learning Environments},
	author = {Sharma, Kshitij and Giannakos, Michail and Dillenbourg, Pierre},
	urldate = {2022-07-12},
	date = {2020-04-26},
	keywords = {Deep learning, Eye-tracking, Learning, {MOOCs}, Massive open online courses, Motivation, Multimodal analytics, Video based learning},
}

@article{shebane_developing_2020,
	title = {Developing and Exploring the Use of Virtual Reality Learning System to Teach Mathematics Toward Minimizing Failure Rate},
	volume = {39},
	issn = {0731-9258},
	url = {https://www.learntechlib.org/p/210973},
	abstract = {Virtual Reality ({VR}) is an artificial environment that is created with software and presented to the user in such a way that the user suspends belief and accepts it as a real environment. Users interact with {VR} applications through on-screen toggles or with optional additional devices such as controllers, keyboard and mouse. However, when it comes to mobile {VR} it is ideal to keep it simple and not add unnecessary content on the {VR} World as this may overwhelm the phones’ {CPU} and may cause lag and jitter of the application. Virtual Reality can be used for many things such as games, simulation and learning. In this paper, we use prototyping methodology to develop a system that make use of {VR} to teach mathematics to students in lower grades, by teaching mathematical formulas in a virtual classroom. Incorporation of Virtual Reality ({VR}) technology into educational experiences include better...},
	pages = {125--147},
	number = {2},
	journaltitle = {Journal of Computers in Mathematics and Science Teaching},
	author = {Shebane, Treasure and Dehinbo, Johnson},
	date = {2020-04},
}

@article{stranc_scanpath_2020,
	title = {Scanpath Analysis of Student Attention During Problem Solving with Worked Examples},
	volume = {12164},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334687/},
	doi = {10.1007/978-3-030-52240-7_56},
	abstract = {We report on the analysis of scanpath data captured by an eye tracker as students solved problems with access to worked examples. Our work makes two contributions: (1) it reports on scanpath analysis using the {MultiMatch} tool, (2) it investigates how type of problem-example similarity and assistance influenced attention patterns captured by scanpaths. We show that both problem-example similarity and type of assistance impact scanpaths.},
	pages = {306--311},
	journaltitle = {Artificial Intelligence in Education},
	shortjournal = {Artificial Intelligence in Education},
	author = {Stranc, Samantha and Muldner, Kasia},
	urldate = {2024-04-26},
	date = {2020-06-10},
}

@inproceedings{sumer_automated_2020,
	location = {Palo Alto, {CA}, {USA}},
	title = {Automated anonymisation of visual and audio data in classroom studies},
	booktitle = {The workshops of the thirty-forth {AAAI} conference on artificial intelligence},
	publisher = {{AAAI} Press},
	author = {Sümer, Ömer and Gerjets, Peter and Trautwein, Ulrich and Kasneci, Enkelejda},
	date = {2020},
}

@inproceedings{wiedbusch_modeling_2020,
	location = {New York, {NY}, {USA}},
	title = {Modeling metacomprehension monitoring accuracy with eye gaze on informational content in a multimedia learning environment},
	isbn = {978-1-4503-7133-9},
	doi = {10.1145/3379155.3391329},
	booktitle = {{ACM} symposium on eye tracking research and applications},
	publisher = {{ACM}},
	author = {Wiedbusch, Megan D. and Azevedo, Roger},
	date = {2020},
}

@inproceedings{zhang_eth-xgaze_2020,
	location = {Cham, Switzerland},
	title = {{ETH}-{XGaze}: a large scale dataset for gaze estimation under extreme head pose and gaze variation},
	doi = {10.1007/978-3-030-58558-7_22},
	pages = {365--381},
	booktitle = {Computer vision – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhang, Xucong and Park, Seonwook and Beeler, Thabo and Bradley, Derek and Tang, Siyu and Hilliges, Otmar},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	date = {2020},
}

@article{appel_cross-task_2021,
	title = {Cross-task and Cross-participant Classification of Cognitive Load in an Emergency Simulation Game},
	issn = {1949-3045},
	doi = {10.1109/TAFFC.2021.3098237},
	abstract = {Assessment of cognitive load is a major step towards adaptive interfaces. However, non-invasive assessment is rather subjective as well as task specific and generalizes poorly, mainly due to methodological limitations. Additionally, it heavily relies on performance data like game scores or test results. In this study, we present an eye-tracking approach that circumvents these shortcomings and allows for effective generalizing across participants and tasks. First, we established classifiers for predicting cognitive load individually for a typical working memory task (n-back), which we then applied to an emergency simulation game by considering the similar ones and weighting their predictions. Standardization steps helped achieve high levels of cross-task and cross-participant classification accuracy between 63.78\% and 67.25\% for the distinction between easy and hard levels of the emergency simulation game. These very promising results could pave the way for novel adaptive computer-human interaction across domains and particularly for gaming and learning environments.},
	journaltitle = {{IEEE} Transactions on Affective Computing},
	author = {Appel, Tobias and Gerjets, Peter and Hoffman, Stefan and Moeller, Korbinian and Ninaus, Manuel and Scharinger, Christian and Sevcenko, Natalia and Wortha, Franz and Kasneci, Enkelejda},
	date = {2021},
	keywords = {Adaptation models, Adaptive and Intelligent Educational Systems, Atmospheric measurements, Cognitive Model, Eye Tracking, Games, Intelligent Systems, Load modeling, Particle measurements, Physiological Measures, Physiology, Psychology, Real-time systems, Task analysis},
}

@article{eftekharifar_role_2021,
	title = {The role of binocular disparity and active motion parallax in cybersickness},
	volume = {239},
	issn = {1432-1106},
	doi = {10.1007/s00221-021-06124-6},
	abstract = {Cybersickness is an enduring problem for users of virtual environments. While it is generally assumed that cybersickness is caused by discrepancies in perceived self-motion between the visual and vestibular systems, little is known about the relative contribution of active motion parallax and binocular disparity to the occurrence of cybersickness. We investigated the role of these two depth cues in cybersickness by simulating a roller-coaster ride using a head-mounted display. Participants could see the tracks via a virtual frame placed at the front of the roller-coaster cart. We manipulated the state of the frame, so it behaved like: (1) a window into the virtual scene, (2) a 2D screen, (3) and (4) a window for one of the two depth cues, and a 2D screen for the other. Participants completed the Simulator Sickness Questionnaire before and after the experiment, and verbally reported their level of discomfort at repeated intervals during the ride. Additionally, participants' electrodermal activity ({EDA}) was recorded. The results of the questionnaire and the continuous ratings revealed the largest increase in cybersickness when the frame behaved like a window, and least increase when the frame behaved like a 2D screen. Cybersickness scores were at an intermediate level for the conditions where the frame simulated only one depth cue. This suggests that neither active motion parallax nor binocular disparity had a more prominent effect on the severity of cybersickness. The {EDA} responses increased at about the same rate in all conditions, suggesting that {EDA} is not necessarily coupled with subjectively experienced cybersickness.},
	pages = {2649--2660},
	number = {8},
	journaltitle = {Experimental Brain Research},
	shortjournal = {Exp Brain Res},
	author = {Eftekharifar, Siavash and Thaler, Anne and Bebko, Adam O. and Troje, Nikolaus F.},
	date = {2021-08},
	keywords = {Active motion parallax, Binocular disparity, Cues, Cybersickness, Depth Perception, Humans, Motion, Motion Perception, Motion Sickness, Roller-coaster, Virtual reality, Vision Disparity},
}

@article{korisky_dimensions_2021,
	title = {Dimensions of Perception: 3D Real-Life Objects Are More Readily Detected Than Their 2D Images},
	volume = {32},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/09567976211010718},
	doi = {10.1177/09567976211010718},
	shorttitle = {Dimensions of Perception},
	abstract = {Most of our interactions with our environment involve manipulating real 3D objects. Accordingly, 3D objects seem to enjoy preferential processing compared with 2D images, for example, in capturing attention or being better remembered. But are they also more readily perceived? Thus far, the possibility of preferred detection for real 3D objects could not be empirically tested because suppression from awareness has been applied only to on-screen stimuli. Here, using a variant of continuous flash suppression ({CFS}) with augmented-reality goggles (“real-life” {CFS}), we managed to suppress both real 3D objects and their 2D representations. In 20 healthy young adults, real objects broke suppression faster than their photographs. Using 3D printing, we also showed in 50 healthy young adults that this finding held only for meaningful objects, whereas no difference was found for meaningless, novel ones (a similar trend was observed in another experiment with 20 subjects, yet it did not reach significance). This suggests that the effect might be mediated by affordances facilitating detection of 3D objects under interocular suppression.},
	pages = {1636--1648},
	number = {10},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Korisky, Uri and Mudrik, Liad},
	urldate = {2024-04-09},
	date = {2021-10-01},
	langid = {english},
}

@article{makransky_cognitive_2021,
	title = {The Cognitive Affective Model of Immersive Learning ({CAMIL}): A theoretical research-based model of learning in immersive virtual reality},
	volume = {33},
	issn = {1573-336X},
	doi = {10.1007/s10648-020-09586-2},
	shorttitle = {The Cognitive Affective Model of Immersive Learning ({CAMIL})},
	abstract = {There has been a surge in interest and implementation of immersive virtual reality ({IVR})-based lessons in education and training recently, which has resulted in many studies on the topic. There are recent reviews which summarize this research, but little work has been done that synthesizes the existing findings into a theoretical framework. The Cognitive Affective Model of Immersive Learning ({CAMIL}) synthesizes existing immersive educational research to describe the process of learning in {IVR}. The general theoretical framework of the model suggests that instructional methods which are based on evidence from research with less immersive media generalize to learning in {IVR}. However, the {CAMIL} builds on evidence that media interacts with method. That is, certain methods which facilitate the affordances of {IVR} are specifically relevant in this medium. The {CAMIL} identifies presence and agency as the general psychological affordances of learning in {IVR}, and describes how immersion, control factors, and representational fidelity facilitate these affordances. The model describes six affective and cognitive factors that can lead to {IVR}-based learning outcomes including interest, motivation, self-efficacy, embodiment, cognitive load, and self-regulation. The model also describes how these factors lead to factual, conceptual, and procedural knowledge acquisition and knowledge transfer. Implications for future research and instructional design are proposed. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {937--958},
	number = {3},
	journaltitle = {Educational Psychology Review},
	author = {Makransky, Guido and Petersen, Gustav B.},
	date = {2021},
	keywords = {Academic Achievement Motivation, Educational Psychology, Learning Theory, Models, School Learning, Self-Regulated Learning, Virtual Reality},
}

@article{rim_introducing_2021,
	title = {Introducing Point-of-Interest as an alternative to Area-of-Interest for fixation duration analysis},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0250170},
	doi = {10.1371/journal.pone.0250170},
	abstract = {Many eye-tracking data analyses rely on the Area-of-Interest ({AOI}) methodology, which utilizes {AOIs} to analyze metrics such as fixations. However, {AOI}-based methods have some inherent limitations including variability and subjectivity in shape, size, and location of {AOIs}. In this article, we propose an alternative approach to the traditional {AOI} dwell time analysis: Weighted Sum Durations ({WSD}). This approach decreases the subjectivity of {AOI} definitions by using Points-of-Interest ({POI}) while maintaining interpretability. In {WSD}, the durations of fixations toward each {POI} is weighted by the distance from the {POI} and summed together to generate a metric comparable to {AOI} dwell time. To validate {WSD}, we reanalyzed data from a previously published eye-tracking study (n = 90). The re-analysis replicated the original findings that people gaze less towards faces and more toward points of contact when viewing violent social interactions.},
	pages = {e0250170},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Rim, Nak Won and Choe, Kyoung Whan and Scrivner, Coltan and Berman, Marc G.},
	urldate = {2022-02-24},
	date = {2021-10-05},
	langid = {english},
	keywords = {Algorithms, Covariance, Dwell time, Ellipses, Eye movements, Gaussian noise, Vision, White noise},
}

@article{frenzel_perceived_2007,
	title = {Perceived learning environment and students' emotional experiences: A multilevel analysis of mathematics classrooms},
	volume = {17},
	issn = {0959-4752},
	url = {https://www.sciencedirect.com/science/article/pii/S0959475207000928},
	doi = {10.1016/j.learninstruc.2007.09.001},
	shorttitle = {Perceived learning environment and students' emotional experiences},
	abstract = {A multilevel approach was used to analyse relationships between perceived classroom environments and emotions in mathematics. Based on Pekrun's (2000) [A social-cognitive, control-value theory of achievement emotions. In J. Heckhausen (Ed.), Motivational psychology of human development (pp. 143–163)] social-cognitive, control-value theory of achievement emotions, we hypothesized that environmental characteristics conveying control and value to the students would be related to their experience of enjoyment, anxiety, anger, and boredom in mathematics. Multilevel modelling of data from 1623 students from 69 classes (grades 5–10) confirmed close relationships between environmental variables and emotional experiences that functioned predominantly at the individual level. Compositional effects further revealed that classes' aggregate environment perceptions as well as their compositions in terms of aggregate achievement and gender ratio were additionally linked to students' emotions in mathematics. Methodological and practical implications of the findings are discussed.},
	pages = {478--493},
	number = {5},
	journaltitle = {Learning and Instruction},
	shortjournal = {Learning and Instruction},
	author = {Frenzel, Anne C. and Pekrun, Reinhard and Goetz, Thomas},
	urldate = {2024-04-30},
	date = {2007-10-01},
	keywords = {Anger, Anxiety, Boredom, Enjoyment, Learning environment, Mathematics, Multilevel modelling},
}

@article{huang_learning_2023,
	title = {Learning in an Immersive {VR} Environment: Role of Learner Characteristics and Relations Between Learning and Psychological Outcomes},
	url = {https://doi.org/10.1177/00472395231216943},
	doi = {10.1177/00472395231216943},
	shorttitle = {Learning in an Immersive {VR} Environment},
	abstract = {In an immersive virtual reality ({IVR}) environment, this study investigated: (1) the role of learner characteristics in various learning and psychological outcomes, including knowledge retention, perceived learning, cognitive load, self-efficacy, enjoyment, presence, and usefulness; and (2) the relationship among these intended outcomes. Forty adult participants experienced an {IVR} nature trail tour involving science learning topics, followed by a knowledge test and multiple surveys. Multivariate analysis of covariance results indicated age by gender interaction effects on perceived presence and usefulness as well as an age by {IVR} prior experience interaction effect on perceived presence. Additionally, multiple correlational relationships were detected among the intended outcomes. This study contributes to the limited research on the role of learner characteristics in {STEM} learning and psychological outcomes in {IVR} environments.},
	pages = {00472395231216943},
	journaltitle = {Journal of Educational Technology Systems},
	author = {Huang, Xiaoxia and Zhao, Qin and Liu, Yang and Harris, Desmond and Shawler, Melissa},
	urldate = {2024-01-02},
	date = {2023-12-03},
	langid = {english},
}

@article{wagner_comparing_2021,
	title = {Comparing and Combining Virtual Hand and Virtual Ray Pointer Interactions for Data Manipulation in Immersive Analytics},
	volume = {27},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2021.3067759},
	abstract = {In this work, we evaluate two standard interaction techniques for Immersive Analytics environments: virtual hands, with actions such as grabbing and stretching, and virtual ray pointers, with actions assigned to controller buttons. We also consider a third option: seamlessly integrating both modes and allowing the user to alternate between them without explicit mode switches. Easy-to-use interaction with data visualizations in Virtual Reality enables analysts to intuitively query or filter the data, in addition to the benefit of multiple perspectives and stereoscopic 3D display. While many {VR}-based Immersive Analytics systems employ one of the studied interaction modes, the effect of this choice is unknown. Considering that each has different advantages, we compared the three conditions through a controlled user study in the spatio-temporal data domain. We did not find significant differences between hands and ray-casting in task performance, workload, or interactivity patterns. Yet, 60\% of the participants preferred the mixed mode and benefited from it by choosing the best alternative for each low-level task. This mode significantly reduced completion times by 23\% for the most demanding task, at the cost of a 5\% decrease in overall success rates.},
	pages = {2513--2523},
	number = {5},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Wagner, Jorge and Stuerzlinger, Wolfgang and Nedel, Luciana},
	date = {2021-05},
	keywords = {Data visualization, Immersive analytics, Mice, Navigation, Presses, Task analysis, Three-dimensional displays, Trajectory, interaction methods, space-time cube, virtual hand, virtual ray pointer},
}

@article{hahn_eye_2022,
	title = {Eye tracking in physics education research: A systematic literature review},
	volume = {18},
	url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.18.013102},
	doi = {10.1103/PhysRevPhysEducRes.18.013102},
	shorttitle = {Eye tracking in physics education research},
	abstract = {Eye tracking is becoming increasingly popular in physics education research ({PER}). As technology has advanced considerably in recent years and has become more user friendly, it is anticipated that eye tracking will play an increasingly significant role in assessing student learning at the process level in future studies. The main objective of this systematic review is to summarize the current status quo regarding eye tracking in {PER} and reviewing (a) the dissemination, (b) the methodological implementation, and (c) the insights provided by eye tracking in {PER}. We identified 33 journal articles, published between March 2005 and April 2021, that used eye tracking for original empirical research in the area of physics education. The results reveal that although eye tracking has been used in many different areas of physics, a clear focus on mechanics is evident, particularly for measuring visual attention in assessment scenarios like problem solving. While a high methodological rigor in the selection and analysis of the visual stimuli was apparent, only a few studies have provided a complete documentation of the technological implementation (e.g., movement restrictions, accuracy, and calibration information) and a theoretical embedding for interpreting eye-tracking data. To synthesize the results of the different studies, we created an inductive category system in accordance with the considered independent variables of the studies. Accordingly, visual attention was most frequently compared between levels of performance (correct vs incorrect or high vs low achievers), thereby leading to performance-discriminating factors of eye movement across studies. Furthermore, learners’ eye movements were compared across different stimuli, different time points, or between student groups to inform multimedia design and shed light on students’ learning progression. In summary, eye tracking is particularly useful for studying processes in different domains that are relevant to {PER}. Specific gaps in the literature, methodological limitations, and implications of existing findings were also identified to recommend future research and practices.},
	pages = {013102},
	number = {1},
	journaltitle = {Physical Review Physics Education Research},
	shortjournal = {Phys. Rev. Phys. Educ. Res.},
	author = {Hahn, L. and Klein, P.},
	urldate = {2023-03-23},
	date = {2022-03-02},
}

@article{riva_virtual_2022,
	title = {Virtual Reality in Clinical Psychology},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7500920/},
	doi = {10.1016/B978-0-12-818697-8.00006-6},
	abstract = {From a technological viewpoint, virtual reality ({VR}) is a set of fancy technologies: a helmet, trackers, and a 3D visualizing system. However, from a psychological viewpoint, {VR} is simultaneously a simulative, a cognitive, and an embodied technology. These features make {VR} the perfect tool for experiential assessment and learning with great clinical potential., This potential is already supported by clinical outcomes. Two recent meta-reviews assessing more than 53 systematic reviews and meta-analyses support its use in anxiety disorders, pain management, and eating and weight disorders, with long-term effects that generalize to the real world. Recent studies have also provided preliminary support for the use of {VR} in the assessment and treatment of psychosis, addictions, and autism.},
	pages = {B978--0--12--818697--8.00006--6},
	journaltitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
	shortjournal = {Reference Module in Neuroscience and Biobehavioral Psychology},
	author = {Riva, Giuseppe},
	urldate = {2022-04-01},
	date = {2022},
}

@article{ugwitz_eye-tracking_2022,
	title = {Eye-Tracking in Interactive Virtual Environments: Implementation and Evaluation},
	volume = {12},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/3/1027},
	doi = {10.3390/app12031027},
	shorttitle = {Eye-Tracking in Interactive Virtual Environments},
	abstract = {Not all eye-tracking methodology and data processing are equal. While the use of eye-tracking is intricate because of its grounding in visual physiology, traditional 2D eye-tracking methods are supported by software, tools, and reference studies. This is not so true for eye-tracking methods applied in virtual reality (imaginary 3D environments). Previous research regarded the domain of eye-tracking in 3D virtual reality as an untamed realm with unaddressed issues. The present paper explores these issues, discusses possible solutions at a theoretical level, and offers example implementations. The paper also proposes a workflow and software architecture that encompasses an entire experimental scenario, including virtual scene preparation and operationalization of visual stimuli, experimental data collection and considerations for ambiguous visual stimuli, post-hoc data correction, data aggregation, and visualization. The paper is accompanied by examples of eye-tracking data collection and evaluation based on ongoing research of indoor evacuation behavior.},
	pages = {1027},
	number = {3},
	journaltitle = {Applied Sciences},
	author = {Ugwitz, Pavel and Kvarda, Ondřej and Juříková, Zuzana and Šašinka, Čeněk and Tamm, Sascha},
	urldate = {2024-01-15},
	date = {2022-01},
	langid = {english},
	keywords = {data collection, data visualization, dynamic environments, eye-tracking, eye-tracking algorithms, interactive environments, virtual reality},
}

@article{zhu_eye_2022,
	title = {Eye movements reveal spatiotemporal dynamics of visually-informed planning in navigation},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.73097},
	doi = {10.7554/eLife.73097},
	abstract = {Goal-oriented navigation is widely understood to depend upon internal maps. Although this may be the case in many settings, humans tend to rely on vision in complex, unfamiliar environments. To study the nature of gaze during visually-guided navigation, we tasked humans to navigate to transiently visible goals in virtual mazes of varying levels of difficulty, observing that they took near-optimal trajectories in all arenas. By analyzing participants’ eye movements, we gained insights into how they performed visually-informed planning. The spatial distribution of gaze revealed that environmental complexity mediated a striking trade-off in the extent to which attention was directed towards two complimentary aspects of the world model: the reward location and task-relevant transitions. The temporal evolution of gaze revealed rapid, sequential prospection of the future path, evocative of neural replay. These findings suggest that the spatiotemporal characteristics of gaze during navigation are significantly shaped by the unique cognitive computations underlying real-world, sequential decision making.},
	pages = {e73097},
	journaltitle = {{eLife}},
	author = {Zhu, Seren and Lakshminarasimhan, Kaushik J and Arfaei, Nastaran and Angelaki, Dora E},
	editor = {Zhang, Hang and Gold, Joshua I and Spiers, Hugo J},
	urldate = {2023-11-06},
	date = {2022-05-03},
	keywords = {active sensing, eye movements, navigation, planning},
}

@article{hershman_contribution_2023,
	title = {The contribution of temporal analysis of pupillometry measurements to cognitive research},
	volume = {87},
	issn = {1430-2772},
	doi = {10.1007/s00426-022-01656-0},
	abstract = {Reaction time ({RT}) is one of the most frequently used measures to detect cognitive processes. When tasks require more cognitive processes/resources, reaction is slower. However, {RTs} may provide only restricted information regarding the temporal characteristics of cognitive processes. Pupils respond reflexively to light but also to cognitive activation. The more cognitive resources a task requires, the more the pupil dilates. However, despite being able to use temporal changes in pupil size (advanced devices measure changes in pupil diameter with sampling rates of above 1000 samples per second), most past studies using pupil dilation have not investigated temporal changes in pupil response. In the current paper, we discuss the advantage of the temporal approach to analyze pupil changes compared to a more traditional perspective, specifically, singular value methods such as mean value and peak amplitude value. Using data from two recent studies conducted in our laboratory, we demonstrate the differences in findings arising from the various analyses. In particular, we focus on the advantage of temporal analysis in detecting hidden effects, investigating temporal characterizations of the effects, and validating the experimental manipulation.},
	pages = {28--42},
	number = {1},
	journaltitle = {Psychological Research},
	shortjournal = {Psychol Res},
	author = {Hershman, Ronen and Milshtein, Dalit and Henik, Avishai},
	date = {2023-02},
	keywords = {Cognition, Humans, Pupil, Reaction Time},
}

@article{bores-garcia_educational_2024,
	title = {Educational Research on the Use of Virtual Reality Combined with a Practice Teaching Style in Physical Education: A Qualitative Study from the Perspective of Researchers},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7102},
	url = {https://www.mdpi.com/2227-7102/14/3/291},
	doi = {10.3390/educsci14030291},
	shorttitle = {Educational Research on the Use of Virtual Reality Combined with a Practice Teaching Style in Physical Education},
	abstract = {(1) Background: the scientific literature has shown that students’ active involvement in the teaching–learning process significantly improves their learning outcomes. (2) Methods: this study shows the perceptions of seven researchers on the process of inquiring about the effects of the combined use of virtual reality ({VR}) and a practice teaching style in physical education in secondary educational institutions. (3) Results: the results obtained from the researchers’ diaries and the focus group, through qualitative design, are arranged in the following categories: difficulties in data collection before, during, and after the intervention; perceived differences between {VR} interventions in laboratory situations and educational contexts; and the perceived transferability of the use of {VR} devices in the educational context. (4) Conclusions: more research is needed on the use of {VR} in the educational context, although the results obtained indicate that the teaching–learning process can be enriched by overcoming the difficulties inherent to the use of this technology in a variable context such as education.},
	pages = {291},
	number = {3},
	journaltitle = {Education Sciences},
	author = {Bores-García, Daniel and Cano-de-la-Cuerda, Roberto and Espada, María and Romero-Parra, Nuria and Fernández-Vázquez, Diego and Delfa-De-La-Morena, José Manuel and Navarro-López, Víctor and Palacios-Ceña, Domingo},
	urldate = {2024-04-02},
	date = {2024-03},
	langid = {english},
	keywords = {educational research, physical education, qualitative design, secondary education, teaching methods, technology, virtual reality},
}

@article{vega_role_2024,
	title = {The Role of Virtual Reality in Enhancing the Effectiveness of Teaching English for Specific Purposes},
	volume = {2},
	rights = {Copyright (c) 2024 Research and Innovation in Applied Linguistics [{RIAL}]},
	issn = {2964-5344},
	url = {https://jurnal.poliupg.ac.id/index.php/RIAL-EJ/article/view/4386},
	doi = {10.31963/rial.v2i1.4386},
	abstract = {This study provides a novel examination of the integration of Virtual Reality ({VR}) technology into English for Specific Purposes ({ESP}) instruction, specifically within the context of Management studies at Universitas Borneo Tarakan. It addresses a significant gap in empirical research by focusing on {VR}'s effectiveness in teaching {ESP} within a management context, an area that has yet to be thoroughly explored. Using pre-and post-test scores from 286 students and detailed evaluation metrics, this research provides robust evidence of {VR}'s transformative potential in enhancing {ESP} instruction. The findings indicate a clear improvement in language proficiency following {VR}-enhanced teaching, with significant increases in top-scoring students and decreases among those struggling with lower scores. Additionally, high levels of student engagement were observed during {VR}-enhanced lessons. Students reported positive usability experiences with the technology and expressed satisfaction with their learning outcomes. They also perceived significant benefits from using {VR} technology in their {ESP} instruction, including improved comprehension, increased motivation and engagement, and enhanced memory retention. However, alongside these positive outcomes are challenges that must be addressed for effective implementation. These include technical issues experienced by some participants suggesting the need for improvements or troubleshooting to enhance user experience; adaptation difficulties faced by learners when transitioning to new teaching methods introduced via virtual reality indicating the importance of providing additional support or guidance; and accessibility challenges emphasizing the need to ensure equal access for all learners when implementing such innovative technologies. In light of these findings, it is recommended that institutions like Universitas Borneo Tarakan looking to integrate {VR} into their {ESP} instruction consider establishing robust technical support structures to address potential issues. They should also provide additional guidance during transitions to new teaching methods introduced via virtual reality and prioritize ensuring accessibility for all learners. This study contributes valuable insights into effective strategies for modernizing {ESP} instruction using innovative technologies like Virtual Reality while highlighting areas where further exploration is needed.},
	pages = {1--13},
	number = {1},
	journaltitle = {Research and Innovation in Applied Linguistics [{RIAL}]},
	author = {Vega, Nofvia De and Rahayu, Rahayu and Basri, Nurfajriah},
	urldate = {2024-04-02},
	date = {2024-02-29},
	langid = {english},
	keywords = {English for Specific Purposes, Language Instruction, Management Studies, Virtual Reality},
}

@article{wang_feedback_2023,
	title = {Feedback Mechanism in Immersive Virtual Reality Influences Physical Hands-on Task Performance and Cognitive Load},
	volume = {0},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2023.2209837},
	doi = {10.1080/10447318.2023.2209837},
	abstract = {The affordance of immersive virtual reality ({VR}) holds great potential for education, supporting learning through simulation and visualization. However, the literature on {STEM} education indicates that {VR} learning is not necessarily more effective than traditional learning, because participants are immersed in learning environments with high visual loads, which increases cognitive load, resulting in poor learning outcomes. In addition, {VR} studies rarely evaluate learner performance in physical hands-on activities after participating in {VR} learning programs. Therefore, in this study, we seek to reduce cognitive load by incorporating feedback into the {VR} learning environment. This study conducts a quasi-experiment and proposes a {VR} learning environment for embedded electronic circuits and a practical hands-on task, to investigate learners’ learning performance on practical hands-on task skills and their cognitive load. Participants were randomly assigned to groups with and without feedback. The results show that feedback in the {VR} environment is effective in reducing participants’ extraneous cognitive load and increasing engagement, with fewer trial-and-error times in the learning units and better performance in the physical hands-on task.},
	pages = {1--13},
	number = {0},
	journaltitle = {International Journal of Human–Computer Interaction},
	author = {Wang, Wei-Sheng and Lin, Chia-Ju and Lee, Hsin-Yu and Wu, Ting-Ting and Huang, Yueh-Min},
	urldate = {2023-05-16},
	date = {2023-05-11},
	keywords = {Immersive virtual reality, cognitive load, embedded electronic circuits, engagement, feedback, hands-on task},
}

@article{heider_experimental_1944,
	title = {An Experimental Study of Apparent Behavior},
	volume = {57},
	issn = {0002-9556},
	url = {https://www.jstor.org/stable/1416950},
	doi = {10.2307/1416950},
	pages = {243--259},
	number = {2},
	journaltitle = {The American Journal of Psychology},
	author = {Heider, Fritz and Simmel, Marianne},
	urldate = {2024-04-30},
	date = {1944},
	note = {Publisher: University of Illinois Press},
}

@article{gardony_what_2014,
	title = {What Does Physical Rotation Reveal About Mental Rotation?},
	volume = {25},
	url = {https://doi.org/10.1177/0956797613503174},
	doi = {10.1177/0956797613503174},
	abstract = {In a classic psychological science experiment, Shepard and Metzler (1971) discovered that the time participants took to judge whether two rotated abstract block figures were identical increased monotonically with the figures’ relative angular disparity. They posited that participants rotate mental images to achieve a match and that mental rotation recruits motor processes. This interpretation has become central in the literature, but until now, surprisingly few researchers have compared mental and physical rotation. We had participants rotate virtual Shepard and Metzler figures mentally and physically; response time, accuracy, and real-time rotation data were collected. Results suggest that mental and physical rotation processes overlap and also reveal novel conclusions about physical rotation that have implications for mental rotation. Notably, participants did not rotate figures to achieve a match, but rather until they reached an off-axis canonical difference, and rotational strategies markedly differed for judgments of whether the figures were the same or different.},
	pages = {605--612},
	number = {2},
	journaltitle = {Psychological Science},
	author = {Gardony, Aaron L. and Taylor, Holly A. and Brunyé, Tad T.},
	date = {2014},
	pmid = {24311475},
	note = {\_eprint: https://doi.org/10.1177/0956797613503174},
}

@inproceedings{stark_using_2024,
	location = {Glasgow, {UK}},
	title = {Using Gaze Transition Entropy to Detect Classroom Discourse in a Virtual Reality Classroom},
	eventtitle = {Eye Tracking Research and Applications ({ETRA})},
	booktitle = {2024 Symposium on Eye Tracking Research and Applications ({ETRA} ’24)},
	publisher = {{ACM}},
	author = {Stark, Philipp and Jung, Alexander and Hahn, Jens-Uwe and Kasneci, Enkelejda and Göllner, Richard},
	date = {2024},
}

@inproceedings{gao_detecting_2023,
	title = {Detecting Teacher Expertise in an Immersive {VR} Classroom: Leveraging Fused Sensor Data with Explainable Machine Learning Models},
	url = {https://ieeexplore.ieee.org/document/10316424},
	doi = {10.1109/ISMAR59233.2023.00083},
	shorttitle = {Detecting Teacher Expertise in an Immersive {VR} Classroom},
	abstract = {Currently, {VR} technology is increasingly being used in applications to enable immersive yet controlled research settings. One such area of research is expertise assessment, where novel technological approaches to collecting process data, specifically eye tracking, in combination with explainable models, can provide insights into assessing and training novices, as well as fostering expertise development. We present a machine learning approach to predict teacher expertise by leveraging data from an off-the-shelf {VR} device collected in a {VirATec} study. By fusing eye-tracking and controller-tracking data, teachers’ recognition and handling of disruptive events in the classroom are taken into account or considered. Three classification models were compared, including {SVM}, Random Forest, and {LightGBM}, with Random Forest achieving the best {ROC}-{AUC} score of 0.768 in predicting teacher expertise. The {SHAP} approach to model interpretation revealed informative features (e.g., fixations on identified disruptive students) for distinguishing teacher expertise. Our study serves as a pioneering effort in assessing teacher expertise using eye tracking within an interactive virtual setting, paving the way for future research and advancements in the field.},
	eventtitle = {2023 {IEEE} International Symposium on Mixed and Augmented Reality ({ISMAR})},
	pages = {683--692},
	booktitle = {2023 {IEEE} International Symposium on Mixed and Augmented Reality ({ISMAR})},
	author = {Gao, Hong and Bozkir, Efe and Stark, Philipp and Goldberg, Patricia and Meixner, Gerrit and Kasneci, Enkelejda and Göllner, Richard},
	urldate = {2024-01-02},
	date = {2023-10},
}

@article{hasenbein_investigating_2023,
	title = {Investigating social comparison behaviour in an immersive virtual reality classroom based on eye-movement data},
	volume = {13},
	rights = {2023 Springer Nature Limited},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-41704-2},
	doi = {10.1038/s41598-023-41704-2},
	abstract = {Higher-achieving peers have repeatedly been found to negatively impact students’ evaluations of their own academic abilities (i.e., Big-Fish-Little-Pond Effect). Building on social comparison theory, this pattern is assumed to result from students comparing themselves to their classmates; however, based on existing research designs, it remains unclear how exactly students make use of social comparison information in the classroom. To determine the extent to which students (N = 353 sixth graders) actively attend and respond to social comparison information in the form of peers’ achievement-related behaviour, we used eye-tracking data from an immersive virtual reality ({IVR}) classroom. {IVR} classrooms offer unprecedented opportunities for psychological classroom research as they allow to integrate authentic classroom scenarios with maximum experimental control. In the present study, we experimentally varied virtual classmates’ achievement-related behaviour (i.e., their hand-raising in response to the teacher’s questions) during instruction, and students’ eye and gaze data showed that they actively processed this social comparison information. Students who attended more to social comparison information (as indicated by more frequent and longer gaze durations at peer learners) had less favourable self-evaluations. We discuss implications for the future use of {IVR} environments to study behaviours in the classroom and beyond.},
	pages = {14672},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Hasenbein, Lisa and Stark, Philipp and Ulrich, Trautwein and Gao, Hong and Kasneci, Enkelejda and Göllner, Richard},
	urldate = {2023-10-19},
	date = {2023-09-06},
	langid = {english},
	keywords = {Human behaviour, Psychology, Psychology and behaviour},
}

@article{jarrold_social_2013,
	title = {Social Attention in a Virtual Public Speaking Task in Higher Functioning Children With Autism},
	volume = {6},
	rights = {© 2013 International Society for Autism Research, Wiley Periodicals, Inc.},
	issn = {1939-3806},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aur.1302},
	doi = {10.1002/aur.1302},
	abstract = {Impairments in social attention play a major role in autism, but little is known about their role in development after preschool. In this study, a public speaking task was used to study social attention, its moderators, and its association with classroom learning in elementary and secondary students with higher functioning autism spectrum disorder ({HFASD}). Thirty-seven students with {HFASD} and 54 age- and intelligence quotient ({IQ})-matched peers without symptoms of {ASD} were assessed in a virtual classroom public speaking paradigm. This paradigm assessed the ability to attend to nine avatar peers seated at a table, while simultaneously answering self-referenced questions. Students with {HFASD} looked less frequently to avatar peers in the classroom while talking. However, social attention was moderated in the {HFASD} sample such that students with lower {IQ}, and/or more symptoms of social anxiety, and/or more attention deficit/hyperactivity disorder inattentive symptoms, displayed more atypical social attention. Group differences were more pronounced when the classroom contained social avatars versus nonsocial targets. Moreover, measures of social attention rather than nonsocial attention were significantly associated with parent report and objective measures of learning in the classroom. The data in this study support the hypothesis of the Social Attention Model of {ASD} that social attention disturbance remains part of the school-aged phenotype of autism that is related to syndrome-specific problems in social learning. More research of this kind would likely contribute to advances in the understanding of the development of the spectrum of autism and educational intervention approaches for affected school-aged children. Autism Res 2013, ●●: ●●–●●. © 2013 International Society for Autism Research, Wiley Periodicals, Inc.},
	pages = {393--410},
	number = {5},
	journaltitle = {Autism Research},
	author = {Jarrold, William and Mundy, Peter and Gwaltney, Mary and Bailenson, Jeremy and Hatt, Naomi and {McIntyre}, Nancy and Kim, Kwanguk and Solomon, Marjorie and Novotny, Stephanie and Swain, Lindsay},
	urldate = {2024-04-24},
	date = {2013},
	langid = {english},
	keywords = {cognition and learning, individual differences, school-aged development, social attention},
}

@article{lundqvist_emotion_2005,
	title = {Emotion Regulates Attention: The Relation between Facial Configurations, Facial Emotion, and Visual Attention},
	volume = {12},
	url = {https://doi.org/10.1080/13506280444000085},
	doi = {10.1080/13506280444000085},
	pages = {51--84},
	number = {1},
	journaltitle = {Visual Cognition},
	author = {Lundqvist, Daniel and Ohman, Arne},
	date = {2005},
}

@article{li_uncovering_2023,
	title = {Uncovering the effect of classroom climates on learning experience and performance in a virtual environment},
	volume = {0},
	issn = {1049-4820},
	url = {https://doi.org/10.1080/10494820.2023.2195450},
	doi = {10.1080/10494820.2023.2195450},
	abstract = {A virtual classroom constructed with virtual reality technology enables a more intuitive and immersive experience for learners. The classroom climate is an important factor affecting students’ learning experience and performance in a virtual environment. However, there is a lack of research on the role of climate on learning experience and performance in virtual classrooms. This study investigated the effect of classroom climates (positive, neutral, and negative) on students’ learning experience (cognitive load, learning motivation, and emotion) and performance (behavior and outcome) in a virtual environment. A total of 96 undergraduate students participated in the study. The results indicated that learners performed better when they perceived a positive classroom climate. Compared with the neutral climate group, the negative climate group reported stronger immersion, more active behavior, and better learning outcomes. The findings provide evidence for instructors and designers to create a suitable climate to improve students’ learning experience and performance in a virtual environment.},
	pages = {1--14},
	number = {0},
	journaltitle = {Interactive Learning Environments},
	author = {Li, Wenhao and Ren, Xiaotong and Qian, Li and Luo, Heng and Liu, Bowen},
	urldate = {2023-04-17},
	date = {2023-04-01},
	keywords = {Virtual classroom, classroom climate, learning experience, performance, virtual reality},
}

@incollection{nale_dispositifapparatus_2014,
	location = {Cambridge},
	title = {Dispositif(Apparatus)},
	isbn = {978-0-521-11921-4},
	url = {https://www.doi.org/10.1017/CBO9781139022309.025},
	pages = {126--132},
	booktitle = {The Cambridge Foucault Lexicon},
	publisher = {Cambridge University Press},
	editor = {Nale, John and Lawlor, Leonard},
	urldate = {2024-04-30},
	date = {2014},
	note = {{DOI}:},
}

@article{linn_emergence_1985,
	title = {Emergence and Characterization of Sex Differences in Spatial Ability: A Meta-Analysis},
	volume = {56},
	issn = {0009-3920},
	url = {https://www.jstor.org/stable/1130467},
	doi = {10.2307/1130467},
	shorttitle = {Emergence and Characterization of Sex Differences in Spatial Ability},
	abstract = {Sex differences in spatial ability are widely acknowledged, yet considerable dispute surrounds the magnitude, nature, and age of first occurrence of these differences. This article focuses on 3 questions about sex differences in spatial ability: (a) What is the magnitude of sex differences in spatial ability? (b) On which aspects of spatial ability are sex differences found? and (c) When, in the life span, are sex differences in spatial ability first detected? Implications for clarifying the linkage between sex differences in spatial ability and other differences between males and females are discussed. We use meta-analysis, a method for synthesizing empirical studies, to investigate these questions. Results of the meta-analysis suggest (a) that sex differences arise on some types of spatial ability but not others, (b) that large sex differences are found only on measures of mental rotation, (c) that smaller sex differences are found on measures of spatial perception, and (d) that, when sex differences are found, they can be detected across the life span.},
	pages = {1479--1498},
	number = {6},
	journaltitle = {Child Development},
	author = {Linn, Marcia C. and Petersen, Anne C.},
	urldate = {2022-06-02},
	date = {1985},
	note = {Number: 6
Publisher: [Wiley, Society for Research in Child Development]},
}

@collection{brandes_network_2005,
	location = {Berlin, Heidelberg},
	title = {Network Analysis},
	volume = {3418},
	isbn = {978-3-540-24979-5},
	url = {http://link.springer.com/10.1007/b106453},
	series = {Lecture Notes in Computer Science},
	publisher = {Springer},
	editor = {Brandes, Ulrik and Erlebach, Thomas},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2024-01-09},
	date = {2005},
	doi = {10.1007/b106453},
	keywords = {Computer, Graph theory, Internet, algorithm analysis and problem complexity, algorithms, applied graph theory, combinatorial optimization, connectivity, data structures, graph algorithms, graph isomorphisms, network algorithms, network analysis, network design, network statistics, statistics, vertices},
}

@book{diestel_graph_2017,
	location = {Berlin Heidelberg},
	edition = {5},
	title = {Graph Theory},
	isbn = {978-3-662-53621-6},
	url = {https://doi.org/10.1007/978-3-662-53622-3},
	series = {Graduate Texts in Mathematics},
	abstract = {This standard textbook of modern graph theory, now in its fifth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each field by one or two deeper results, again with proofs given in full detail. The book can be used as a reliable text for an introductory course, as a graduate text, and for self-study. From the reviews: “This outstanding book cannot be substituted with any other book on the present textbook market. It has every chance of becoming the standard textbook for graph theory.” Acta Scientiarum Mathematiciarum “Deep, clear, wonderful. This is a serious book about the heart of graph theory. It has depth and integrity.” Persi Diaconis \& Ron Graham, {SIAM} Review “The book has received a very enthusiastic reception, which it amply deserves. A masterly elucidation of modern graph theory.” Bulletin of the Institute of Combinatorics and its Applications “Succeeds dramatically ... a hell of a good book.” {MAA} Reviews “A highlight of the book is what is by far the best account in print of the Seymour-Robertson theory of graph minors.” Mathematika “ ... like listening to someone explain mathematics.” Bulletin of the {AMS}},
	publisher = {Springer-Verlag},
	author = {Diestel, Reinhard},
	urldate = {2021-08-27},
	date = {2017},
	langid = {english},
}

@inproceedings{zhu_exploratory_2015,
	location = {New York, {NY}, {USA}},
	title = {An exploratory study using social network analysis to model eye movements in mathematics problem solving},
	isbn = {978-1-4503-3417-4},
	url = {https://doi.org/10.1145/2723576.2723591},
	doi = {10.1145/2723576.2723591},
	series = {{LAK} '15},
	abstract = {Eye tracking is a useful tool to understand students' cognitive process during problem solving. This paper offers a unique perspective by applying techniques from social network analysis to eye movement patterns in mathematics problem solving. We construct and visualize transition networks using eye-tracking data collected from 37 8th grade students while solving linear function problems. By applying network analysis on the constructed transition networks, we find general transition patterns between areas of interest ({AOIs}) for all students, and we also compare patterns for high- and low-performing students. Our results show that even though students share general transition patterns during problem solving, high-performing students made more strategic transitions among {AOI} triples than low-performing students.},
	pages = {383--387},
	booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Mengxiao and Feng, Gary},
	urldate = {2024-04-29},
	date = {2015},
	keywords = {eye tracking, network analysis, problem solving},
}

@article{souchet_measuring_2022,
	title = {Measuring Visual Fatigue and Cognitive Load via Eye Tracking while Learning with Virtual Reality Head-Mounted Displays: A Review},
	volume = {38},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2021.1976509},
	doi = {10.1080/10447318.2021.1976509},
	shorttitle = {Measuring Visual Fatigue and Cognitive Load via Eye Tracking while Learning with Virtual Reality Head-Mounted Displays},
	abstract = {Virtual Reality Head-Mounted Displays ({HMDs}) reached the consumer market and are used for learning purposes. Risks regarding visual fatigue and high cognitive load arise while using {HMDs}. These risks could impact learning efficiency. Visual fatigue and cognitive load can be measured with eye tracking, a technique that is progressively implemented in {HMDs}. Thus, we investigate how to assess visual fatigue and cognitive load via eye tracking. We conducted this review based on five research questions. We first described visual fatigue and possible cognitive overload while learning with {HMDs}. The review indicates that visual fatigue can be measured with blinks and cognitive load with pupil diameter based on thirty-seven included papers. Yet, distinguishing visual fatigue from cognitive load with such measures is challenging due to possible links between them. Despite measure interpretation issues, eye tracking is promising for live assessment. More researches are needed to make data interpretation more robust and document human factor risks when learning with {HMDs}.},
	pages = {801--824},
	number = {9},
	journaltitle = {International Journal of Human–Computer Interaction},
	author = {Souchet, Alexis D. and Philippe, Stéphanie and Lourdeaux, Domitile and Leroy, Laure},
	urldate = {2024-04-14},
	date = {2022-05-28},
}

@article{cristino_scanmatch_2010,
	title = {{ScanMatch}: A novel method for comparing fixation sequences},
	volume = {42},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/BRM.42.3.692},
	doi = {https://doi.org/10.3758/BRM.42.3.692},
	abstract = {We present a novel approach to comparing saccadic eye movement sequences based on the Needleman-Wunsch algorithm used in bioinformatics to compare {DNA} sequences. In the proposed method, the saccade sequence is spatially and temporally binned and then recoded to create a sequence of letters that retains fixation location, time, and order information. The comparison of two letter sequences is made by maximizing the similarity score computed from a substitution matrix that provides the score for all letter pair substitutions and a penalty gap. The substitution matrix provides a meaningful link between each location coded by the individual letters. This link could be distance but could also encode any useful dimension, including perceptual or semantic space. We show, by using synthetic and behavioral data, the benefits of this method over existing methods. The {ScanMatch} toolbox for {MATLAB} is freely available online (www.scanmatch.co.uk).},
	pages = {692--700},
	number = {3},
	journaltitle = {Behavior Research Methods},
	author = {Cristino, Filipe and Mathôt, Sebastiaan and Theeuwes, Jan and Gilchrist, Iain D.},
	date = {2010},
}

@article{coutrot_scanpath_2018,
	title = {Scanpath modeling and classification with hidden Markov models},
	volume = {50},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-017-0876-8},
	doi = {https://doi.org/10.3758/s13428-017-0876-8},
	abstract = {How people look at visual information reveals fundamental information about them; their interests and their states of mind. Previous studies showed that scanpath, i.e., the sequence of eye movements made by an observer exploring a visual stimulus, can be used to infer observer-related (e.g., task at hand) and stimuli-related (e.g., image semantic category) information. However, eye movements are complex signals and many of these studies rely on limited gaze descriptors and bespoke datasets. Here, we provide a turnkey method for scanpath modeling and classification. This method relies on variational hidden Markov models ({HMMs}) and discriminant analysis ({DA}). {HMMs} encapsulate the dynamic and individualistic dimensions of gaze behavior, allowing {DA} to capture systematic patterns diagnostic of a given class of observers and/or stimuli. We test our approach on two very different datasets. Firstly, we use fixations recorded while viewing 800 static natural scene images, and infer an observer-related characteristic: the task at hand. We achieve an average of 55.9\% correct classification rate (chance = 33\%). We show that correct classification rates positively correlate with the number of salient regions present in the stimuli. Secondly, we use eye positions recorded while viewing 15 conversational videos, and infer a stimulus-related characteristic: the presence or absence of original soundtrack. We achieve an average 81.2\% correct classification rate (chance = 50\%). {HMMs} allow to integrate bottom-up, top-down, and oculomotor influences into a single model of gaze behavior. This synergistic approach between behavior and machine learning will open new avenues for simple quantification of gazing behavior. We release {SMAC} with {HMM}, a Matlab toolbox freely available to the community under an open-source license agreement.},
	pages = {362--379},
	number = {1},
	journaltitle = {Behavior Research Methods},
	author = {Coutrot, Antoine and Hsiao, Janet H. and Chan, Antoni B.},
	date = {2018},
}

@article{nolin_clinicavr_2016,
	title = {{ClinicaVR}: Classroom-{CPT}: A Virtual Reality Tool for Assessing Attention and Inhibition in Children and Adolescents},
	volume = {59},
	doi = {10.1016/j.chb.2016.02.023},
	pages = {327--333},
	journaltitle = {Computers in Human Behavior},
	author = {Nolin, Pierre and Stipanicic, Annie and Henry, Mylene and Lachapelle, Yves and Lussier-Desrochers, Dany and Rizzo, Albert and Allain, Philippe},
	date = {2016},
}

@article{diaz-orueta_aula_2014,
	title = {{AULA} Virtual Reality Test as an Attention Measure: Convergent Validity with Conners’ Continuous Performance Test},
	volume = {20},
	url = {https://doi.org/10.1080/09297049.2013.792332},
	doi = {10.1080/09297049.2013.792332},
	abstract = {The majority of neuropsychological tests used to evaluate attention processes in children lack ecological validity. The {AULA} Nesplora ({AULA}) is a continuous performance test, developed in a virtual setting, very similar to a school classroom. The aim of the present study is to analyze the convergent validity between the {AULA} and the Continuous Performance Test ({CPT}) of Conners. The {AULA} and {CPT} were administered correlatively to 57 children, aged 6–16 years (26.3\% female) with average cognitive ability ({IQ} mean = 100.56, {SD} = 10.38) who had a diagnosis of attention deficit/hyperactivity disorder ({ADHD}) according to {DSM}-{IV}-{TR} criteria. Spearman correlations analyses were conducted among the different variables. Significant correlations were observed between both tests in all the analyzed variables (omissions, commissions, reaction time, and variability of reaction time), including for those measures of the {AULA} based on different sensorial modalities, presentation of distractors, and task paradigms. Hence, convergent validity between both tests was confirmed. Moreover, the {AULA} showed differences by gender and correlation to Perceptual Reasoning and Working Memory indexes of the {WISC}-{IV}, supporting the relevance of {IQ} measures in the understanding of cognitive performance in {ADHD}. In addition, the {AULA} (but not Conners’ {CPT}) was able to differentiate between {ADHD} children with and without pharmacological treatment for a wide range of measures related to inattention, impulsivity, processing speed, motor activity, and quality of attention focus. Additional measures and advantages of the {AULA} versus Conners’ {CPT} are discussed.},
	pages = {328--342},
	number = {3},
	journaltitle = {Child Neuropsychology},
	author = {Díaz-Orueta, Unai and Garcia-López, Cristina and Crespo-Eguílaz, Nerea and Sánchez-Carpintero, Rocío and Climent, Gema and Narbona, Juan},
	date = {2014},
}

@article{rizzo_virtual_2005,
	title = {A Virtual Reality Scenario for All Seasons: The Virtual Classroom},
	volume = {11},
	doi = {10.1017/S1092852900024196},
	pages = {35--44},
	number = {1},
	journaltitle = {{CNS} spectrums},
	author = {Rizzo, Albert and Bowerly, Todd and Buckwalter, J. and Klimchuk, Dean and Mitura, Roman and Parsons, Thomas},
	date = {2005},
}

@inproceedings{stark_pupil_2023,
	location = {Germany},
	title = {Pupil Diameter during Counting Tasks as Potential Baseline for Virtual Reality Experiments},
	url = {https://doi.org/10.1145/3588015.3588414},
	doi = {10.1145/3588015.3588414},
	eventtitle = {Eye Tracking Research and Applications ({ETRA})},
	pages = {7},
	booktitle = {2023 Symposium on Eye Tracking Research and Applications ({ETRA} ’23)},
	publisher = {{ACM}},
	author = {Stark, Philipp and Tobias, Appel and Milo, Olbrich and Enkelejda, Kasneci},
	date = {2023-06-30},
}

@article{shiferaw_review_2019,
	title = {A review of gaze entropy as a measure of visual scanning efficiency},
	volume = {96},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763418303075},
	doi = {10.1016/j.neubiorev.2018.12.007},
	abstract = {While the concept of entropy has been applied to gaze analysis, it is unclear what aspects of visual scanning it measures. In this review, we first outline gaze control as a complex system of spatial prediction. Second, we provide a brief introduction to the concept of entropy within the context of information theory as the foundation for gaze entropy measures; with a specific focus on equations for Shannon’s entropy and conditional entropy. The application of these equations to gaze data is described as stationary gaze entropy ({SGE}) and gaze transition entropy ({GTE}) respectively. Third, we present an updated model of gaze orientation and propose an adaptable definition of {GTE} as a measure of visual scanning efficiency that underlies overall gaze dispersion measured by {SGE}. Finally, we review studies that have utilised {GTE} and {SGE} to assess visual scanning and discuss their results in relation to our proposed definitions and associated hypotheses. Methodological limitations in gaze entropy measures are discussed and suggestions provided to improve interpretability and generalisability of future studies.},
	pages = {353--366},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Shiferaw, Brook and Downey, Luke and Crewther, David},
	urldate = {2023-11-07},
	date = {2019-01-01},
	keywords = {Fixations, Gaze, Gaze entropy, Visual scanning},
}

@inproceedings{raptis_implicit_2017,
	location = {New York, {NY}, {USA}},
	title = {On Implicit Elicitation of Cognitive Strategies using Gaze Transition Entropies in Pattern Recognition Tasks},
	isbn = {978-1-4503-4656-6},
	url = {https://dl.acm.org/doi/10.1145/3027063.3053106},
	doi = {10.1145/3027063.3053106},
	series = {{CHI} {EA} '17},
	abstract = {Recent research provides evidence that individual differences on human cognitive strategies affect user performance and experience in diverse application domains. However, state-of-the-art elicitation methods of human cognitive strategies require from researchers to apply explicit, in-lab, and time-consuming "paper-and-pencil" techniques, compromising real-time integration of human cognitive strategies in interactive system design. Aiming to elaborate an implicit elicitation framework of human cognitive strategies, this paper reports on an in-lab eye-tracking study, which embraced sixty seven participants, who performed a credible "paper-and-pencil" cognitive strategy elicitation technique. Eye tracking analysis based on gaze transition entropies revealed quantitative differences on visual search patterns among individuals within visual pattern recognition tasks of varying complexity. Results of this study could drive the development of an implicit elicitation framework of human cognitive strategies.},
	pages = {1993--2000},
	booktitle = {Proceedings of the 2017 {CHI} Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Raptis, George E. and Fidas, Christos A. and Avouris, Nikolaos M.},
	urldate = {2024-01-21},
	date = {2017},
	keywords = {cognitive strategies, eye tracking, user study, visual pattern recognition tasks},
}

@inproceedings{mejia-romero_gaze_2021,
	location = {Singapore},
	title = {Gaze Movement’s Entropy Analysis to Detect Workload Levels},
	isbn = {978-981-334-673-4},
	doi = {10.1007/978-981-33-4673-4_13},
	series = {Advances in Intelligent Systems and Computing},
	abstract = {The gaze movement from a driver represents specific skills related to safe driving. Driving maneuvering evaluation is to know driving fitness. We found that gaze movement entropy is highly sensitive to visual behavior demands while driving a vehicle and the workload level. Entropy measures were more sensitive, more robust, and easier to calculate than gaze established measures. The gaze movement measures were collected using a driving simulator, using five different simulate routes and tree different workload scenarios; one route of familiarization was used to create a baseline. Because the workload became more difficult, drivers looked more at the central part of the road for more extended periods, the gaze movement entropy values decreased when the workload was increased, and the results show differences between two levels of workload. Also, the entropy result is compared against the classical analysis of the spatial distribution of gaze.},
	pages = {147--154},
	booktitle = {Proceedings of International Conference on Trends in Computational and Cognitive Engineering},
	publisher = {Springer},
	author = {Mejia-Romero, Sergio and Michaels, Jesse and Eduardo Lugo, J. and Bernardin, Delphine and Faubert, Jocelyn},
	editor = {Kaiser, M. Shamim and Bandyopadhyay, Anirban and Mahmud, Mufti and Ray, Kanad},
	date = {2021},
	langid = {english},
	keywords = {Driving performance, Driving simulator, Entropy, Eye tracking, Mental workload},
}

@article{di_stasi_gaze_2016,
	title = {Gaze entropy reflects surgical task load},
	volume = {30},
	issn = {1432-2218},
	url = {https://doi.org/10.1007/s00464-016-4851-8},
	doi = {10.1007/s00464-016-4851-8},
	abstract = {Task (over-)load imposed on surgeons is a main contributing factor to surgical errors. Recent research has shown that gaze metrics represent a valid and objective index to asses operator task load in non-surgical scenarios. Thus, gaze metrics have the potential to improve workplace safety by providing accurate measurements of task load variations. However, the direct relationship between gaze metrics and surgical task load has not been investigated yet. We studied the effects of surgical task complexity on the gaze metrics of surgical trainees.},
	pages = {5034--5043},
	number = {11},
	journaltitle = {Surgical Endoscopy},
	shortjournal = {Surg Endosc},
	author = {Di Stasi, Leandro L. and Diaz-Piedra, Carolina and Rieiro, Héctor and Sánchez Carrión, José M. and Martin Berrido, Mercedes and Olivares, Gonzalo and Catena, Andrés},
	urldate = {2024-01-21},
	date = {2016-11-01},
	langid = {english},
	keywords = {Eye metrics, Neuroergonomics, Patient safety, Saccades, Surgical skills assessment},
}

@article{andrist_network_2018,
	title = {A Network Analytic Approach to Gaze Coordination during a Collaborative Task},
	volume = {89},
	doi = {10.1016/j.chb.2018.07.017},
	journaltitle = {Computers in Human Behavior},
	shortjournal = {Computers in Human Behavior},
	author = {Andrist, Sean and Ruis, Andrew and Shaffer, David},
	date = {2018-07-01},
}

@article{andrist_look_2015,
	title = {Look together: analyzing gaze coordination with epistemic network analysis},
	volume = {6},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01016},
	doi = {https://doi.org/10.3389/fpsyg.2015.01016},
	shorttitle = {Look together},
	abstract = {When conversing and collaborating in everyday situations, people naturally and interactively align their behaviors with each other across various communication channels, including speech, gesture, posture, and gaze. Having access to a partner's referential gaze behavior has been shown to be particularly important in achieving collaborative outcomes, but the process in which people's gaze behaviors unfold over the course of an interaction and become tightly coordinated is not well understood. In this paper, we present work to develop a deeper and more nuanced understanding of coordinated referential gaze in collaborating dyads. We recruited 13 dyads to participate in a collaborative sandwich-making task and used dual mobile eye tracking to synchronously record each participant's gaze behavior. We used a relatively new analysis technique—epistemic network analysis—to jointly model the gaze behaviors of both conversational participants. In this analysis, network nodes represent gaze targets for each participant, and edge strengths convey the likelihood of simultaneous gaze to the connected target nodes during a given time-slice. We divided collaborative task sequences into discrete phases to examine how the networks of shared gaze evolved over longer time windows. We conducted three separate analyses of the data to reveal (1) properties and patterns of how gaze coordination unfolds throughout an interaction sequence, (2) optimal time lags of gaze alignment within a dyad at different phases of the interaction, and (3) differences in gaze coordination patterns for interaction sequences that lead to breakdowns and repairs. In addition to contributing to the growing body of knowledge on the coordination of gaze behaviors in joint activities, this work has implications for the design of future technologies that engage in situated interactions with human users.},
	journaltitle = {Frontiers in Psychology},
	author = {Andrist, Sean and Collier, Wesley and Gleicher, Michael and Mutlu, Bilge and Shaffer, David},
	urldate = {2024-01-09},
	date = {2015},
}

@article{mcintyre_scanpath_2018,
	title = {Scanpath analysis of expertise and culture in teacher gaze in real-world classrooms},
	volume = {46},
	issn = {1573-1952},
	url = {https://doi.org/10.1007/s11251-017-9445-x},
	doi = {10.1007/s11251-017-9445-x},
	abstract = {Humans are born to learn by understanding where adults look. This is likely to extend into the classroom, making teacher gaze an important topic for study. Expert teacher gaze has mainly been investigated in the laboratory, and has focused mostly on one cognitive process: teacher attentional (i.e., information-seeking) gaze. No known research has made direct cultural comparisons of teacher gaze or successfully found expert–novice differences outside Western settings. Accordingly, we conducted a real-world study of expert teacher gaze across two cultural settings, exploring communicative (i.e., information-giving) as well as attentional gaze. Forty secondary school teachers wore eye-tracking glasses, with 20 teachers (10 expert; 10 novice) from the {UK} and 20 teachers (10 expert; 10 novice) from Hong Kong. We used a novel eye-tracking scanpath analysis to ascertain the importance of expertise and culture, individually and as a combination. Attentional teacher scanpaths were significantly more similar within than across expertise and expertise + culture sub-groups; communicative scanpaths were significantly more similar within than across expertise and culture. Detailed analysis suggests that (1) expert teachers refer back to students constantly through focused gaze during both attentional and communicative gaze and that (2) expert teachers in Hong Kong scan students more than experts do in the {UK}.},
	pages = {435--455},
	number = {3},
	journaltitle = {Instructional Science},
	shortjournal = {Instr Sci},
	author = {{McIntyre}, Nora A. and Foulsham, Tom},
	urldate = {2023-01-16},
	date = {2018-06-01},
	langid = {english},
	keywords = {Cross-cultural comparisons, Eye-tracking, Real-world, Scanpath analysis, Teacher expertise},
}

@article{jarodzka_eye-tracking_2021,
	title = {Eye-Tracking in Educational Practice: Investigating Visual Perception Underlying Teaching and Learning in the Classroom},
	volume = {33},
	issn = {1573-336X},
	url = {https://doi.org/10.1007/s10648-020-09565-7},
	doi = {10.1007/s10648-020-09565-7},
	shorttitle = {Eye-Tracking in Educational Practice},
	abstract = {Classrooms full of pupils can be very overwhelming, both for teachers and students, as well as for their joint interactions. It is thus crucial that both can distil the relevant information in this complex scenario and interpret it appropriately. This distilling and interpreting happen to a large extent via visual perception, which is the core focus of the current Special Issue. Six empirical studies present examples of how to capture visual perception in the complexity of a classroom lesson. These examples open up new avenues that go beyond studying perception in restricted and artificial laboratory scenarios: some using video recordings from authentic lessons to others studying actual classrooms. This movement towards more realistic scenarios allows to study the visual perception in classrooms from new perspectives, namely that of the teachers, the learners, and their interactions. This in turn enables to shed novel light onto well-established theoretical concepts, namely students’ engagement during actual lessons, teachers’ professional vision while teaching, and establishment of joint attention between teachers and students in a lesson. Additionally, one theoretical contribution provides the very first model of teachers’ cognitions during teaching in relation to their visual perception, which in turn will allow future research to move beyond explorations towards hypothesis testing. However, to fully thrive, this field of research has to address two crucial challenges: (i) the heterogeneity of its methodological approaches (e.g., varying age groups, subjects taught, lesson formats) and (ii) the recording and processing of personal data of many people (often minors). Hence, these new approaches bear not only new chances for insights but also new responsibilities for the researchers.},
	pages = {1--10},
	number = {1},
	journaltitle = {Educational Psychology Review},
	shortjournal = {Educ Psychol Rev},
	author = {Jarodzka, Halszka and Skuballa, Irene and Gruber, Hans},
	urldate = {2023-03-26},
	date = {2021-03-01},
	langid = {english},
	keywords = {Eye-tracking, Professional vision, Student, Student engagement, Teacher, Teacher-student interaction, Visual perception},
}

@article{kosel_identifying_2021,
	title = {Identifying Expert and Novice Visual Scanpath Patterns and Their Relationship to Assessing Learning-Relevant Student Characteristics},
	volume = {5},
	issn = {2504-284X},
	url = {https://www.frontiersin.org/articles/10.3389/feduc.2020.612175},
	abstract = {The paper addresses cognitive processes during a teacher's professional task of assessing learning-relevant student characteristics. We explore how eye-movement patterns (scanpaths) differ across expert and novice teachers during an assessment situation. In an eye-tracking experiment, participants watched an authentic video of a classroom lesson and were subsequently asked to assess five different students. Instead of using typically reported averaged gaze data (e.g., number of fixations), we used gaze patterns as an indicator for visual behavior. We extracted scanpath patterns, compared them qualitatively (common sub-pattern) and quantitatively (scanpath entropy) between experts and novices, and related teachers' visual behavior to their assessment competence. Results show that teachers' scanpaths were idiosyncratic and more similar to teachers of the same expertise group. Moreover, experts monitored all target students more regularly and made recurring scans to re-adjust their assessment. Lastly, this behavior was quantified using Shannon's entropy score. Results indicate that experts' scanpaths were more complex, involved more frequent revisits of all students, and that experts transferred their attention between all students with equal probability. Experts' visual behavior was also statistically related to higher judgment accuracy.},
	journaltitle = {Frontiers in Education},
	author = {Kosel, Christian and Holzberger, Doris and Seidel, Tina},
	urldate = {2023-11-29},
	date = {2021},
	keywords = {expert-novice, on-task, video ratings},
}

@article{ma_eye_2023,
	title = {From eye movements to scanpath networks: A method for studying individual differences in expository text reading},
	volume = {55},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-022-01842-3},
	doi = {10.3758/s13428-022-01842-3},
	shorttitle = {From eye movements to scanpath networks},
	abstract = {Eye movements have been examined as an index of attention and comprehension during reading in the literature for over 30 years. Although eye-movement measurements are acknowledged as reliable indicators of readers’ comprehension skill, few studies have analyzed eye-movement patterns using network science. In this study, we offer a new approach to analyze eye-movement data. Specifically, we recorded visual scanpaths when participants were reading expository science text, and used these to construct scanpath networks that reflect readers’ processing of the text. Results showed that low ability and high ability readers’ scanpath networks exhibited distinctive properties, which are reflected in different network metrics including density, centrality, small-worldness, transitivity, and global efficiency. Such patterns provide a new way to show how skilled readers, as compared with less skilled readers, process information more efficiently. Implications of our analyses are discussed in light of current theories of reading comprehension.},
	pages = {730--750},
	number = {2},
	journaltitle = {Behavior Research Methods},
	shortjournal = {Behav Res},
	author = {Ma, Xiaochuan and Liu, Yikang and Clariana, Roy and Gu, Chanyuan and Li, Ping},
	urldate = {2024-01-09},
	date = {2023-02-01},
	langid = {english},
	keywords = {Eye tracking, Knowledge representation, Network metrics, Reading comprehension, Scanpath},
}

@article{wu_guidance_2014,
	title = {Guidance of visual attention by semantic information in real-world scenes},
	volume = {5},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00054},
	doi = {https://doi.org/10.3389/fpsyg.2014.00054},
	abstract = {Recent research on attentional guidance in real-world scenes has focused on object recognition within the context of a scene. This approach has been valuable for determining some factors that drive the allocation of visual attention and determine visual selection. This article provides a review of experimental work on how different components of context, especially semantic information, affect attentional deployment. We review work from the areas of object recognition, scene perception, and visual search, highlighting recent studies examining semantic structure in real-world scenes. A better understanding on how humans parse scene representations will not only improve current models of visual attention but also advance next-generation computer vision systems and human-computer interfaces.},
	journaltitle = {Frontiers in Psychology},
	author = {Wu, Chia-Chien and Wick, Farahnaz and Pomplun, Marc},
	urldate = {2024-01-11},
	date = {2014},
}

@article{frischen_gaze_2007,
	title = {Gaze cueing of attention: Visual attention, social cognition, and individual differences},
	volume = {133},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.133.4.694},
	shorttitle = {Gaze cueing of attention},
	abstract = {During social interactions, people's eyes convey a wealth of information about their direction of attention and their emotional and mental states. This review aims to provide a comprehensive overview of past and current research into the perception of gaze behavior and its effect on the observer. This encompasses the perception of gaze direction and its influence on perception of the other person, as well as gaze-following behavior such as joint attention, in infant, adult, and clinical populations. Particular focus is given to the gaze-cueing paradigm that has been used to investigate the mechanisms of joint attention. The contribution of this paradigm has been significant and will likely continue to advance knowledge across diverse fields within psychology and neuroscience. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {694--724},
	number = {4},
	journaltitle = {Psychological Bulletin},
	author = {Frischen, Alexandra and Bayliss, Andrew P. and Tipper, Steven P.},
	date = {2007},
	keywords = {Eye Fixation, Face Perception, Individual Differences, Observers, Social Cognition, Social Interaction, Visual Attention},
}

@article{hasenbein_experimental_2023,
	title = {An experimental test of the Big-Fish-Little-Pond Effect using an immersive virtual reality classroom},
	doi = {10.1007/s11251-023-09646-4},
	journaltitle = {Instructional Science},
	author = {Hasenbein, Lisa and Trautwein, Ulrich and Hahn, Jens-Uwe and Soller, Stephan and Göllner, Richard},
	date = {2023},
}

@article{rau_under_2021,
	title = {Under which conditions are physical versus virtual representations effective? Contrasting conceptual and embodied mechanisms of learning},
	issn = {1939-2176},
	doi = {10.1037/edu0000689},
	shorttitle = {Under which conditions are physical versus virtual representations effective?},
	abstract = {Abundant prior research has compared effects of physical and virtual manipulatives on students’ conceptual learning. However, most prior research has been based on conceptual salience theory; that is, it has explained mode effects by the manipulative’s capability to draw students’ attention to conceptually relevant (visual or haptic) features. Yet, research based on embodied schema theory suggests that other mechanisms, which do not rely on students’ explicit attention to specific features, also affect students’ learning from manipulatives. This paper presents a study that contrasts predictions by different theoretical perspectives by comparing multiple versions of physical and virtual manipulatives. Specifically, we conducted a lab experiment with 119 undergraduate students who learned about 3 concepts related to atomic structure using 1 of 4 versions of energy diagram manipulatives. The 4 versions varied the representation mode (i.e., physical vs. virtual) and the actions students used to manipulate the representation (i.e., via actions that draw attention or activate embodied schemas). We assessed students’ learning via reproduction and transfer posttests and interviews that measured the quality of students’ explanation and the gestures they used while explaining the concepts. Our results suggest that embodied schema theory accounts for effects on the reproduction posttest, whereas conceptual salience theories account for effects on the transfer posttest. Further, when physical manipulatives offered relevant haptic cues, we found an advantage of physical manipulatives on transfer. We interpret these results based on the complexity of embodied schema and conceptual salience learning mechanisms and the complexity of the assessment tasks. ({PsycInfo} Database Record (c) 2021 {APA}, all rights reserved)},
	journaltitle = {Journal of Educational Psychology},
	author = {Rau, Martina A. and Herder, Tiffany},
	date = {2021},
	keywords = {College Students, Concept Formation, Cutaneous Sense, Posttesting, Schema, School Learning, Tactual Stimulation, Transfer (Learning), Virtual Environment},
}

@article{lawson_effect_2024,
	title = {Effect of Pre-Training and Role of Working Memory Characteristics in Learning with Immersive Virtual Reality},
	volume = {0},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2024.2325176},
	doi = {10.1080/10447318.2024.2325176},
	abstract = {Immersive virtual reality ({IVR}) is being incorporated into education, but not all learners have expertise in using this technology. As such, this research examined whether pre-training in {IVR} can reduce the novelty of this technology and enhance learning from {IVR} lessons and understand the role of individual differences in managing incoming information (i.e., executive function) and capacity for holding information (i.e., working memory capacity) in learning from an {IVR} lesson. Participants were split into two conditions; half of the participants played a game in {IVR} to become knowledgeable about {IVR} technology and the other half did not play this game. All participants then learned a lesson in {IVR}, took a posttest, and completed working memory tasks. The results showed that playing the game prior to learning in {IVR} did not change the learners’ experience of distraction or their learning outcome, indicating that {IVR} game-playing was not an effective form of pre-training. Additionally, several measures of executive function and working memory capacity were correlated with posttest performance, indicating that students with better executive function learn better with distracting media such as {IVR}, regardless of pre-training. Theoretical and practical implications are discussed.},
	pages = {1--18},
	number = {0},
	journaltitle = {International Journal of Human–Computer Interaction},
	author = {Lawson, Alyssa P. and Mayer, Richard E.},
	urldate = {2024-03-14},
	date = {2024},
	keywords = {Executive function, immersive virtual reality, learning, pre-training, working memory capacity},
}

@article{de_la_rosa_virtual_2018,
	title = {Virtual reality: A new track in psychological research},
	volume = {109},
	issn = {2044-8295},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bjop.12302},
	doi = {10.1111/bjop.12302},
	shorttitle = {Virtual reality},
	abstract = {One major challenge of social interaction research is to achieve high experimental control over social interactions to allow for rigorous scientific reasoning. Virtual reality ({VR}) promises this level of control. Pan and Hamilton guide us with a detailed review on existing and future possibilities and challenges of using {VR} for social interaction research. Here, we extend the discussion to methodological and practical implications when using {VR}.},
	pages = {427--430},
	number = {3},
	journaltitle = {British Journal of Psychology},
	author = {de la Rosa, Stephan and Breidt, Martin},
	urldate = {2022-04-01},
	date = {2018},
	langid = {english},
	keywords = {ecological validity, social interaction, virtual reality},
}

@misc{song_optimizing_2023,
	title = {Optimizing Foreign Language Learning in Virtual Reality: A Comprehensive Theoretical Framework Based on Constructivism and Cognitive Load Theory ({VR}-{CCL})},
	url = {https://www.preprints.org/manuscript/202309.2101/v1},
	doi = {10.20944/preprints202309.2101.v1},
	shorttitle = {Optimizing Foreign Language Learning in Virtual Reality},
	abstract = {With the widespread Application of Virtual Reality ({VR}) in education, optimizing foreign language learning in {VR} has become a focal point of research. This paper introduces a comprehensive theoretical framework ({VR}-{CCL}) based on constructivism and cognitive load theory to enhance foreign language learning in {VR}. Through a literature review, we explore the applications of {VR} in education, foreign language learning theories, and prior works on technology-assisted language learning. We further detail the three main components of the {VR}-{CCL} framework and validate its effectiveness through two case studies: Duolingo {VR} and Rosetta Stone {VR}. Finally, we discuss the strengths and limitations of the framework and its implications for educators and developers.},
	number = {2023092101},
	publisher = {Preprints},
	author = {Song, Chuanxiang and Shin, Seong-Yoon and Shin, Kwang-Seong},
	urldate = {2023-10-06},
	date = {2023-09-29},
	langid = {english},
	keywords = {Cognitive Load Theory, Constructivism, Foreign Language Learning, Technology-Assisted Learning, {VR}-{CCL} Framework, Virtual Reality},
}